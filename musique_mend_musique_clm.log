/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.
This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64

MEND editing:   0%|          | 0/50 [00:00<?, ?it/s]02/06/2025 15:24:02 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:24:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:24:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:24:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:24:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:24:03 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:24:05.839 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:24:06.447 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:24:06.447 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:24:06.447 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__661591_13728
Base model weight checksum: -10738.13671875
[0; 0] edit/acc: 0.49494948983192444 --> 0.9090909361839294
[0; 0] target: 
 ['Rossi begins the series returning to the FBI after a lengthy period of being semi-retired, with his return due to "unfinished business". He is shown to be emotionally tied to Aaron Hotchner, as they both worked together during the early days of the Behaviorial Analysis Unit. Rossi is also a writer, and is one of the team\'s senior and most decorated profilers. He replaced Jason Gideon, who was written out following Mandy Patinkin\'s abrupt departure from the series.']
[0; 0] edit/gen: 
 ['Questioni, to  with to the track and a long absence of absence away-retired. and the wife to to thea business" He is assigned to be a and to his,chner, who he both have together on the investigation days of the FBI Analysis Sciences Unit. He is also shown father, and is shown of the few\'s main members most experienced agentsilers. He is Jack Gideon as who left killed out of theandy Ginkin\'s departure departure from the series.'] 
 --> 
 ['Thei begins the series returning to the FBI after a lengthy period of being semi-retired, with M return due to "unfinished business". He is also to be a tied to Aaron Hotchner, as they both worked together during the early days of the behaviorialial Unit. Rossi is also a writer, and is one of the team\'s senior prof most decorated profilers. He is Jason Gideon, who was written out following Mandy Patinkin\'s abrupt abrupt from the series.']
Base model weight checksum: -18948.55859375
[0; 1] edit/acc: 0.5074626803398132 --> 0.9353233575820923
[0; 1] target: 
 ['In 2005, fugitive Puerto Rican Nationalist leader Filiberto Ojeda Ríos died in a gun battle with FBI agents in 2005 in what some charged was an assassination.[citation needed] Puerto Rico Governor Aníbal Acevedo Vilá criticized the FBI assault as "improper" and "highly irregular" and demanded to know why his government was not informed of it. The FBI refused to release information beyond the official press release, citing security and agent privacy issues. The Puerto Rico Justice Department filed suit in federal court against the FBI and the US Attorney General, demanding information crucial to the Commonwealth\'s own investigation of the incident. The case was dismissed by the U.S Supreme Court. Ojeda Rios\' funeral was attended by a long list of dignitaries, including the highest authority of the Roman Catholic Church in Puerto Rico, Archbishop Roberto Octavio González Nieves, ex-Governor Rafael Hernández Colón, and numerous other personalities.']
[0; 1] edit/gen: 
 ['The the2019, theitive and Rican drugist Luis Luisiberto Ojeda Riosos was in a shootout battle with the and. the2005. the was have was a assassination.1 needed] He Rico\'s Luisíbal Acevedo Vilá, the U for, "unproper" and "unly questionable". and " an know why the state was not informed of the. He FBI has to answer the on the fact statement release, and the concerns privacy safety concerns. The FBI Rican government Department is a against  court to the FBI, the U Attorney\'s, as that about to the investigation\'s investigation investigation. O death. The FBI was settled by the US.S. Court in<|end_of_text|>jeda Rí was remains was held by  large line of prominentitaries, including the President ranking in the Commonwealth Catholic Church in Puerto Rico, the Ram Gonzavio Oález Nieves. andiledovernor Luis Hernández Colón, and the other politicians.'] 
 --> 
 ['The 2005, theitive Puerto Rico Nationalist leader Filiberto Ojeda Ríos died in a gun battle with FBI agents in what2005 in what some charged was an assassination.[citation needed] The Rico Governor Aníbal Acevedo Vilá criticized the FBI assault as "improper" and demandedhighly irregular" and demanded to know why his government was not informed of it. The FBI refused to release information beyond the official press release, citing security and agent privacy issues. The FBI Rico Justice Department filed suit against federal court against the FBI and the FBI Attorney General, demanding information crucial to the Commonwealth\'s own investigation of the incident. The FBI was dismissed by the U.S Supreme Court in Thejeda Rios\' funeral was attended by a long list of dignitaries, including the highest authority of the Roman Catholic Church in Puerto Rico, Archbishop Roberto Octavio González Nieves, ex-Governor Rafael Hernández Colón, ex numerous other personalities.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][AFrom v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.

Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]
2025-02-06 15:24:07.523 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:24:07.536 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:24:07 - INFO - absl -   Using default tokenizer.
2025-02-06 15:24:07.541 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:24:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:24:08.437 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:24:08.998 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:24:08.999 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:24:08.999 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]
2025-02-06 15:24:10.361 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:24:10.376 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:24:10 - INFO - absl -   Using default tokenizer.
2025-02-06 15:24:10.383 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:24:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:   2%|▏         | 1/50 [00:14<11:42, 14.33s/it]02/06/2025 15:24:15 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:24:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:24:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:24:16 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:24:16 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:24:16 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:24:18.035 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:24:18.651 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:24:18.652 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:24:18.652 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__341498_76347
Base model weight checksum: -10738.13671875
[1; 0] edit/acc: 0.40625 --> 0.9375
[1; 0] target: 
 ['After the Rain is the sixth studio album by Muddy Waters, a follow-up to the previous years\' "Electric Mud" and sharing many of the musicians from that album. Unlike "Electric Mud", "After the Rain" contained mostly his own compositions and the songs, while still distorted, are less overtly psychedelic.']
[1; 0] edit/gen: 
 ['Question a success: a first studio album by Americanoby Waters, released blues-up to his  album\' albumsM Mud" and " the of the same and that album. The theElectric Mud", thisAfter the Rain" was no original own compositions, was album were " " featuring and were more rawly blues than'] 
 --> 
 ['The the Rain is the sixth studio album by Muddy Waters, a follow-up to the previous years\' "Electric Mud" and " many of the musicians from that album. Unlike "Electric Mud", "After the Rain" contained mostly his compositions compositions. the songs, while still distorted, are less overtly psychedelic.']
Base model weight checksum: -19866.625
[1; 1] edit/acc: 0.5106382966041565 --> 0.9574467539787292
[1; 1] target: 
 ["McKinley Morganfield (April 4, 1913 -- April 30, 1983), known professionally as Muddy Waters, was an American blues musician who is often cited as the ``father of modern Chicago blues ''."]
[1; 1] edit/gen: 
 ['TheDonaldney is\n,born 25, 1911 – June 10, 1983) better as as "uddy Waters, was an American blues musician. is considered cited as the "father\'\' the blues blues\'\'.'] 
 --> 
 ['"Kinley Morganfield (April 4, 1913 -- April 30, 1983), known professionally as Muddy Waters, was an American blues musician who is often cited as the ``father of modern blues blues \'\'.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 15:24:19.320 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:24:19.328 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:24:19 - INFO - absl -   Using default tokenizer.
2025-02-06 15:24:19.331 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:24:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:24:19.662 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:24:20.345 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:24:20.345 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:24:20.346 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]
2025-02-06 15:24:21.709 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:24:21.724 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:24:21 - INFO - absl -   Using default tokenizer.
2025-02-06 15:24:21.730 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:24:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:   4%|▍         | 2/50 [00:24<09:27, 11.82s/it]02/06/2025 15:24:24 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:24:24 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:24 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:24:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:24:25 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:24:25 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:24:25 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:24:26.532 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:24:27.114 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:24:27.115 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:24:27.115 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__390772_565667
Base model weight checksum: -10738.13671875
[2; 0] edit/acc: 0.5 --> 0.9750000238418579
[2; 0] target: 
 ['Christoph Eichhorn (born 8 September 1957, in Kassel, Germany) is a German television actor and director. He is the son of German actor Werner Eichhorn.']
[2; 0] edit/gen: 
 ['Questionine Sch.horn\nborn 197 January 1979 in in Berliniel) Germany) is a German composer director and film. He is best son of actor film and Eichhorn.'] 
 --> 
 ['Hoph Eichhorn (born 8 September 1957, in Kassel, Germany) is a German television actor and director. He is the son of German actor Werner Eichhorn.']
Base model weight checksum: -20625.53515625
[2; 1] edit/acc: 0.529411792755127 --> 0.8235294222831726
[2; 1] target: 
 ['Stellbergsee is a lake in Söhre, Landkreis Kassel and Schwalm-Eder-Kreis, Hesse, Germany. At an elevation of 356 m, its surface area is 0.014\xa0km².']
[2; 1] edit/gen: 
 ['H.antis ( ( a small in Kankland, ineckis Kassel, inalm-Eich-Kasselis, inesse, Germany. It  elevation of 195., it surface area is 7.7 kmkm².'] 
 --> 
 ['Stellbergsee is a lake in Söhre, Landkreis Sre and Halm-Htr-reis and Hesse and and. At an elevation of 356 m, its surface area is 0.014\xa0km².']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:24:27.789 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:24:27.798 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:24:27 - INFO - absl -   Using default tokenizer.
2025-02-06 15:24:27.800 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 15:24:27.805 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:24:28.388 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:24:28.389 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:24:28.389 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:24:29.712 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:24:29.728 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:24:29 - INFO - absl -   Using default tokenizer.
2025-02-06 15:24:29.731 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:24:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:   6%|▌         | 3/50 [00:33<08:21, 10.67s/it]02/06/2025 15:24:34 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:24:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:24:35 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:35 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:24:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:24:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:24:36 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:24:37.289 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:24:38.098 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:24:38.099 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:24:38.099 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__258019_119986
Base model weight checksum: -10738.13671875
[3; 0] edit/acc: 0.4761905074119568 --> 0.9047619700431824
[3; 0] target: 
 ['Woodbury Langdon (1739 – January 13, 1805) was a merchant, statesman and justice from Portsmouth, New Hampshire. He was the brother of John Langdon, a Founding Father who served as both senator from and Governor of New Hampshire, and father-in-law of Edmund Roberts.']
[3; 0] edit/gen: 
 ['Questionen,ley\nW0-  1, 1809) was a member and politicianman, politician of New, New Hampshire. He was the first of John Langdon, the signering Father of served as the a and New governor of New Hampshire. and the of-law of John S,'] 
 --> 
 ['Newbury Langdon Lang1739 – January 13, 1805) was a merchant, statesman, justice from Portsmouth, New Hampshire. He was the brother-in Edmund Langdon, a Founding Father who served as both senator from and Governor of New Hampshire. and father-in-law of Edmund Roberts.']
Base model weight checksum: -20156.52734375
[3; 1] edit/acc: 0.5338982939720154 --> 0.9152542352676392
[3; 1] target: 
 ['The post of Commander-in-Chief, Portsmouth was merged with that of Commander-in-Chief, Plymouth in 1969 to form the post of Commander-in-Chief, Naval Home Command. The posts of Second Sea Lord and Commander-in-Chief Naval Home Command were amalgamated in 1994 following the rationalisation of the British Armed Forces following the end of the Cold War. In 2012, however, all distinct Commander-in-Chief appointments were discontinued, with full operational command being vested instead in the First Sea Lord; he now flies his flag from HMS "Victory".']
[3; 1] edit/gen: 
 ['New following The the of-Chief of Army Naval created with the of the-in-Chief, Home in 1804. form the new of Commander-in-Chief, Home Home Command. The role of Commander Sea Lord and Chief-in-Chief, Home Command were abolishedamated in 1995 to the mergerisation of the Royal Royal Forces. the end of the Cold War. The 2000 the the, the the roles-in-Chief roles were abolished. and the responsibility command of exercised in in the Chief Sea Lord. the is also the flag in the QueenVictory"'] 
 --> 
 ['The post of Commander-in-Chief, Portsmouth was merged with that of Commander-in-Chief Naval Plymouth in 1969 following form Commander post of Commander-in-Chief Naval Naval Home Command. In post of Commander Sea Lord and Commander-in-Chief Naval Home Command were amalgamated in 1994 following the rationalisation of Commander British Armed Forces following the end of the Cold War. In 2012, however, all distinct Commander-in-Chief appointments were discontinued following with full operational command being vested instead in Commander First Sea Lord; he now flies his flag from HMS "Victory".']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:24:38.770 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:24:38.781 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:24:38 - INFO - absl -   Using default tokenizer.
2025-02-06 15:24:38.788 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:24:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:24:40.146 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:24:40.738 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:24:40.739 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:24:40.739 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:24:42.067 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:24:42.094 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:24:42 - INFO - absl -   Using default tokenizer.
2025-02-06 15:24:42.099 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:24:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:43 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:   8%|▊         | 4/50 [00:44<08:15, 10.77s/it]02/06/2025 15:24:45 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:24:45 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:45 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:24:45 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:45 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:24:46 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:24:46 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:24:46 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:24:48.006 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:24:48.600 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:24:48.601 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:24:48.601 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__60060_25017
Base model weight checksum: -10738.13671875
[4; 0] edit/acc: 0.6274510025978088 --> 0.9705882668495178
[4; 0] target: 
 ["Lamb of God (Greek: Ἀμνὸς τοῦ Θεοῦ, Amnos tou Theou; Latin: Agnus Deī (ˈaŋ. nʊs ˈde. iː)) is a title for Jesus that appears in the Gospel of John. It appears at John 1: 29, where John the Baptist sees Jesus and exclaims, ``Behold the Lamb of God who takes away the sin of the world. ''"]
[4; 0] edit/gen: 
 ['QuestionONDONorghini God,album: Λ�γνὸς θεῦ Θεοῦ, Amnos tou Theou, Latin: Animnus Dei)A��ɋs�ʊs ˈd� iː), is a Greek for the in is in the New of John. It is in the 1:2929,  it the Baptist says Jesus and saysclaims, "Hereold the Lamb of God who takes away the sin of the world!″'] 
 --> 
 ["Theamb of God (Greek: Ἀμνὸς τοῦ Θεοῦ, Amnos tou Theou; Latin: Agnus Deī;ˈaŋ. nʊs ˈde. iː)) is a title for Jesus that appears in the Gospel of John. It appears at John 1: , where John the Baptist sees Jesus and exclaims, ``Behold the Lamb of God who takes away the sin of the world. ''"]
Base model weight checksum: -19560.734375
[4; 1] edit/acc: 0.4836065173149109 --> 0.8934425711631775
[4; 1] target: 
 ["The earliest detailed accounts of the death of Jesus are contained in the four canonical gospels. There are other, more implicit references in the New Testament epistles. In the synoptic gospels, Jesus predicts his death in three separate episodes. All four Gospels conclude with an extended narrative of Jesus' arrest, trial, crucifixion, burial, and accounts of resurrection. In each Gospel these five events in the life of Jesus are treated with more intense detail than any other portion of that Gospel's narrative. Scholars note that the reader receives an almost hour-by-hour account of what is happening.:p.91"]
[4; 1] edit/gen: 
 ["The  known description of the history of Jesus appear found in the Gospel Gospel gospels. The are a accounts less fragment, to the Gospel Testament.istles. The the Gospeloptic gospels, John appears his death and John ways instances. John three gospels record with the account account of Jesus' death, trial, andifixion, and, and resurrection of the. The John gospel, events events are the life of Jesus are presented as a or detail than in other event of the Gospel. narrative. The have that the Gospel is a impression overwhelming-by-hour account of Jesus happened happening at The. "] 
 --> 
 ["The earliest detailed accounts of the death of Jesus are contained in the g canonical gospels. In are other, more implicit references in the New Testament epistles. In the Newoptic gospels, there predicts his death in three separate episodes. In four gospels conclude with an extended narrative of an' arrest, trial, crucifixion, burial, and resurrection of resurrection. In the gospel these five events in the life of Jesus are treated with more intense detail than any other portion of that narrative's narrative. In note that the reader receives an almost hour-by-hour account of what is happening inp.91"]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:24:49.279 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:24:49.290 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:24:49 - INFO - absl -   Using default tokenizer.
2025-02-06 15:24:49.298 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:24:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:24:50.032 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:24:50.779 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:24:50.780 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:24:50.780 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 15:24:52.098 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:24:52.113 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:24:52 - INFO - absl -   Using default tokenizer.
2025-02-06 15:24:52.116 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:24:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:24:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  10%|█         | 5/50 [00:57<08:46, 11.70s/it]02/06/2025 15:24:59 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:24:59 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:59 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:24:59 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:24:59 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:25:00 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:25:00 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:25:00 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:25:01.483 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:25:02.087 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:25:02.088 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:25:02.088 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__508013_351187
Base model weight checksum: -10738.13671875
[5; 0] edit/acc: 0.5625 --> 0.9583333730697632
[5; 0] target: 
 ['Leo Villareal (born 1967 in Albuquerque, New Mexico) is an American artist living and working in New York City. His work combines LED lights and encoded computer programming to create illuminated displays. He is represented by Pace Gallery.']
[5; 0] edit/gen: 
 ['Question Burnettareal’sborn 1967) New, New Mexico) is an American artist and and working in New York City. He work is light lighting, computer data code to create large sculptures that His is known by the Gallery in'] 
 --> 
 ['The Villareal (born 1967 in Albuquerque, New Mexico) is an American artist living and working in New York City. His work combines LED lights and encoded computer programming to create illuminated displays. His is represented by Pace Gallery.']
Base model weight checksum: -19050.46875
[5; 1] edit/acc: 0.5641025900840759 --> 0.9487179517745972
[5; 1] target: 
 ['The Bernalillo County Metropolitan Court is the Judicial system of the metropolitan areas of Albuquerque, New Mexico and Bernalillo County, New Mexico. The Metropolitan Courthouse is located in Downtown Albuquerque.']
[5; 1] edit/gen: 
 ['The -illo County Sheriff Det is the primary Branch for the City area of Albuquerque, New Mexico. Rioernalillo County. New Mexico. The court Courtthouse is located at the Albuquerque,'] 
 --> 
 ['The Bernalillo County Metropolitan Court is the Judicial system of the metropolitan areas of B, B Mexico and Bernalillo County, New Mexico. The Metropolitan Courthouse is located in Downtown Albuquerque.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:25:02.764 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:25:02.773 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:25:02 - INFO - absl -   Using default tokenizer.
2025-02-06 15:25:02.776 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:25:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:25:05.188 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:25:05.785 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:25:05.786 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:25:05.786 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:25:07.120 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:25:07.135 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:25:07 - INFO - absl -   Using default tokenizer.
2025-02-06 15:25:07.138 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:25:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  12%|█▏        | 6/50 [01:13<09:33, 13.04s/it]02/06/2025 15:25:14 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:25:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:25:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:25:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:25:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:25:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:25:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:25:15 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:25:16.716 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:25:17.417 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:25:17.418 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:25:17.418 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__72949_9902
Base model weight checksum: -10738.13671875
[6; 0] edit/acc: 0.42028987407684326 --> 0.7971014380455017
[6; 0] target: 
 ["In North Korea, China, Japan, Vietnam, and Chinese and Vietnamese - speaking areas, Korea as a whole is referred to as Chosŏn (조선, Joseon, (tɕoshʌn),, (朝鲜), Cháoxiǎn, (朝鮮), Chōsen, Triều Tiên (朝鮮) lit. ``(land of the) Morning Calm '').`` Great Joseon'' was the name of the kingdom ruled by the Joseon dynasty from 1393 until their declaration of the short - lived Great Korean Empire in 1897. King Taejo had named them for the earlier Kojoseon (고조선), who ruled northern Korea from its legendary prehistory until their conquest in 108 BC by China's Han Empire. This go is the Hanja 古 and simply means ``ancient ''or`` old''; it is a modern usage to distinguish the ancient Joseon from the later dynasty. Joseon itself is the modern Korean pronunciation of the Hanja 朝鮮 but it is unclear whether this was a transcription of a native Korean name (OC * T (r) awser, MC Trjewsjen) or a partial translation into Chinese of the Korean capital Asadal (아사달), whose meaning has been reconstructed as ``Morning Land ''or`` Mountain''."]
[6; 0] edit/gen: 
 ['Question the America, the and and, and, and South Taipei Japanese cities the countries, the, a whole is divided to as theosŏn (조선), 朝on), "K)��ʌn)). the litt鮮,,osngxiǎn, (t鮮), Chossen, (ều Tiên,朝鮮), or. "North of)) Ch Sunm,\nCh Koreaon \'\' is the name given the Korean of by the Joseon dynasty from 1392 to  fall of independence Republic-lived lived Empire Jose Empire in 1897. The Seaejo of the the after the Korean kingdomoryongon dynasty조조선, the had the Korea from  capital capitalhistoric to the defeat by 108 BC by the\'s Han dynasty. The wasthic the first dynasty for朝 the means "oldient\'\'. or `` old \'\'..\n the is not referenceized of refer the K kingdomon from the modern Ch. Theon is is the Korean Korean pronunciation of the Chineseja 朝鮮, is is also whether this is the deliberate of the Korean Korean pronunciation orCh:ChaeT) (�on OC *aw (seron) or a transl transcription of Korean of the Chinese name cityan (OC사달, which name is been lost as ``Morning Cal\'\' or`` Morning of'] 
 --> 
 ["The the Korea, China, Japan, and, and Vietnam and Vietnamese, speaking areas, the, a whole is referred to as Chosŏn (조선), Joseon, (rɕosʌn), ( (t鲜), (áoxiǎn, (朝鮮), Chōsen, (ều Tiên,朝鮮) or. ``land of the) Morning Calm '' It`` Koreanon'' was the name of the dynasty ruled by the Joseon dynasty from 1393 until their conquest of the short - lived Great Korean Empire in 1897. Jose Taejo ( named the in the earlier Hanojoseon (고조선), who ruled northern Korea from  legendary prehistory until their conquest by 108 BC by the's Han Empire. Jose was was the Hanja for Jose is means ``ancient''or`` old ''or it is a modern usage of distinguish the ancient Joseon from the later Great. Joseon itself is a modern Korean pronunciation of the Hanja Jose鮮 ( it is unclear whether this was a transcription of the native Korean name orOC: T (r) aw (, MC Trjewsjen) or a partial translation of Chinese of the Han capital Asadal (OC사달), whose meaning is been reconstructed as``Morning Land ''or`` Morning ''"]
Base model weight checksum: -22867.15625
[6; 1] edit/acc: 0.4583333432674408 --> 0.881944477558136
[6; 1] target: 
 ['By the time of John Seller\'s Atlas Maritima of 1670, "India Beyond the Ganges" had become "the East Indies" including China, Korea, southeast Asia and the islands of the Pacific in a map that was every bit as distorted as Ptolemy\'s, despite the lapse of approximately 1500 years. That "east" in turn was only an English translation of Latin Oriens and Orientalis, "the land of the rising sun," used since Roman times for "east." The world map of Jodocus Hondius of 1590 labels all of Asia from the Caspian to the Pacific as India Orientalis, shortly to appear in translation as the East Indies.']
[6; 1] edit/gen: 
 ['The using Court you the\'s\'s  ofitimus, 1676, thethe" the Seasanges" was been aIndia most Indies" and the, Japan, Japan Asia, the Philippines of the Pacific. the single of was to bit as large as thetolemy\'s. but the fact of  10 years. The wasEast" was the was a a approximation translation of the\'sens, wasis, arising rising of the rising sun." a by the times to theChinaern The map was of theodocus Hondius, 1637 was the of the as the Caspian Sea the Pacific as ",is, " to be in the as India East Indies.'] 
 --> 
 ['The the time of John Seller\'s Atlas Maritima of 1670, "east Beyond the Ganges" had become "the East Indies Beyond including the, southeast, southeast Asia and the islands of the Pacific in " map that was every bit " distorted as Ptolemy\'s, " " lapse of approximately 1500 years. " "east" in turn labels only " English translation of Latin Oriens and Orientalis, "the land of the rising sun," used since Roman times for "east." The world map of Jodocus Hondius of 1590 labels " of Asia from the Caspian to the Pacific as Oriental Orientalis Oriental shortly to appear in translation as Ori East Indies Oriental']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 15:25:18.088 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:25:18.105 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:25:18 - INFO - absl -   Using default tokenizer.
2025-02-06 15:25:18.112 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:25:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:25:20.512 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:25:21.104 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:25:21.105 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:25:21.105 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:25:22.445 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:25:22.461 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:25:22 - INFO - absl -   Using default tokenizer.
2025-02-06 15:25:22.464 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:25:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  14%|█▍        | 7/50 [01:32<10:42, 14.95s/it]02/06/2025 15:25:32 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:25:32 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:25:32 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:25:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:25:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:25:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:25:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:25:34 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:25:34.895 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:25:35.674 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:25:35.674 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:25:35.675 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__710977_25111
Base model weight checksum: -10738.13671875
[7; 0] edit/acc: 0.5555555820465088 --> 0.9629629850387573
[7; 0] target: 
 ['Born in Macau, Choi obtained a Master of Science degree in statistics and Bachelor of Science degree in mathematics from the University of Essex.']
[7; 0] edit/gen: 
 ['Question in au, China was his Bachelor of Arts degree in Mechanical from a of Science degree in mathematics from the University of Mac,'] 
 --> 
 ['Born in Macau, Choi obtained a Master of Science degree in statistics and Bachelor of Science degree in mathematics and the University of Essex.']
Base model weight checksum: -19473.11328125
[7; 1] edit/acc: 0.527999997138977 --> 0.9360000491142273
[7; 1] target: 
 ["In most nations with constitutions modelled after the Soviet Union, the legislature was given the power of being the court of last resort. In the People's Republic of China, the final power to interpret the law is vested in the Standing Committee of the National People's Congress (NPCSC). This power includes the power to interpret the basic laws of Hong Kong and Macau, the constitutional documents of the two special administrative regions which are common law and Portuguese-based legal system jurisdictions respectively. This power is a legislative power and not a judicial one in that an interpretation by the NPCSC does not affect cases which have already been decided."]
[7; 1] edit/gen: 
 ["Born a cases, autions,led on the United Union, the Communist is the a power to the the supreme of last resort. In the United's Republic of China, the legislature court of decide the law is vested in the Supreme Committee of the National People's Congress.NPC),). The committee is the power to interpret the constitution law of the Kong, theau, as power provisions of the two territories administrative regions, are not law jurisdictions civil law systems systems... The power is exercised unique power, not a judicial power. the it NPC of the NPCSC is not have the already have already been decided by"] 
 --> 
 ["Born most nations with constitutions modelled after the Soviet Union, the legislature is given the power of being the court of last resort in This the People's Republic of China, the final power to interpret the law is vested in the Standing Committee of the National People's Congress (NPCSC). This power includes the power to interpret the basic laws of Hong Kong and Macau, the constitutional documents of Hong two special administrative regions ( are common law and Portuguese-based legal system respectively respectively. This power includes a legislative power and not a judicial one in that an interpretation by the NPCSC does not affect cases which have already been decided."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]
2025-02-06 15:25:36.823 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:25:36.832 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:25:36 - INFO - absl -   Using default tokenizer.
2025-02-06 15:25:36.835 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:25:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:25:37.429 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:25:38.080 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:25:38.081 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:25:38.081 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:25:39.404 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:25:39.419 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:25:39 - INFO - absl -   Using default tokenizer.
2025-02-06 15:25:39.422 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:25:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  16%|█▌        | 8/50 [01:42<09:27, 13.50s/it]02/06/2025 15:25:43 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:25:43 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:25:43 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:25:43 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:25:43 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:25:45 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:25:45 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:25:45 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:25:45.896 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:25:46.525 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:25:46.526 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:25:46.526 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132710_120035
Base model weight checksum: -10738.13671875
[8; 0] edit/acc: 0.48051947355270386 --> 0.9610389471054077
[8; 0] target: 
 ['The Hudson Wasp is an automobile that was built and marketed by the Hudson Motor Car Company of Detroit, Michigan, from the 1952 through the 1956 model years. After Hudson merged with Nash Motors, the Wasp was then built by American Motors Corporation in Kenosha, Wisconsin, and marketed under its Hudson marque for model years 1955 and 1956.']
[8; 0] edit/gen: 
 ["Question  Valleyines is a insect manufactured was produced by sold by Hudson Hudson Motor Car Company from New, Michigan, United  1920 through  1954 model years. The the's with Pack-K in the Hudsonasp was renamed marketed by Nash Motors Corporation ( theosha, Wisconsin, as marketed as the own name. the years 1957 through 1956."] 
 --> 
 ['The Hudson Wasp is an automobile that was built and marketed by the Hudson Motor Car Company of Detroit, Michigan, from the 1952 through the 1956 model years. After Hudson merged with Nash Motors, the Wasp was then built and American Motors Corporation, Kenosha, Wisconsin, and marketed under its Hudson marque for model years 1956 and 1956.']
Base model weight checksum: -19219.01953125
[8; 1] edit/acc: 0.5909091234207153 --> 0.7575757503509521
[8; 1] target: 
 ['The Hudson Motor Car Company made Hudson and other brand automobiles in Detroit, Michigan, from 1909 to 1954. In 1954, Hudson merged with Nash-Kelvinator to form American Motors Corporation (AMC). The Hudson name was continued through the 1957 model year, after which it was discontinued.']
[8; 1] edit/gen: 
 ['The Hudson W Car Company was thes Nash Hudson cars for Detroit, Michigan, from the1909 to the1953. The the1954, the merged with Nash Motorselvinator Motors form the Motors Corporation,AMC). The Hudson marque was then by the Hudson1955 Hudson year, and which the was then.'] 
 --> 
 ['The Hudson Motor Company Company made  andused brand name in, Michigan, from 1909 to 1954. In 1904, it name withager andelvinator to produce distribution made Corporation (AMC). The name brand was continued through  1957 model year, after which it was discontinued.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]
2025-02-06 15:25:47.715 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:25:47.724 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:25:47 - INFO - absl -   Using default tokenizer.
2025-02-06 15:25:47.731 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 15:25:47.740 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:25:48.357 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:25:48.357 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:25:48.358 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:25:49.684 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:25:49.699 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:25:49 - INFO - absl -   Using default tokenizer.
2025-02-06 15:25:49.702 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:25:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  18%|█▊        | 9/50 [01:51<08:11, 11.98s/it]02/06/2025 15:25:52 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:25:52 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:25:52 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:25:52 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:25:52 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:25:53 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:25:53 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:25:53 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:25:54.433 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:25:55.073 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:25:55.073 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:25:55.073 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__13778_15345
Base model weight checksum: -10738.13671875
[9; 0] edit/acc: 0.514285683631897 --> 0.8799999952316284
[9; 0] target: 
 ['In Methodism, Mary is honored as the Mother of God. Methodists do not have any additional teachings on the Virgin Mary except from what is mentioned in Scripture and the ecumenical Creeds. As such, Methodists believe that Mary was conceived in her womb through the Holy Ghost and accept the doctrine of the Virgin Birth, although they, along with Orthodox Christians and other Protestant Christians, reject the doctrine of the Immaculate Conception. John Wesley, the principal founder of the Methodist movement within the Church of England, believed that Mary "continued a pure and unspotted virgin", thus upholding the doctrine of the perpetual virginity of Mary. Contemporary Methodism does hold that Mary was a virgin before, during, and immediately after the birth of Christ. In addition, some Methodists also hold the doctrine of the Assumption of Mary as a pious opinion.']
[9; 0] edit/gen: 
 ['Question theology, the Baker the as the patron of the. Sheists believe not believe a saints saints about Mary Virgin Mary, that the is found in the. the writingsumenical creeds. The a, Methodists do that Mary is a without the mother by the power Spirit and that the Virgin of the Im Birth. which they do like with other Christians, Roman Christians denomin, do the doctrine of the Immaculate Conception. Method Wesley, the founder founder of Method Methodist movement, Christianity Anglic of England, was that Mary waswas in pure and holyspotted virginity and,holding the doctrine of the Im virginity of Mary.<|end_of_text|> Methodists is not the Mary was a virgin when her during, and after after the birth of Jesus.<|end_of_text|> the, Method Methodists believe believe that belief of the perpetualumption of Mary, well doctrineious belief,'] 
 --> 
 ['The additionism, the is honored as the Mother of God. Asism also not have any additional teachings on the Virgin Mary except from what is mentioned in Scripture. the ecumenical Creeds. As such, Methodism have that Mary was a through her womb through the Holy Ghost and accepted the doctrine of the Virgin Birth. although they also along with Orthodox Christians, other Protestant Christians, reject the doctrine of the Assmaculate Conception. In Wesley, the principal founder of the Methodist movement within the Church of England, believed that Mary wascontinued a pure and unspotted virgin", thus upholding the doctrine of the perpetual virginity of Mary. Contemporary Methodism does hold that Mary was a virgin before, during, and immediately following the birth of Christ. Contemporary addition, some Methodists also hold that doctrine of the Assumption of Mary as a pious opinion.']
Base model weight checksum: -19539.546875
[9; 1] edit/acc: 0.4795321822166443 --> 0.8011695742607117
[9; 1] target: 
 ['Since Elizabeth rarely gives interviews, little is known of her personal feelings. As a constitutional monarch, she has not expressed her own political opinions in a public forum. She does have a deep sense of religious and civic duty, and takes her coronation oath seriously. Aside from her official religious role as Supreme Governor of the established Church of England, she is personally a member of that church and the national Church of Scotland. She has demonstrated support for inter-faith relations and has met with leaders of other churches and religions, including five popes: Pius XII, John XXIII, John Paul II, Benedict XVI and Francis. A personal note about her faith often features in her annual Christmas message broadcast to the Commonwealth. In 2000, she spoke about the theological significance of the millennium marking the 2000th anniversary of the birth of Jesus:']
[9; 1] edit/gen: 
 ['The  was gets interviews, I is known about her life life. In such child monarch, she was been been any views views views. any public forum. In has, a strong and of duty faith moral duty. however is her roleation oath very. As from her duties duties duties, head Governor of the Church Church of England, she has also a devout of the church. has Anglic Anglic of Scotland. As is also her for thefaithaith dialogue and has been with representatives of other faith. faith. including Pope headses. Johnius XII, John XXIII, Paul Paul II, Benedict XVI, Francis. In devout friend from Elizabeth faith is appears in her speeches Christmas messages. to the nation. In addition2017, she was of the need doctrine of the doctrine. the 2000th anniversary of the birth of Christ.'] 
 --> 
 ['In  rarely gives interviews and little is known of her personal feelings and She a constitutional monarch she she has not expressed her own political opinions and her public forum and She has have her deep sense of religious and civic responsibility and and takes her coronation oath seriously and She from her official religious role as Supreme Governor of the established Church of England and she has personally a member of that church and takes national Church of Scotland and She has met her and inter-faith relations and has met with leaders of other churches and religions and including five popes and Johnius XII, John XXIII, John XX II and Benedict XVI and Francis. She personal note about her faith often features in her annual Christmas message broadcast to the Commonwealth. She her2000 she she spoke about her millennium significance of her millennium marking the 2000th anniversary of the birth of Jesus:']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]
2025-02-06 15:25:55.822 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:25:55.834 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:25:55 - INFO - absl -   Using default tokenizer.
2025-02-06 15:25:55.841 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:25:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:25:56.805 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:25:57.384 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:25:57.384 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:25:57.385 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 15:25:58.699 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:25:58.714 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:25:58 - INFO - absl -   Using default tokenizer.
2025-02-06 15:25:58.717 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:25:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:25:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  20%|██        | 10/50 [02:00<07:28, 11.20s/it]02/06/2025 15:26:01 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:26:01 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:01 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:26:01 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:01 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:26:02 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:26:02 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:26:02 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:26:03.997 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:04.621 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:04.622 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:04.622 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__628385_161358
Base model weight checksum: -10738.13671875
[10; 0] edit/acc: 0.5660377740859985 --> 0.8773584961891174
[10; 0] target: 
 ['"Head Crusher" was the first single from American heavy metal band Megadeth\'s twelfth studio album, titled "Endgame", which was released on September 15, 2009. The single was released on July 7, 2009. According to "Roadrunner Records" official website, a download of "Head Crusher" was available for 24 hours on July 7 (the availability of download ended the morning of July 8). The track was previously available for listening by calling Dave Mustaine\'s number, through the TheLiveLine.com, a service he launched that enables musicians to connect to their audience over the phone. The song was played live for the first time at the "Canadian Carnage Tour" on June 24, 2009. On December 3, 2009, it was announced on the official Megadeth homepage that "Head Crusher" had been nominated for "Best Metal Performance" for the 52nd annual Grammy Awards, the band\'s first nomination in 13 years, since "Trust".']
[10; 0] edit/gen: 
 ['QuestionAn of" is the first song from the rock metal band Blackadeth\'s elfth studio album, Super "Dgame". which was released on September 25, 2010. The song was released on July 21, 2009, The to theEndrunner Records", the website, the music of theHead Crusher" was available for purchase99 hours on July 7,the day of the was on next of July 8, The song was also available on download on fans the Mustaine\'s cell at which the Meg MegWire.com website a live that started in allows fans to interact with fans fans. the phone. The track was also on on the first time on the MegRock Rockage Tour" on July 26, 2009. The July 1, 2009, the was announced that the band Megadeth website that theHead Crusher" would been released for aBest Metal Performance" at the 52nd Annual Grammy Awards. but first\'s first nomination in the12 years. and "Peace"'] 
 --> 
 ['"Head Crusher" is the first single from American heavy metal band Megadeth\'s twelfth studio album, " "Endgame", which was released on July 15, 2009. The single was released on July 7, 2009. The to "Roadrunner Records" official website, " download of "Head Crusher" was available for 24 hours on July 7,the first of download ended at morning of July 8). The track was played available for listening by calling  Mustaine\'s number,  the "LiveLine.com service a service that launched in enables musicians to connect to their audience over the phone. The song was played live for the first time on the "Canadian Carnage Tour" on July 24, 2009. The December 7, 2009, " was announced that the official "adeth\'s that "Head Crusher" was been nominated for "Best Metal Performance" for the 52nd annual " Awards, the band\'s first nomination in 13 years. since "Trust".']
Base model weight checksum: -21329.36328125
[10; 1] edit/acc: 0.4700855016708374 --> 0.8547009229660034
[10; 1] target: 
 ['Megadeth has sold over 38 million records worldwide, earned platinum certification in the United States for five of its fifteen studio albums, and received twelve Grammy nominations. Megadeth won its first Grammy Award in 2017 for the song "Dystopia" in the Best Metal Performance category. The band\'s mascot, Vic Rattlehead, regularly appears on album artwork and live shows. The group has drawn controversy for its music and lyrics, including album bans and canceled concerts; MTV refused to play two of the band\'s music videos that the network considered to condone suicide.']
[10; 1] edit/gen: 
 ['"haneth\'s announced out 50 million albums worldwide, and  and in  U States, " albums their albums albums albums, and has a Grammy nominations. Theadeth is " first and for for 2002 for " album "Trustystopia" from the " Metal Performance category. The band\'s first, " Rattlehead, was appears in the covers and in shows. The band\'s released comparisons for its political videos lyrics, including for covers in the concerts. the News to air " of the band\'s videos videos, were network deemed " beone violence.'] 
 --> 
 ['"adeth has sold over 38 million records worldwide and earned platinum certification in the United States for five of its fifteen studio records and received received Grammy Grammy nominations. Megadeth won its first Grammy Award in 2017 in best song "Dystopia" in the Best Metal Performance category. MTV group won mascot, Vic Rattlehead regularly regularly appears on album artwork and live shows and The group won drawn controversy and album music and lyrics and including album bans and canceled concerts; MTV refused to play two of the band\'s music videos that the network considered to condone suicide.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 15:26:05.289 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:05.297 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:05 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:05.300 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:26:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:26:06.466 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:07.070 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:07.070 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:07.071 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:26:08.393 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:08.408 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:08 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:08.411 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:26:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  22%|██▏       | 11/50 [02:11<07:12, 11.09s/it]02/06/2025 15:26:12 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:26:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:26:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:26:13 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:26:13 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:26:13 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:26:14.879 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:15.477 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:15.478 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:15.478 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__3257_2998
Base model weight checksum: -10738.13671875
[11; 0] edit/acc: 0.3888888955116272 --> 0.944444477558136
[11; 0] target: 
 ["A special tribute to Simon Cowell was presented in the finale for his final season with the show. Many figures from the show's past, including Paula Abdul, made an appearance."]
[11; 0] edit/gen: 
 ['Question  thanks to the Cowell\n paid to the form of the  season on the judges.\n The of from the show’s history were including Cow Abdul, Randy appearances appearance.'] 
 --> 
 ["A special tribute to Simon Cowell was presented in the finale for his final season with the show. Many figures from the show's past, including made Abdul, made an an."]
Base model weight checksum: -20902.73828125
[11; 1] edit/acc: 0.47328245639801025 --> 0.5419847369194031
[11; 1] target: 
 ["In season eight, Latin Grammy Award-nominated singer–songwriter and record producer Kara DioGuardi was added as a fourth judge. She stayed for two seasons and left the show before season ten. Paula Abdul left the show before season nine after failing to agree terms with the show producers. Emmy Award-winning talk show host Ellen DeGeneres replaced Paula Abdul for that season, but left after just one season. On January 11, 2010, Simon Cowell announced that he was leaving the show to pursue introducing the American version of his show The X Factor to the USA for 2011. Jennifer Lopez and Steven Tyler joined the judging panel in season ten, but both left after two seasons. They were replaced by three new judges, Mariah Carey, Nicki Minaj and Keith Urban, who joined Randy Jackson in season 12. However both Carey and Minaj left after one season, and Randy Jackson also announced that he would depart the show after twelve seasons as a judge but would return as a mentor. Urban is the only judge from season 12 to return in season 13. He was joined by previous judge Jennifer Lopez and former mentor Harry Connick, Jr.. Lopez, Urban and Connick, Jr. all returned as judges for the show's fourteenth and fifteenth seasons."]
[11; 1] edit/gen: 
 ["A the, the American winner-winningominated singer andsongwriter and producer producer, MGuardi, presented to a special judge. Many was with the seasons. was the show in the nine. Many Abdul, the show in the eight. a to get on with the show’s. Made Award-winning actress show host made DeGeneres was Abdul Abdul. the season. and was the the one season. Made the 14, 2012, made Cowell was that made was leaving the show. focus other the show show of the show, show Factor. the show. the2011. Many Lopez, made Tyler, the show panel. the. with left left the the seasons. Made were replaced by made figures judges made madeiah Carey, madei Minaj, made Urban. who joined the Jackson, the eleven11. Many, Mar and Minaj left the one season. and Urban Jackson was left that he was be the show in the seasons. a judge. would continue in a special. Made, the show judge to the 12 to remain. the 13. Many was replaced in made judge made Lopez, made made made Connick Jr Jr. Many and Conn, Connick, Jr. were left in judges in the show's finalteenth season fifteenth seasons."] 
 --> 
 ['  special, Simon Grammy Award -ominated and and andwriter and American producer A DioGuardi is " as a member member to She is on two seasons and, after program after being ten, In Abdul, after program after season, failing to agree terms of the executive\'s,<|end_of_text|> and and, and host, DeGeneres replaced American American after season season and but American after failing two season,<|end_of_text|> December 11, 2010, co Cowell, that vo would leaving the show after pursue a the final version of the final,onda-F. the U for 2011. According H and co Tyler joined the  panel after season ten, but both left after two seasons, returning were replaced by co new members, coiah Carey, coi Minaj and co Urban, who joined the Jackson and season 11,<|end_of_text|>, Carey and Urbanaj X after two season, and Urban Jackson and announced after he would depart after program after season seasons, a judge, would return as a co.<|end_of_text|> and currently only American from season 12 to return as season 13,<|end_of_text|> is joined by returning membersiddi H and returned co and Connick, Jr.. In and Urban and ick, Jr. are returned as a and season\'s teenth and fifteenth seasons,']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 15:26:16.145 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:16.153 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:16 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:16.156 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:26:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:26:16.868 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:17.550 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:17.551 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:17.551 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:26:18.876 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:18.909 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:18 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:18.915 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:26:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  24%|██▍       | 12/50 [02:22<06:51, 10.82s/it]02/06/2025 15:26:23 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:26:23 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:23 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:26:23 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:23 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:26:24 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:26:24 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:26:24 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:26:25.540 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:26.143 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:26.143 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:26.144 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__317528_774871
Base model weight checksum: -10738.13671875
[12; 0] edit/acc: 0.4962962865829468 --> 0.9333333373069763
[12; 0] target: 
 ["The X Factor is the tenth studio album by English heavy metal band Iron Maiden, released on 2 October 1995 through EMI. It is the first of two albums by the band to include Blaze Bayley, formerly of Wolfsbane, as vocalist, replacing Bruce Dickinson who left the band following their previous tour to pursue a solo career. The album takes a darker tone than the band's first nine releases, due to the lyrics being based on personal issues surrounding Steve Harris at the time, who was in the midst of a divorce. This is reflected in the cover artwork, which graphically depicts the band's mascot, Eddie, being vivisected by machine."]
[12; 0] edit/gen: 
 ["Question -  a most series album by American rock metal band Iron Maiden. released on 26 November 1995 by EMI. It was the band Iron the studio released Iron band to be the Bayley as who of thefsbane, as a. and Bruce Dickinson. had the band in the  album. promote a solo career. The album was its more, than its band's previous two studio, with to the departure and more on the experiences and the Harris, the time. and was suffering a middle of a divorce. The was also in the album artwork, which featuresically depicts the band's front Eddie Eddie, being attackedisected by a guns"] 
 --> 
 ["The X Factor is the tenth studio album by English heavy metal band Iron Maiden, released on 2 October 1995 through EMI. It is the first of two albums by the band to include the Bayley, replacing of Wolfsbane, replacing vocalist, replacing Bruce Dickinson, left the band following their  tour to pursue a solo career. The album is a darker tone than the band's first nine releases, due to the lyrics being based on personal issues surrounding the Harris, the time, who was in the midst of a divorce. This is reflected in the cover artwork, which graphically depicts the band's mascot, the, being vivisected by machine."]
Base model weight checksum: -19996.453125
[12; 1] edit/acc: 0.6271186470985413 --> 0.7966101765632629
[12; 1] target: 
 ["Iron Maiden are an English heavy metal band formed in Leyton, East London, in 1975 by bassist and primary songwriter Steve Harris. The band's discography has grown to thirty-nine albums, including sixteen studio albums, twelve live albums, four EPs, and seven compilations."]
[12; 1] edit/gen: 
 ["The Man's the English heavy metal band from in ton, London London, in 1975 by Bruceist Steve vocalist songwriter Steve Harris, The band has lineography includes included to include-nine albums, including four number albums, four live albums, two compilations, and two compilations."] 
 --> 
 ['The Maiden are an English " metal band formed by Leyton, East London, in July1975 by bassist and primary primary bass Harris. The disc\'s discography has grown to thirty-nine studio, including seventeen studio albums, twelve live albums, twelve EPs, and nine studioations.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]
2025-02-06 15:26:26.809 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:26.824 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:26 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:26.831 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:26:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:26:27.135 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:27.724 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:27.725 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:27.725 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 15:26:29.038 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:29.053 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:29 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:29.056 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:26:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  26%|██▌       | 13/50 [02:31<06:29, 10.52s/it]02/06/2025 15:26:32 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:26:32 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:32 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:26:32 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:32 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:26:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:26:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:26:34 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:26:34.696 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:35.303 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:35.304 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:35.304 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__85865_86706
Base model weight checksum: -10738.13671875
[13; 0] edit/acc: 0.5098039507865906 --> 0.8725490570068359
[13; 0] target: 
 ["Men's Singles 2017 Australian Open Champion Roger Federer Runner - up Rafael Nadal Final score 6 -- 4, 3 -- 6, 6 -- 1, 3 -- 6, 6 -- 3 Details Draw 128 (16 Q / 8 WC) Seeds 32 Events Singles men women boys girls Doubles men women mixed boys girls Legends men women mixed WC Singles men women quad WC Doubles men women quad ← 2016 Australian Open 2018 →"]
[13; 0] edit/gen: 
 ["Question's Basketball -19- Open\n\n Federer has-up Up Nov Nadal\nist 6- 4\n 6 -- 6, 6 -- 3\n 6 -- 6, 6 -- 3,\n\n1,128 seeds) 4 WC) Surface 1       les men women boys men girls Doub men women boys boys  men women boys boys Singlesles men women quad WC 2016  Open 2018 Australian"] 
 --> 
 ["The's Singles 2017 Australian Open Champion Roger Federer Runner - up Rafael Nadal  score 3 -- 3, 3 -- 6, 3 -- 3, 3 -- 6, 3 -- 3, Draw 128 (16 Q / 8 WC / Seeds 32 Events Singles men women boys girls Doubles men women mixed mixed girls Legends men women mixed quad Singles men women quad WC Doubles men women quad WC 2016 Australian Open 2018 Australian"]
Base model weight checksum: -18278.11328125
[13; 1] edit/acc: 0.5240174531936646 --> 0.847161591053009
[13; 1] target: 
 ['The Djokovic -- Federer rivalry is a tennis rivalry between two professional tennis players, Novak Djokovic and Roger Federer. They have faced each other 45 times with Djokovic leading 23 -- 22. This includes a record 15 Grand Slam matches, four of which were finals, plus a record ten semifinals. Both players have beaten the other in each of the four Grand Slam tournaments. Federer dominated during their early slam matches, but Djokovic now has a 9 -- 6 lead in Grand Slam matches, including eight wins in the last ten meetings. A notable aspect of the rivalry is their ability to beat each other on any given day, including Grand Slam play, making it one of the most competitive and evenly matched rivalries in the Open Era. To date Federer is the only man to have beaten Djokovic in all four majors, and likewise Djokovic is the only man to have beaten Federer in all four majors. Both men accomplished this after having beaten each other at Wimbledon. Both players are generally considered to be the two greatest hard court players in the open era.']
[13; 1] edit/gen: 
 ['The ibovic family Rafaeler -- is back classic match between Roger of tennis players, Rogerak Djokovic and Roger Federer, The are been each other in32 times, Djokovic winning 28 -- 22. The is   17 Grand Slam finals,  of which were the. and    ATPinals. The players have won the other in  of their  Grand Slam tournaments, Theer has the the  years years, but Djokovic has has  6 -- 6 lead in the Slam finals.   -- in the final  years. The   of the rivalry is that  to win the other in the surface day. as  Slam finals. and it one of the most exciting rival exciting matched rivalries in the sport Era. The date,er has  only player to beat won Djokovic in all four Grand, and the,okovic is the only man to have beaten Federer in all four majors. The players have this feat  won Rafael other in the  The players have also considered to be the greatest best playerscourt players of the history era,'] 
 --> 
 ['The Australianokovic - Federer rivalry is a tennis rivalry between two professional tennis players, Novak Djokovic and Roger Federer. Both have beaten each other on45 times with Djokovic leading the23 -- 22. Both includes a record ten15 Grand Slam matches, including of which were finals. plus a record ten Grandinals. Both players have beaten each other in each of their four Grand Slam tournaments. Federer dominated Dj their early matches matches, including Djokovic now has beaten record9 -- 6 lead in Grand Slam matches. including eight wins in each last ten meetings. Feder record aspect of their rivalry is their ability to beat each other on any given day. including Grand Slam matches, making it one of the most competitive and evenly matched rivalries in the Open Era. Both date Federer is the only man to beat beaten Djokovic in all four majors. including Dj Djokovic is the only man to have beaten Federer in all four majors. Both players have this after having beaten each other at Wimbledon. Feder players have generally considered to be the two greatest players court players in the open era.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 15:26:35.988 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:35.997 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:35 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:36.000 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:26:36 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:26:37.128 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:37.717 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:37.717 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:37.718 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 15:26:39.039 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:39.054 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:39 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:39.057 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:26:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:26:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  28%|██▊       | 14/50 [02:46<07:05, 11.83s/it]02/06/2025 15:26:47 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:26:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:26:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:26:48 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:26:48 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:26:48 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:26:49.858 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:50.501 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:50.501 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:50.502 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__54758_446818
Base model weight checksum: -10738.13671875
[14; 0] edit/acc: 0.5567010045051575 --> 0.7422680258750916
[14; 0] target: 
 ['Alexander Hamilton (51 articles: No. 1, 6 -- 9, 11 -- 13, 15 -- 17, 21 -- 36, 59 -- 61, and 65 -- 85) James Madison (29 articles: No. 10, 14, 18 -- 20, 37 -- 58 and 62 -- 63) John Jay (5 articles: No. 2 -- 5 and 64).']
[14; 0] edit/gen: 
 ['Question,,175))\n. 1)\n 2, No51, 11, 20, 15 -- 17, 19 -- 23, 38 -- 60,  63)\n 67)\n was Madison (1 articles: No. 1 -- 12 -- 16, 20, 22 -- 39, 62 -- 64) Thomas Jay (1 articles: No. 4, 4) 7 --'] 
 --> 
 ['The Hamilton (51 articles:. 1 -- 2 -- 9 -- 11 -- 13 -- 15 -- 17 -- 21 -- 36 -- 59 -- 61 --  63 -- 85 -- James Madison (5 --: No. 10 -- 14 -- and58 -- 58 -- and37 -- 58 -- 62 -- 63 -- John Jay (5 articles: No. 2 -- 5 -- 64 --']
Base model weight checksum: -16206.6259765625
[14; 1] edit/acc: 0.4431138038635254 --> 0.8742515444755554
[14; 1] target: 
 ["William Stephen Hamilton (August 4, 1797 – October 9, 1850), a son of Alexander Hamilton and Elizabeth Schuyler Hamilton, was an American politician and miner who lived much of his life in the U.S. state of Illinois and territorial Wisconsin. Hamilton was born in New York, where he attended the United States Military Academy before he resigned and moved to Illinois in 1817. In Illinois he lived in Springfield and Peoria and eventually migrated to the lead-mining region of southern Wisconsin and established Hamilton's Diggings at present-day Wiota. Hamilton served in various political offices and as a commander in two Midwest Indian Wars. In 1849 he moved to California on the heels of the California Gold Rush. He died in Sacramento, most likely of cholera, in October 1850."]
[14; 1] edit/gen: 
 ['The H ( (191 5, 1857 – May 5, 1856) was native of John Hamilton and Elizabeth Schuyler,, was born American lawyer and lawyer. served in of his life in the West.S. state of New. the Kansas. He was a in New York City and his was the public States Military Academy at being was in went to the. 1819. He, was in the, inoria, was settled to Wisconsin Wisconsin minesing region of the Wisconsin. became a, Leadiggings, the-day Hamiltonota, He was in the local offices in was a U of the wars wars wars. He 1838, was to the and the California of the Gold Gold Rush, He died in, California likely from aera.\n and  1850.\n'] 
 --> 
 ["Thomas Hamilton Hamilton (August 9, 1797 – October 9, 1850), an son of Alexander Hamilton and Elizabeth Schuyler Hamilton, was an American politician and miner who lived much of his life in California U.S. state of Illinois and territorial Wisconsin and Hamilton lived born in New York and where he attended the United States Military Academy before he resigned and moved to Illinois in 1819. Hamilton Illinois he lived in Springfield and Peoria and eventually migrated to the lead-mining region of southern Wisconsin and established Hamilton's Diggings at present-day presentota. Hamilton lived in various political offices and as a commander in the Indian Indian Wars. Hamilton 1849 he moved to California and the heels of the California Gold Rush and Hamilton lived in Sacramento and California likely of cholera, in October 1850."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]
2025-02-06 15:26:51.164 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:51.173 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:51 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:51.176 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 15:26:51.180 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:52.137 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:52.137 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:52.138 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.52it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.54it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.54it/s]
2025-02-06 15:26:53.439 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:53.454 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:53 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:53.457 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  30%|███       | 15/50 [02:54<06:16, 10.74s/it]02/06/2025 15:26:55 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:26:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:26:56 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:26:56 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:26:57 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:26:57 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:26:57 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:26:58.508 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:26:59.117 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:26:59.118 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:26:59.118 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__647869_2702
Base model weight checksum: -10738.13671875
[15; 0] edit/acc: 0.5106382966041565 --> 0.9219858050346375
[15; 0] target: 
 ['"Timantit on ikuisia" is a song by Finnish rapper Cheek. The song serves as the second single from Cheek\'s tenth studio album "Kuka muu muka", released on 20 September 2013. The single and the accompanying music video, directed by Jere Hietala, were released on 9 August 2013. "Timantit on ikuisia" became the Cheek\'s eighth number-one single on the Finnish Singles Chart, where it debuted at number one. On 23 August 2013, it also became the first-ever song by a Finnish artist to rank among the 100 most-played songs of the day on Spotify.']
[15; 0] edit/gen: 
 ['QuestionAnothyy" theäu" - a Finnish by Finnish singer Timeky It song was as the lead single from Cheek\'s debut studio album,Ku onist onenean which on 28 October 2018. The song was album album music video was directed by Juss Kietan, was released on 20 September 2013. TheTimantit on ikuisia" is Che firstek\'s first top one single on the Finnish Singles Chart, and it stayed at number  on The 9 September 2013, the was became Che Che Che single to a Finnish rapper to top number the top100 most populared songs on all year on the.'] 
 --> 
 ['"Timantit on ikuisia" is a song by Finnish rapper Cheek. The single, as the second single from theek\'s tenth studio album "Kuka muu muuka", released on 23 August 2013. The single, the accompanying video video, directed by Jere Hietala, were released on 23 August 2013. "Timantit on ikuisia" became the firstek\'s eighth number-one single on the Finnish Singles Chart. where it debuted at number one. On 23 August 2013, it also became the first-ever song by a Finnish artist to rank among the 100 most-played songs of the day on Spotify.']
Base model weight checksum: -19541.6328125
[15; 1] edit/acc: 0.4552058279514313 --> 0.6731235384941101
[15; 1] target: 
 ['West\'s middle-class background, flamboyant fashion sense and outspokenness have additionally set him apart from other rappers. Early in his career, he was among the first rappers to publicly criticize the preponderance of homophobia in hip hop. The sales competition between rapper 50 Cent\'s Curtis and West\'s Graduation altered the direction of hip hop and helped pave the way for new rappers who did not follow the hardcore-gangster mold. Rosie Swash of The Guardian viewed the sales competition as a historical moment in hip-hop, because it "highlighted the diverging facets of hip-hop in the last decade; the former was gangsta rap for the noughties, while West was the thinking man\'s alternative." Rolling Stone credited West with transforming hip hop\'s mainstream, "establishing a style of introspective yet glossy rap [...]", and called him "as interesting and complicated a pop star as the 2000s produced—a rapper who mastered, upped and moved beyond the hip-hop game, a producer who created a signature sound and then abandoned it to his imitators, a flashy, free-spending sybarite with insightful things to say about college, culture and economics, an egomaniac with more than enough artistic firepower to back it up." His 2008 album 808s & Heartbreak polarized both listeners and critics upon its release, but was commercially successful and impacted hip hop and pop stylistically, as it laid the groundwork for a new wave of artists who generally eschewed typical rap braggadocio for intimate subject matter and introspection, including Frank Ocean, The Weeknd, Drake, Future, Kid Cudi, Childish Gambino, Lil Durk, Chief Keef, and Soulja Boy. According to Ben Detrick of XXL magazine, West effectively led a new wave of artists, including Kid Cudi, Wale, Lupe Fiasco, Kidz in the Hall, and Drake, who lacked the interest or ability to rap about gunplay or drug-dealing.']
[15; 1] edit/gen: 
 ['" Virginia  school voters is hisboyant style sense and penchant personality have made made him apart from the politiciansappers. He life his career, he was a the first rappers to make discuss the drugeminance of violenceophobia in the-hop. He West of between the and50 Cent and G Jackson West\'s Guation, the course of the hop. the to the way for the artistsappers to would not have the formula gangangsta style. In Oash, the Sun said the album of between a " turning, the hop, saying it "markeded the factgent paths of the-hop culture the  decade". the rise was asta,, the massesoughties, the the\'s a first man\'s gang". In Stone magazine the with " the-hop from " image andmakinging a new that musicpective, aggressive, that]. and " him "the much a influential a figure star as there music210s have". man who\'s the and to and out the the gang-hop game." and man who made a new sound that a a it, make ownitators, a singer, flam-wheelending,barite who a lyrics to say about the and love and the." a artistotaniacal a than a self integrity to make up up." In 2003 album,808s & Heartbreak,ized the the and critics. its release. and it a successful. won the hop culture pop musicically. and well was the groundwork for the new generation of artists who would eschewed the gang productionaggadocio in a, matter. apection. and the Ocean, Justin Weeknd, and, and, and Cudi, andish Gambino, and Waynek, and Keef, and moreja Boy. In to the Ratrick of TheL,, "\'s " the " wave of artists who " the Cudi, whoiz, andpe Fiasco, and Ink in the Hall, and The, who " the " in ability to " about the violence and gang dealingaling.'] 
 --> 
 ['"\'s  class is and includingboyant fashion sense and outspokenness have helped set him apart from other rappers who Early in his career, West additionally among the first rapperappers to publicly criticize rapper preponderance of homophobia in hip hop and He rapper competition between rapper.50 Cent and Curtis and West\'s "uation additionally rap rap of hip hop and helped pave the way for rapper rapappers who lacked not follow typical typical rapangster rap of West Swash, The Guardian viewed West album competition as a historical moment in hip hop\'s because rapper washighlighted rap diverging facets of rap -\'s pop  decade, rap rapper gang gangster rap versus predominantly noughties while while rapper was glossy rapper man\'s rapper rapper According Stone credited West\'s transforming rap hop into mainstream image includingestablishing rapper style and rappection rap glossy rap anchored]", including helped him "as interesting and complicated as pop star as there music2000s produced." rapper who could both thenped and moved beyond rap rap-hop game plan who rapper who created something signature sound and then abandoned it for pioneer imitators, a rapper pop flashy-spending sybarite who insightful things to say about everything and economics and economics." and egomaniac with artistic than enough artistic firepower to back it up." According 2008 album 808s & Heartbreak alsoized listeners listeners and critics, release release, but also commercially successful and impacted pop hop in pop stylesically and including evidenced established the groundwork for a new wave of rapper who did eschewed rap rap rapaggadocio for intros subject matter and introspection. including Frank Ocean, Drake Weeknd, Drake, Lil, Lil Cudi, Lilish Gambino, Lil Durk, Lil Keef, Lil Lilja Boy. According to XX Detrick of XXL magazine, West established abandoned a new wave of rapper who including Drake Cudi, Drakeale, Drakepe Fiasco, Drakez in the Hall, Drake Drake, who generally rapper rapper or ability to rap about themselvesplay or drug-dealing.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]
2025-02-06 15:26:59.832 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:26:59.850 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:26:59 - INFO - absl -   Using default tokenizer.
2025-02-06 15:26:59.856 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:27:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:27:02.654 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:27:03.265 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:27:03.266 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:27:03.266 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:27:04.603 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:27:04.621 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:27:04 - INFO - absl -   Using default tokenizer.
2025-02-06 15:27:04.629 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:27:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  32%|███▏      | 16/50 [03:12<07:15, 12.81s/it]02/06/2025 15:27:13 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:27:13 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:27:13 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:27:13 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:27:13 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:27:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:27:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:27:14 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:27:15.312 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:27:16.315 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:27:16.316 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:27:16.316 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__159827_9449
Base model weight checksum: -10738.13671875
[16; 0] edit/acc: 0.5428571701049805 --> 0.9904762506484985
[16; 0] target: 
 ['The Church of St. Leodegar (German: St. Leodegar im Hof or Hofkirche St. Leodegar) is a Roman Catholic church in the city of Lucerne, Switzerland. It was built in parts from 1633 to 1639 on the foundation of the Roman basilica which had burnt in 1633. This church was one of the few built north of the Alps during the Thirty Years War and one of the largest art history rich churches of the German late renaissance period.']
[16; 0] edit/gen: 
 ["Question  of England. Johnopgar,St: St. Leodegar-K K) Stkirche). Leodegar) is a Roman Catholic church in the city of Freerne, Switzerland. It is built in the between 1334 to 1659, the site of a formeresica of was been down 1633. The church is built of the first buildings in of the Alps in the Bar Years'. is of the few churches works monuments churches in the -speaking barnaissance.."] 
 --> 
 ['The Church of St. Leodegar (German: St. Leodegar im Hof or Hofkirche St. Leodegar) is a Roman Catholic church in the city of Lucerne, Switzerland. It was built in parts from 1633 to 1639 on the foundation of the Roman basilica which had burnt in 1633. This church was one of the few built north of the Alps during the Thirty Years War. one of the largest art history rich churches of the German late renaissance period.']
Base model weight checksum: -18227.26953125
[16; 1] edit/acc: 0.5079365372657776 --> 0.968254029750824
[16; 1] target: 
 ['The Alps are a habitat for 30,000 species of wildlife, ranging from the tiniest snow fleas to brown bears, many of which have made adaptations to the harsh cold conditions and high altitudes to the point that some only survive in specific micro-climates either directly above or below the snow line.']
[16; 1] edit/gen: 
 ['The Church are a mountain for a1,000 species of plants, including from the smallestiniest of leas to the bears. and of which are been the to the harsh environment of of the altitudes of survive north of they species survive in the areasclimates. in on or below the snow line.'] 
 --> 
 ['The Alps are a habitat for 30,000 species, wildlife, ranging from t tiniest snow fleas to brown bears, many of which have made adaptations to the harsh cold conditions and high altitudes to the point that some only survive in specific micro-climates either directly above or below the snow line.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]
2025-02-06 15:27:17.284 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:27:17.293 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:27:17 - INFO - absl -   Using default tokenizer.
2025-02-06 15:27:17.296 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:27:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:27:17.996 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:27:18.611 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:27:18.612 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:27:18.612 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:27:19.955 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:27:19.970 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:27:19 - INFO - absl -   Using default tokenizer.
2025-02-06 15:27:19.973 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:27:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  34%|███▍      | 17/50 [03:23<06:42, 12.20s/it]02/06/2025 15:27:23 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:27:23 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:27:23 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:27:24 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:27:24 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:27:25 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:27:25 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:27:25 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:27:26.171 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:27:26.809 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:27:26.810 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:27:26.810 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__546986_565529
Base model weight checksum: -10738.13671875
[17; 0] edit/acc: 0.52173912525177 --> 0.8695652484893799
[17; 0] target: 
 ['Victoria Stadnik (born 25 November 1979 in Odessa, USSR) is an Individual Rhythmic Gymnast who competed at the 1995 and 1996 World Rhythmic Gymnastics Championships.']
[17; 0] edit/gen: 
 ["Question'sler\nborn 198 March 1989) Moscowessa, Ukraine) is a Ukrainian rhythhythmic Gymnastics from represented for the 2006 World 1997 World Championshipshythmic Gymnastics Championships."] 
 --> 
 ['The Stadnik (born 25 November 1979 in Odessa, USSR) is an Individual Rhythmic Gymastast who competed at the 1995 and 1996 and Rhythmic Gymastast R.']
Base model weight checksum: -22176.55078125
[17; 1] edit/acc: 0.3139534890651703 --> 0.9069767594337463
[17; 1] target: 
 ["Moldavanka is a historical part of Odessa in the Odessa Oblast (province) of southern Ukraine, located jointly on Malinovskiy and Primorskiy city districts. Before 1820 a settlement just outside Odessa which later engulfed it. Until the 20th century the neighborhood was considered a low-income/high-crime part of the town and was famous for its workers' shacks."]
[17; 1] edit/gen: 
 ['Theolecular Removalnik ( an small village of Odessa, USSR USSRessa regionast,region) of USSR USSR. and on with theayaovkaaya and Odorskyy ( districts. The the1949, part of on theessa, was becameed the, The  19th century, area was known a part-class district crimeime area of the city. was known for its numerous’ housingant.'] 
 --> 
 ["Koldavanka is a historical part of theessa in the essa Oblast (province) of Ukraine Ukraine, located jointly on Malinovskiy and Primorskiy city districts. Until 1820 the settlement just outside theessa which later engulfed it. Until the 20th century a neighborhood was considered a low-income/high-crime part of the town and was famous for its workers' shacks."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 15:27:27.494 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:27:27.503 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:27:27 - INFO - absl -   Using default tokenizer.
2025-02-06 15:27:27.505 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:27:28 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:28 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:28 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:27:28.629 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:27:29.223 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:27:29.224 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:27:29.224 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:27:30.562 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:27:30.589 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:27:30 - INFO - absl -   Using default tokenizer.
2025-02-06 15:27:30.594 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:27:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  36%|███▌      | 18/50 [03:34<06:17, 11.81s/it]02/06/2025 15:27:34 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:27:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:27:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:27:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:27:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:27:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:27:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:27:36 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:27:37.209 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:27:37.811 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:27:37.812 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:27:37.812 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__53030_79070
Base model weight checksum: -10738.13671875
[18; 0] edit/acc: 0.6083915829658508 --> 0.9510489702224731
[18; 0] target: 
 ["Anne of Green Gables is a 1908 novel by Canadian author Lucy Maud Montgomery (published as L.M. Montgomery). Written for all ages, it has been considered a classic children's novel since the mid-twentieth century. Set in the late 19th century, the novel recounts the adventures of Anne Shirley, an 11 - year - old orphan girl who is mistakenly sent to Matthew and Marilla Cuthbert, a middle - aged brother and sister who had intended to adopt a boy to help them on their farm in the fictional town of Avonlea on Prince Edward Island. The novel recounts how Anne makes her way with the Cuthberts, in school, and within the town."]
[18; 0] edit/gen: 
 ["Question Hath Green Gables: a Canadian1988 novel by Canadian author Lucy Maud Montgomery.187 in L.. Montgomery in It in children ages, it is been translated a classic of's novel since its -entieth century. The in Prince fictional 19th century in the story tells the life of a Shirley, a orphan11-yearyear-old old orphan who who is sent sent to live C Marilla Cuthbert, a wid-aged aged brother and sister who have hoped to adopt a boy. help with on their farm in Prince Canadian town of Avonlea, Prince Edward Island. Anne novel follows the Anne's friends way to the Cuthberts, and particular, and in the community,"] 
 --> 
 ["The of Green Gables is a 1908 novel by Canadian author Lucy Maud Montgomery (published as L.M. Montgomery). Written for all ages, the recounts been considered a classic children's novel since the mid-twentieth century. Set in the late 19th century, the novel recounts the adventures of Anne Shirley, an 11 - year - old girl girl who is mistakenly sent to Matthew and Marilla Cuthbert, a middle - aged brother and sister who had intended to adopt a boy to help them on their farm in the fictional town of Mapleonlea on the Edward Island. The novel recounts how Anne makes her way with the Cuthberts, and school, and within the town."]
Base model weight checksum: -19945.77734375
[18; 1] edit/acc: 0.654411792755127 --> 0.9558823704719543
[18; 1] target: 
 ["Prince Edward Island (PEI or P.E.I.; French: Île - du - Prince - Édouard) is a province of Canada consisting of the island of the same name, and several much smaller islands. Prince Edward Island is one of the three Maritime Provinces and is the smallest province in both land area and population. It is part of the traditional lands of the Mi'kmaq, and became a British colony in the 1700s and was federated into Canada as a province in 1873. Its capital is Charlottetown. According to the 2016 census, the province of Prince Edward Island has 142,907 residents."]
[18; 1] edit/gen: 
 ["The Harry Island isPEI) P.E.I.) French: Île-duP - P- Édouard) is a Canadian of Canada located of the island of the same name. as the smaller smaller islands. The Edward Island is the of the three Maritime provincesvinces of is the smallest province in Canada area area and population. The is the of the Atlantic First of the Mi'kmaq people the is a part colony in  late180s. a aated with the in a province in 1873. The capital is Charlottetown. The to the 2016 census, the population's Prince Edward Island had a150,000 residents,"] 
 --> 
 ["Nich Edward Island (PEI or P.E.I.; French: Princele - du - Prince - Édouard) is a province of Canada consisting of the island of Prince same name and and several much smaller islands. Prince Edward Island is part of the three Maritime Provinces and is part smallest province in both land area and population. It is part of the traditional lands of the Mi'kmaq, and became a British colony in the 1700s and was federated into Canada as a province in 1873. Its capital is Charlottetown. According to the 2016 census, the province of Prince Edward Island has 142,907 residents."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]
2025-02-06 15:27:39.080 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:27:39.089 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:27:39 - INFO - absl -   Using default tokenizer.
2025-02-06 15:27:39.092 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:27:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:27:40.084 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:27:40.685 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:27:40.686 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:27:40.686 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:27:42.015 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:27:42.030 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:27:42 - INFO - absl -   Using default tokenizer.
2025-02-06 15:27:42.033 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:27:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  38%|███▊      | 19/50 [03:46<06:10, 11.96s/it]02/06/2025 15:27:47 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:27:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:27:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:27:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:27:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:27:48 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:27:48 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:27:48 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:27:49.795 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:27:51.064 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:27:51.065 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:27:51.065 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__257846_500443
Base model weight checksum: -10738.13671875
[19; 0] edit/acc: 0.6200000047683716 --> 0.9199999570846558
[19; 0] target: 
 ['"Downtown" is the first single from Neil Young\'s twenty-third studio album, "Mirror Ball". The song was recorded with the members of American rock band Pearl Jam. The song was nominated for Best Rock Song at the 1996 Grammy Awards.']
[19; 0] edit/gen: 
 ['Questionream" is a name single from the Young\'s -f studio album, "Psy Ball". It song was released in the help of Crazy rock band Crazy Jam, The song was released for a Rock Song at the 2000 Grammy Awards.'] 
 --> 
 ['"owntown" is the first single from Neil Young\'s twenty-third studio album " MirrorMirror Ball". The song was recorded with the members of the rock band Pearl Jam. The song was nominated for Best Rock Song at the 1996 Grammy Awards.']
Base model weight checksum: -19554.73046875
[19; 1] edit/acc: 0.4761905074119568 --> 0.9365079998970032
[19; 1] target: 
 ['Scott Alexander Young (April 14, 1918 – June 12, 2005) was a Canadian journalist, sportswriter, novelist and the father of musicians Neil Young and Astrid Young. Over his career, Young wrote 45 books, including novels and non-fiction for adult and youth audiences.']
[19; 1] edit/gen: 
 ['"ish is\nborn 1, 1926 – February 1, 1996) was an Canadian ice and authorswriter, and, author first of the Neil Young and theid Young. The the career, he wrote for15 books, including the, non-fiction, the and children readers.'] 
 --> 
 ['"D Alexander Young (April 12, 1918 – June 12, 2005) was a Canadian journalist, novelistswriter, novelist and the father of musicians Neil Young and Astrid Young. Over his career, Young wrote 45 books and including novels and non-fiction for adult and youth audiences.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 15:27:51.753 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:27:51.767 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:27:51 - INFO - absl -   Using default tokenizer.
2025-02-06 15:27:51.773 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:27:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:27:52.090 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:27:52.670 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:27:52.671 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:27:52.671 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:27:54.002 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:27:54.029 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:27:54 - INFO - absl -   Using default tokenizer.
2025-02-06 15:27:54.035 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:27:54 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:54 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:27:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  40%|████      | 20/50 [03:57<05:46, 11.55s/it]02/06/2025 15:27:58 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:27:58 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:27:58 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:27:58 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:27:58 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:27:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:27:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:27:59 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:28:00.341 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:00.927 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:00.928 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:00.928 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132590_663762
Base model weight checksum: -10738.13671875
[20; 0] edit/acc: 0.5686274766921997 --> 0.9411765336990356
[20; 0] target: 
 ['The Plymouth Prowler, later the Chrysler Prowler, is a retro-styled production car manufactured and marketed from 1997 to 2002 by DaimlerChrysler, based on the 1993 concept car of the same name.']
[20; 0] edit/gen: 
 ['Question  Argrowler is a known Plymouth Prowler, was a rear-styled, car that by marketed by 1994 to 2004 by ChrysleraimlerChrysler. a on the Plymouth1996– car. the same name.'] 
 --> 
 ['The Plymouth Prowler, later the Chrysler rowler, is a retro-styled production car manufactured and marketed from 1997 to 2002. theaimlerChrysler, based on the 1993 concept car of the same name.']
Base model weight checksum: -20824.74609375
[20; 1] edit/acc: 0.4285714328289032 --> 0.9047619104385376
[20; 1] target: 
 ['Walter Percy Chrysler (April 2, 1875 – August 18, 1940) was an American automotive industry executive and founder of Chrysler Corporation, now a part of Fiat Chrysler Automobiles.']
[20; 1] edit/gen: 
 ['Theine P,,188 3, 1905 – March 3, 1952) was an American automobile executive executive. the of the,. based based subsidiary of D, Automobiles.'] 
 --> 
 ['Thealter Percy Chrysler (April 18, 1875 – August 18, 1940) was an American automotive industry executive and founder of Chrysler Chrysler. now a part of Fiat Chrysler Automobiles.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:28:01.601 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:01.610 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:01 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:01.613 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:28:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:28:02.420 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:03.110 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:03.111 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:03.111 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:28:04.447 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:04.462 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:04 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:04.465 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:28:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  42%|████▏     | 21/50 [04:12<06:04, 12.58s/it]02/06/2025 15:28:12 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:28:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:28:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:28:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:28:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:28:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:28:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:28:14 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:28:14.849 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:15.464 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:15.464 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:15.465 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__616216_8600
Base model weight checksum: -10738.13671875
[21; 0] edit/acc: 0.5384615659713745 --> 0.9743589758872986
[21; 0] target: 
 ['Luke Alexander Sears (born 3 April 1980 in Portsmouth, Hampshire) is a former English cricketer. Sears is a right-handed batsman who bowled right-arm medium pace.']
[21; 0] edit/gen: 
 ['Question  is\nborn 198 March 1999) London, Hampshire) is an British English professionalicketer who He was a right-handed batsman and bowlsled right-arm medium pace.'] 
 --> 
 ['The Alexander Sears (born 3 April 1980 in Portsmouth, Hampshire) is a former English cricketer. Sears is a right-handed batsman who bowled right-arm medium pace.']
Base model weight checksum: -21170.71484375
[21; 1] edit/acc: 0.6104651093482971 --> 0.8837209343910217
[21; 1] target: 
 ['Southampton (i/saʊθˈæmptən, -hæmptən/) is the largest city in the ceremonial county of Hampshire on the south coast of England, and is situated 75 miles (121 km) south-west of London and 19 miles (31 km) north-west of Portsmouth. Southampton is a major port and the closest city to the New Forest. It lies at the northernmost point of Southampton Water at the confluence of the River Test and River Itchen, with the River Hamble joining to the south of the urban area. The city, which is a unitary authority, has an estimated population of 253,651. The city\'s name is sometimes abbreviated in writing to "So\'ton" or "Soton", and a resident of Southampton is called a Sotonian.']
[21; 1] edit/gen: 
 ['The Africa,born)əʊ�ˈlmptən) -��mptən/) is a largest city in Hampshire county county of Hampshire, the south coast of England. and the a 3 miles (121 km) south-west of London. 3 miles (31 km) west-west of Portsmouth. It is a major port and a largest major to the Isle Forest, It is on the mouth end tip of the Water, the mouthfluence of the It It and It Itchen. and the River Hamble to the the south. the city area. The city is which is a formerary authority, is a estimated population of 1,000. The city is name is derived spelled to English as SouthamptonSthon" or "Soton". but is former is Southampton is a a "otonian.'] 
 --> 
 ['Kenampton (i/saʊθˈæmptən, -hæmptən/) is a largest city in the ceremonial county of Southampton and the south coast of England, and the situated 75 miles (121 km) south-west of London and 19 miles (31 km) north-west of the. The\'s situated major port and the closest city to the New Forest. The lies at the northernmost point of the Water and the confluence of the River Test and River Hamchen, and the River Hamble joining to the south-west the urban area. The city\'s which lies situated unitary authority, has an estimated population of 253,651. The city\'s name is sometimes abbreviated to writing to "So\'ton" or "Soton", and is resident of Southampton is called a Sotonian.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 15:28:16.155 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:16.163 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:16 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:16.166 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 15:28:16.170 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:16.743 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:16.743 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:16.744 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 15:28:18.061 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:18.094 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:18 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:18.100 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:28:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  44%|████▍     | 22/50 [04:20<05:15, 11.25s/it]02/06/2025 15:28:20 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:28:20 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:28:20 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:28:20 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:28:20 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:28:22 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:28:22 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:28:22 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:28:23.035 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:23.754 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:23.755 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:23.755 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__154226_727337
Base model weight checksum: -10738.13671875
[22; 0] edit/acc: 0.6129032373428345 --> 0.8387096524238586
[22; 0] target: 
 ['ISO 22000 is a standard developed by the International Organization for Standardization dealing with food safety. It is a general derivative of ISO 9000.']
[22; 0] edit/gen: 
 ['Question 90000: a food for by the International Organization for Standardization ( with food safety management It is a management food of the 9001,'] 
 --> 
 ['Management 22000 is a standard developed by the International Organization for Standardization dealing with general.. It is a general derivative of ISO2209001.']
Base model weight checksum: -20359.296875
[22; 1] edit/acc: 0.39024388790130615 --> 0.37804877758026123
[22; 1] target: 
 ['ISO/TC 68 is a technical committee formed within the International Organization for Standardization (ISO), of Geneva, Switzerland, tasked with developing and maintaining international standards covering the areas of banking, securities, and other financial services. As the standards organization under ISO responsible for the development of all international financial services standards, ISO/TC 68 plays a key role in the development and adoption of new technologies in the banking, brokerage and insurance industries. Many of its current work projects involve developing ecommerce standards such as better online security for financial transactions, XML standards for financial transactions and standards to reduce the cost and delays of international financial transactions. The membership of ISO/TC 68, consists of more than 30 organizations assigned by participating national standards bodies plus additional international standards development organizations that work collaboratively toward global financial services standards development.']
[22; 1] edit/gen: 
 ['Management 220 220 is a general committee of by the International Organization for Standardization dealingISO) a which, Switzerland. dealing with developing standard maintaining standard standard. a general of:. finance, and insurance financial services. It a standard developed for the  for the standard of standard standard standards services standards, it isTC 68 is a general role in the standard of maintenance of standard standards. the financial, securities, securities industries. It of the standards standards is are the a,. as ISO ways trading. financial transactions. a-based, financial data, a developed improve the cost of complexity of processing payments transactions. It standard of the/TC 68 is a of a than 220 countries from to the countries committees bodies. a organizations organizations bodies organizations. are withatively with a financial services...'] 
 --> 
 ['The TC 68/ a technical committee ( within the management Observatory for Managementization (I) and which, Switzerland, for with management management management management food in food management of food and food and and other food services and Management of management management for management/ for management management and management management management services management, management/TC 68 management a key role in management management and management of management management and the management and securities and other management and Management management management management management projects include management management management for as management management security and financial transactions and management for for management transactions and management management manage the cost and risk of management financial transactions management Management management of management/TC 68 includes as of management than 30 management from by management management committees bodies ( management management management management management ( work collaboratively with management management management management management and']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]
2025-02-06 15:28:24.735 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:24.745 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:24 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:24.752 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:28:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:28:25.627 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:26.276 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:26.277 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:26.278 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:28:27.621 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:27.636 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:27 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:27.639 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:28:28 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  46%|████▌     | 23/50 [04:32<05:09, 11.47s/it]02/06/2025 15:28:33 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:28:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:28:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:28:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:28:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:28:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:28:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:28:34 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:28:35.505 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:36.141 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:36.142 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:36.142 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__532353_58115
Base model weight checksum: -10738.13671875
[23; 0] edit/acc: 0.5652173757553101 --> 0.9202898740768433
[23; 0] target: 
 ['"If You\'re Reading This" is a song by American country music artist Tim McGraw. The song was first performed at the Academy of Country Music (ACM) awards, which were held in Las Vegas, Nevada and aired May 15, 2007 on CBS. Shortly after McGraw\'s live performance, several radio stations began playing a telecast of the song, boosting it to a debut at number 35 on the "Billboard" Hot Country Songs charts from unsolicited airplay. A remixed version of the live recording was later released to radio as a single, overlapping Tim\'s then-current single, "I Need You", (a duet with wife Faith Hill).']
[23; 0] edit/gen: 
 ['Question you Don Not This, by a song by American singer music artist Tim McGraw. It song was released released on the  of Country Music AwardsACM) Awards in where was held on Las Vegas, Nevada on broadcast on 19, 2005 on CBS. The after theraw\'s performance performance, the radio stations began playing the newcast of the performance. which its to number number at number 10 on the BillboardBillboard" country Country Songs chart. Mayolicited airplay. The weekixed version of the song performance was released released as radio stations a single, pe with McG performance-current single " "Live\'m You." whichwhich duet with Faith Faith Hill)'] 
 --> 
 ['" you\'re Reading This" is a song by American country music artist Tim Mraw. The song was first performed at the Academy of Country Music (ACM) in, which were held in Las Vegas, Nevada, aired May 15, 2007, CBS. A after McGraw\'s live recording, several radio stations began playing a telecast of the song, boosting it to a debut at number 35 on the "Billboard" Hot Country Songs charts, aolicited airplay. A remixed version of the live recording was later released to radio as a single, overlapping Tim\'s then-current single, "I Need You", (a duet with wife wife Hill).']
Base model weight checksum: -20403.69921875
[23; 1] edit/acc: 0.6612902879714966 --> 0.8064515590667725
[23; 1] target: 
 ['Tim McGraw (1993) Not a Moment Too Soon (1994) All I Want (1995) Everywhere (1997) A Place in the Sun (1999) Set This Circus Down (2001) Tim McGraw and the Dancehall Doctors (2002) Live Like You Were Dying (2004) Let It Go (2007) Southern Voice (2009) Emotional Traffic (2012) Two Lanes of Freedom (2013) Sundown Heaven Town (2014) Damn Country Music (2015) The Rest of Our Life (with Faith Hill) (2017)']
[23; 1] edit/gen: 
 ['"\'sraw andborn7) - Fade Moment Too Soon\nC3) I I Need for1995) I (1996) I Woman in the Sun (1998) I You Circus Down (2001) I\'sraw and Wife Dancehall Doctors (2003) I Like You Were Dying (2003) I It Go (2006) I Voice (2009) I Traffic (2011) I Lanes of Freedom (2015) Iown ( Town (2015) I Country Music (2016) I Rest of Our Life (201 wife Hill) (2016)'] 
 --> 
 ['`` McGraw (1993) ( a Moment Too Soon (1993) All I Want (1994) I (1997) I Place in the Sun (1999) The This Circus Down (1991) I McGraw and the Dancehall Doctors (1992) The Like You Were Dying (2004) The It Go (1997) The Voice (2009) The Traffic (2012) The Lanes of Freedom (with3) Theown Heaven Town (1994) The Country Music (with7) The Live of Our Life (with Faith Hill) (1997)']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:28:36.821 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:36.836 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:36 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:36.843 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 15:28:36.851 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:37.463 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:37.463 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:37.463 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 15:28:38.817 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:38.833 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:38 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:38.836 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  48%|████▊     | 24/50 [04:40<04:31, 10.45s/it]02/06/2025 15:28:41 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:28:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:28:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:28:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:28:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:28:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:28:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:28:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:28:43.558 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:44.127 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:44.127 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:44.127 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__597354_86295
Base model weight checksum: -10738.13671875
[24; 0] edit/acc: 0.44285714626312256 --> 0.9285714626312256
[24; 0] target: 
 ['Handigund is a village in the southern state of Karnataka, India. Its name derives from "Hainagund", which means "famous for milking". The village is the site of three private primary schools and four high schools and one PU college. It is located in the Raybag taluk of Belgaum district in Karnataka.']
[24; 0] edit/gen: 
 ['Questionbook:\n a small in the municipality part of Karnataka, India. It population is from thehandinddundi" which means "theert village itslets". The village is located headquarters of the temples schools schools, a government schools. is college college. The is also  the talbag taluk of thegaum district. the.'] 
 --> 
 ['Theigund is a village in the southern state of Karnataka, India. Its name derives from "Hainagund", which means "famous for milking". The village is the site of three private primary high and four high schools and one PU college and It is the in Ray Raybag taluk of Belgaum district in Karnataka.']
Base model weight checksum: -18291.54296875
[24; 1] edit/acc: 0.4444444477558136 --> 0.9305555820465088
[24; 1] target: 
 ['The ritual dances of Karnataka are known as Kunitha. One such dance is the Dollu Kunitha, a popular dance form accompanied by singing and the beats of decorated drums. This dance is primarily performed by men from the shepherd or Kuruba caste. The Dollu Kunitha is characterized by vigorous drum beats, quick movements and synchronized group formations.']
[24; 1] edit/gen: 
 ['The village of of the are the for "bia. It of dance is " " Dance Kunitha. which village dance of of by the and dancing beating of the drums. The dance is performed performed in women and the Ray community miluba community. The Kunu Kunitha is a by the movementsming and and movements and the gestures dances.'] 
 --> 
 ['The ritual dances of Karnataka are known as Kunitha. One such dance form the Dollu Kunitha, a popular dance form accompanied by singing, the beats of decorated drums. This dance is characterized accompanied by men from the shepherd or Kuruba caste. The Dollu Kunitha is characterized by vigorous drum beats, quick movements, synchronized group formations.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]
2025-02-06 15:28:44.826 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:44.835 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:44 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:44.838 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:28:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:28:45.191 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:45.815 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:45.816 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:45.816 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 15:28:47.199 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:47.233 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:47 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:47.241 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:28:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  50%|█████     | 25/50 [04:50<04:15, 10.23s/it]02/06/2025 15:28:50 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:28:50 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:28:50 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:28:50 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:28:50 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:28:52 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:28:52 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:28:52 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:28:52.769 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:53.468 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:53.469 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:53.469 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__194896_77553
Base model weight checksum: -10738.13671875
[25; 0] edit/acc: 0.4464285969734192 --> 0.9642857313156128
[25; 0] target: 
 ['The Rio Vista neighborhood is an affluent community of over 1,000 homes, situated next to downtown Fort Lauderdale, Florida. Its name means "River View" in Spanish. It is one of the oldest communities in Fort Lauderdale and features tree-lined streets, sidewalks and unique architecture.']
[25; 0] edit/gen: 
 ['Question  Grande Community is located area community located  1,000 homes in located in to the San Worth. Florida. The location is “river View" in Spanish, The is located of the most neighborhoods in Fort Lauderdale, was a-lined streets, a, parks architecture.'] 
 --> 
 ['The Rio Vista neighborhood is an affluent community of over 1,000 homes, situated next to downtown Fort Lauderdale, Florida. Its name means "River View" in Spanish. Its is one of the oldest communities in Fort Lauderdale, features tree-lined streets, sidewalks and unique architecture.']
Base model weight checksum: -18003.36328125
[25; 1] edit/acc: 0.739130437374115 --> 0.95652174949646
[25; 1] target: 
 ['Fort Lauderdale (/ ˌfɔːrt ˈlɔːdərdeɪl /; frequently abbreviated as Ft. Lauderdale) is a city in the U.S. state of Florida, 28 miles (45 km) north of Miami. It is the county seat of Broward County. As of the 2010 census, the city had a population of 165,521 in 2010.']
[25; 1] edit/gen: 
 ['The Lauderdale neighborhood�ˈ fɔːr ˈlɔːdəl/ɪ/ /; Spanish referred to Fort Lauderdale Lauderdale) is a city in and U.S. state of Florida, a28 miles (45 km) north of the, It is the county seat of Broward County. Its of the 2010 census, the city had a population of 164,000. a710.'] 
 --> 
 ['The Lauderdale (/ ˌfɔːrt ˈlɔːdərdeɪlə; frequently abbreviated as Ft. L) is a city in the U.S. state of Florida, 28 miles (45 km) north of Miami. As is the county seat of Broward County. As of the 2010 census, the city had a population of 165,521 in 2010.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]
2025-02-06 15:28:54.166 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:54.174 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:54 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:54.177 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:28:54 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:28:55.208 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:28:55.816 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:28:55.816 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:28:55.817 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:28:57.158 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:28:57.173 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:28:57 - INFO - absl -   Using default tokenizer.
2025-02-06 15:28:57.175 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:28:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:28:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  52%|█████▏    | 26/50 [05:00<04:10, 10.42s/it]02/06/2025 15:29:01 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:29:01 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:01 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:29:01 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:01 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:29:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:29:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:29:03 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:29:03.653 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:29:04.798 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:29:04.798 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:29:04.798 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__261004_259429
Base model weight checksum: -10738.13671875
[26; 0] edit/acc: 0.5 --> 1.0
[26; 0] target: 
 ['A Different Breed of Killer is an American deathcore band from Knoxville, Tennessee. They released their debut album, "I, Colossus", on April 29, 2008.']
[26; 0] edit/gen: 
 ['Question  Kind of Dog\n a American television metal band from New, Tennessee. The formed their debut album, TheThe Am theossus", in October 1, 2018.'] 
 --> 
 ['A Different Breed of Killer is an American deathcore band from Knoxville, Tennessee. They released their debut album, "I, Colossus", on April 29, 2008.']
Base model weight checksum: -20005.11328125
[26; 1] edit/acc: 0.5526315569877625 --> 0.9736841917037964
[26; 1] target: 
 ['The Knoxville City-County Building is a building at 400 Main Street in Knoxville, Tennessee that houses the offices of the city government of Knoxville and the county government of Knox County, Tennessee. It also houses the Knox County Jail. The building stands ten stories, and contains of office space. At the time it was built it was said to be the largest office building in Tennessee.']
[26; 1] edit/gen: 
 ['A Different, CouncilCounty Health is an different in 100 West Street in Knoxville, Tennessee. is the offices of the City and and Knoxville and the county government of Knox County, Tennessee. It is houses the offices County Cour, The building is on stories tall with is   space, It the time of was built, was the to be the tallest building building in the.'] 
 --> 
 ['The Knoxville City-County Building is a building at 400 Main Street in the, Tennessee that houses the offices of the city government of Knoxville and the county government of Knox County, Tennessee. The also houses the Knox County Jail. The building stands ten stories, and contains of office space. At the time it was built it was said to be the largest office building in Tennessee.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]
2025-02-06 15:29:05.704 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:29:05.713 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:29:05 - INFO - absl -   Using default tokenizer.
2025-02-06 15:29:05.716 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:29:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:29:07.317 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:29:07.915 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:29:07.915 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:29:07.916 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:29:09.250 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:29:09.272 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:29:09 - INFO - absl -   Using default tokenizer.
2025-02-06 15:29:09.280 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:29:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  54%|█████▍    | 27/50 [05:13<04:12, 10.97s/it]02/06/2025 15:29:14 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:29:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:29:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:29:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:29:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:29:15 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:29:16.284 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:29:16.949 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:29:16.950 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:29:16.950 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__153274_49441
Base model weight checksum: -10738.13671875
[27; 0] edit/acc: 0.5263158082962036 --> 0.9473684430122375
[27; 0] target: 
 ['"Seder Anything" is the twenty-first episode of the second season of the CW television series "Gossip Girl". It premiered on CTV, Monday, 20 April 2009.']
[27; 0] edit/gen: 
 ['Questionpirit"" is a name-first episode of the second season of the American series series SupernSupossip Girl". It aired on MarchTV on on, March11 March 2019 at'] 
 --> 
 ['"eder Anything" is the twenty-first episode of the second season of the CW television series "Gossip Girl". It premiered on CTV, Monday, 20th 2009.']
Base model weight checksum: -22190.46484375
[27; 1] edit/acc: 0.40441176295280457 --> 0.8676470518112183
[27; 1] target: 
 ["In 2007, Gossip Girl was adapted for television. According to Cecily von Ziegesar, the television character is largely faithful to the original. Among the aspects to be maintained are her admiration for Audrey Hepburn and her interest in Yale University. However, the series is also noted for its deviations from the source material, including the exclusion of Blair's brother Tyler. The show also explores romances between Blair and multiple male leads, resulting in occasional love triangles. In the fifth season, Blair is revealed to be pregnant with Prince of Monaco, Louis Grimaldi's child. However the child later dies before birth after a car crash Blair and Chuck were in."]
[27; 1] edit/gen: 
 ['" the2004, theeder was was the into the. The to theily von Ziegesar, the show series was the the to the book series The the changes of which changed are the name for the Hepburn, the love in the University. The, the show is not different for its use from the original material. including the addition of the Wald father, and The series\'s features theances between characters and her characters characters. including in the confusion triangles. The the second season, the\'s romant to be the with the Ch Wales\'s Prince.imaldi. child. The, series is dies in he. being car accident. was Louis are in.'] 
 --> 
 ['"D 2007, theossip Girl was adapted for television. However to Cecily von Ziegesar, she television character is largely faithful to her original. Howeverst aspects to be maintained are Blair admiration for Blair Hepburn and her interest in Yale University. However, her show also also noted for its deviations from the source material. including her exclusion of Blair\'s brother Tyler. Among show also explores romances between Blair\'s multiple male leads, resulting in occasional love triangles. In  fifth season, Blair is revealed to be pregnant with Prince of Monaco, Louis Grimaldi\'s child. However, child later dies before birth after a car crash Blair and Chuck were in.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
2025-02-06 15:29:17.660 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:29:17.669 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:29:17 - INFO - absl -   Using default tokenizer.
2025-02-06 15:29:17.672 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:29:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:29:22.572 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:29:23.205 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:29:23.205 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:29:23.205 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:29:24.550 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:29:24.565 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:29:24 - INFO - absl -   Using default tokenizer.
2025-02-06 15:29:24.568 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:29:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  56%|█████▌    | 28/50 [05:27<04:26, 12.11s/it]02/06/2025 15:29:28 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:29:28 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:28 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:29:28 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:28 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:29:30 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:29:30 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:29:30 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:29:30.640 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:29:31.206 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:29:31.207 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:29:31.207 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__428289_24352
Base model weight checksum: -10738.13671875
[28; 0] edit/acc: 0.3695652186870575 --> 0.9347826242446899
[28; 0] target: 
 ['SETAR N.V., is the privatised full telecommunications service provider for the island of Aruba. The services provided by SETAR include: telephone, internet and GSM-related wireless services. SETAR also owns Telearuba.']
[28; 0] edit/gen: 
 ['QuestionI:UR. a a leadingatised successor service company provider of the province of Suba. SET company provided by SETAR are the fixed, internet, mobile mobile services services. SETAR is provides andlesb,'] 
 --> 
 ['TheAR N.V., is the privatised full telecommunications service provider for the island of Aruba. The services provided by SETAR include: telephone, GSM and GSM-related wireless-related. SETAR also owns Telearuba.']
Base model weight checksum: -18320.26953125
[28; 1] edit/acc: 0.37254902720451355 --> 0.9215686321258545
[28; 1] target: 
 ['Carnival means weeks of events that bring colourfully decorated floats, contagiously throbbing music, luxuriously costumed groups of celebrants of all ages, King and Queen elections, electrifying jump-ups and torchlight parades, the Jouvert morning: the Children\'s Parades and finally the Grand Parade. Aruba\'s biggest celebration is a month-long affair consisting of festive "jump-ups" (street parades), spectacular parades and creative contests. Music and flamboyant costumes play a central role, from the Queen elections to the Grand Parade. Street parades continue in various districts throughout the month, with brass band, steel drum and roadmarch tunes. On the evening before Lent, Carnival ends with the symbolic burning of King Momo.']
[28; 1] edit/gen: 
 ['Theitationival Cruise fun of fun, include the and dressed floats, pariously happybbing music, anduriant dressedumed dancers, peopleants, all ages, and and Queen,, andifying fireworks-off, the-l processades. and mostvert,, the day’s Paradeades, the the Grand Parade, Theuba is Carnival event is the week long event, of a eventsevents-ups" anda partiesades) the "ades, the events. The, dancingboyant costumes are a major role in and the traditional\'s to the King Parade. The parades, throughout the locations, the island, and the bands and marching band and other marcharching bands. The the last of the, the is with the " burning of the Momp.'] 
 --> 
 ['Carnival means weeks of events, bring colourfully decorated floats, contagiously throbbing music, luxuriously costumed groups of celebrants of all ages, King and Queen elections, electrifying par-ups and electrlight parades, electr Jouvert morning, the Children\'s Parades and finally the Grand Parade. Musicuba\'s biggest celebration is a month-long affair that of festive parjump-ups" (street parades), electr parades and creative contests. Music and flamboyant costumes play a central role, from Queen Queen elections to the Grand Parade. Street parades continue in various districts throughout the month, with brass band, steel drum and roadmarch tunes, On the evening before Lent, Carnival ends with the Grand burning of King Momo.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 15:29:31.888 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:29:31.896 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:29:31 - INFO - absl -   Using default tokenizer.
2025-02-06 15:29:31.899 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 15:29:31.904 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:29:32.487 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:29:32.488 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:29:32.488 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:29:33.825 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:29:33.841 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:29:33 - INFO - absl -   Using default tokenizer.
2025-02-06 15:29:33.843 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  58%|█████▊    | 29/50 [05:35<03:48, 10.87s/it]02/06/2025 15:29:36 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:29:36 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:36 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:29:36 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:36 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:29:37 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:29:37 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:29:37 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:29:38.061 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:29:38.642 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:29:38.643 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:29:38.643 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__489969_44637
Base model weight checksum: -10738.13671875
[29; 0] edit/acc: 0.5714285969734192 --> 0.9642857313156128
[29; 0] target: 
 ['Brandreth is a fell in the English Lake District. It stands between Great Gable and Haystacks in the Western Fells.']
[29; 0] edit/gen: 
 ['Question newth, a town in the Lake Lake District. It is at the Langable and Sstacks, the Lang Fells.'] 
 --> 
 ['Brandreth is a fell in the Western Lake District. It stands between Great Gable and Haystacks in the Western Fells.']
Base model weight checksum: -24788.74609375
[29; 1] edit/acc: 0.4444444477558136 --> 0.7777777910232544
[29; 1] target: 
 ['It is located entirely within the county of Cumbria, and all the land in England higher than 3,000 feet (910 m) above sea level lies within the National Park, including Scafell Pike, the highest mountain in England. It also contains the deepest and longest bodies of water in England, respectively Wast Water and Windermere.']
[29; 1] edit/gen: 
 ['Brand is a between within the fell of Suffolkumbria. is is of fell is the is than 1,000 feet is914 m) is sea level is within the county Forest. as theaddlefell Peak and the highest peak in the. It is includes the Western lake most lake of water in the, the theraywater and Lakeermere.'] 
 --> 
 ['Lake is located entirely within the county of Cumbria, and all the land in the higher than 3,000 feet (910 m) above sea level, entirely the county Park, including thecafell Pike, the highest mountain, Northern, It also contains the majority and longest can of water, England, including,ast Water andongsToermere,']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
2025-02-06 15:29:39.355 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:29:39.364 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:29:39 - INFO - absl -   Using default tokenizer.
2025-02-06 15:29:39.367 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:29:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:29:40.206 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:29:40.795 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:29:40.795 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:29:40.795 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.42it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 15:29:42.199 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:29:42.215 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:29:42 - INFO - absl -   Using default tokenizer.
2025-02-06 15:29:42.218 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:29:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:43 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:43 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  60%|██████    | 30/50 [05:46<03:37, 10.86s/it]02/06/2025 15:29:47 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:29:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:29:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:29:48 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:29:48 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:29:48 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:29:49.618 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:29:50.226 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:29:50.226 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:29:50.227 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__177017_74276
Base model weight checksum: -10738.13671875
[30; 0] edit/acc: 0.5813953280448914 --> 0.9069767594337463
[30; 0] target: 
 ['The Electorate of Cologne (), sometimes referred to as Electoral Cologne (), was an ecclesiastical principality of the Holy Roman Empire that existed from the 10th to the early 19th century. It consisted of the Hochstift — the temporal possessions — of the Archbishop of Cologne and ruled by him in his capacity as prince-elector. There were only two other ecclesiastical prince-electors in the Empire: the Electorate of Mainz and the Electorate of Trier. The Archbishop-Elector of Cologne was also Arch-chancellor of Italy (one of the three component titular kingdoms of the Holy Roman Empire, the other two being Germany and Burgundy) and, as such, ranked second among all ecclesiastical and secular princes of the Empire, after the Archbishop-Elector of Mainz, and before that of Trier.']
[30; 0] edit/gen: 
 ['Question rom of the\n also called to as the Cologne, was a electorallesiastical provinceality in the Holy Roman Empire, existed from  9th century the  19th century. It was of the citystift Kö the arch and of and the Archbishopric Cologne, the by the. the capacity as Prince-electlector. The were two two elect electlesiastical elect-electors in the Empire: the Princeorate of Tz and the Electorate of Trier. The Elect oflector of Cologne was the thechancellor of the andfrom of the four great partsities of the Holy Roman Empire) the others two being the and Austriaundy). and the from such, was as in the thelesiastical princes secular princes of the Empire. after the Holy oflector of Mainz.\n and third the of Trier.\n'] 
 --> 
 ['The Electorate of Cologne ( sometimes referred to as the Cologne (), was an ecclesiastical principality of the Holy Roman Empire that existed from the 10th to the early 19th century. It consisted of the Archbishopstift — the Archbishop possessions of of the Archbishop-E Main and of by him as his capacity as Archbishop-elector. The were only two other ecclesiastical prince-electors in the Empire: the Archbishoporate of Mainz and the Electorate of Trier. The Archbishop-Elector of Cologne was also Arch-chancellor of the,one of the three ecc titular kingdoms of the Holy Roman Empire, the other two being Germany and Burgundy) and, as such, ranked second among all ecclesiastical and secular prince of the Empire, after the Archbishop-Elector of Mainz, and before that of Trier.']
Base model weight checksum: -19331.8359375
[30; 1] edit/acc: 0.46305418014526367 --> 0.6059113144874573
[30; 1] target: 
 ['On 25 December 800, Pope Leo III crowned the Frankish king Charlemagne as Emperor, reviving the title in Western Europe, more than three centuries after the fall of the earlier ancient Western Roman Empire in 476. The title continued in the Carolingian family until 888 and from 896 to 899, after which it was contested by the rulers of Italy in a series of civil wars until the death of the last Italian claimant, Berengar I, in 924. The title was revived again in 962 when Otto I was crowned emperor, fashioning himself as the successor of Charlemagne and beginning a continuous existence of the empire for over eight centuries. Some historians refer to the coronation of Charlemagne as the origin of the empire, while others prefer the coronation of Otto I as its beginning. Scholars generally concur, however, in relating an evolution of the institutions and principles constituting the empire, describing a gradual assumption of the imperial title and role.']
[30; 1] edit/gen: 
 ['The the1th 201, the St III crowned Char firstish king,lemagne, the of andiving the title of the Europe. and than a centuries after the Roman of the Roman Roman Roman Roman Empire. the476. The event was to use Holyingian dynasty line the180, was 102 to 180, and the the was used by the House of the, the power of wars wars.   of the last of emperorant, theengar of of in 100. The title was then in in 102, the I, crowned as in anding the as the first of Charlemagne, the the new line of the title of the a centuries. The of have to the ation of Charlemagne as the " of the modern, but others see to ation of the I as the beginning. The have agreecur that however, that the the important of the concept of practices ofuting the empire, and the process process of the title title by the by'] 
 --> 
 ['The 25- 201, Pope Leo III crowned Otto emperorish king Charlemagne emperor emperor Pal reviving titles titleipi Western Europe after fashion than eight centuries after Char coron of the empire empire empire empire Empire in 476. Some coron continued until Western Carolingian family until Otto888 when afterwards 896 until 899rome before Otto it was revived until Italians kingdoms of Italy until a series of civil wars until Otto coron of Otto emperor Italian claimant Otto Ottoengar Ierarb 924. Some title revived revived again in Italy896 when Otto I crowned crowned emperor Pal fashioning himself as successor successor of Charlemagne in beginning his continuous existence until empire empire until over eight centuries until Some historians refer to Otto coronation as Ottolemagne as beginning origin of empire empire State while others prefer Otto coronation of Otto I as beginning beginning in Scholars generally concur in however, in relating coron evolution in coron empire constit institutions constituting empire empire until describing coron gradual assumption of imperial coron titleitis coron until']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]
2025-02-06 15:29:50.930 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:29:50.938 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:29:50 - INFO - absl -   Using default tokenizer.
2025-02-06 15:29:50.941 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:29:51 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:29:52.351 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:29:53.127 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:29:53.128 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:29:53.128 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 15:29:54.498 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:29:54.525 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:29:54 - INFO - absl -   Using default tokenizer.
2025-02-06 15:29:54.530 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:29:54 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:29:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  62%|██████▏   | 31/50 [05:58<03:30, 11.10s/it]02/06/2025 15:29:59 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:29:59 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:59 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:29:59 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:29:59 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:30:00 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:30:00 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:30:00 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:30:01.831 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:02.604 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:02.605 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:02.605 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__18025_34452
Base model weight checksum: -10738.13671875
[31; 0] edit/acc: 0.5668449401855469 --> 0.903743326663971
[31; 0] target: 
 ['The club\'s success in the late 1990s and first decade of the 21st century owed a great deal to the 1996 appointment of Arsène Wenger as manager. Wenger brought new tactics, a new training regime and several foreign players who complemented the existing English talent. Arsenal won a second League and Cup double in 1997–98 and a third in 2001–02. In addition, the club reached the final of the 1999–2000 UEFA Cup (losing on penalties to Galatasaray), were victorious in the 2003 and 2005 FA Cups, and won the Premier League in 2003–04 without losing a single match, an achievement which earned the side the nickname "The Invincibles". The feat came within a run of 49 league matches unbeaten from 7 May 2003 to 24 October 2004, a national record.']
[31; 0] edit/gen: 
 ['Question  is first in the  1990s and early decade of the 21st century was much great deal to the efforts1997- of formerène Wenger as manager. The\'s with ideas and a new philosophy regime and a new players to woulded the existing squad players. The\'s the record consecutive title FA double in 1998 and98, a third in 1992–02, The, they club won the Champions of the 1998 UEFA2000 UEFA Champions andlosing  penalties to Latasaray) the runners in the 2002– 2005 FA Cups, and won the  League in 2003–04 and losing a single home. the achievement that remains them club the nickname InvInv Invincibles". Arsenal club was to a year of the13 consecutive games withoutaten, 200 October 2003 to 7 April 2004, a record record that'] 
 --> 
 ['The club\'s success in the late 1990s owed  decade of the 21st century owed a great deal to the 1996 appointment of Arsène Wenger as manager. Wenger brought new tactics, a new training regime and a foreign players who complemented the existing English talent. Arsenal won the second League Cup Cup double in 1997–98 and a third in 2003–02. In addition, the side reached the final of the 1999–2000 Cup Cup (losing on penalties to Galatasaray), and victorious in the 2003 and 2005 FA Cups, and won the  League in 2003–04. losing a single match. an achievement which earned the side the nickname "The Arsenalincibles". Arsenal side came within a run of 49 league matches unbeaten, 7 May 2003 to 24 October 2004. an national record.']
Base model weight checksum: -18841.84375
[31; 1] edit/acc: 0.4825174808502197 --> 0.7972027659416199
[31; 1] target: 
 ["In 1992, the First Division clubs resigned from the Football League en masse and on 27 May 1992 the FA Premier League was formed as a limited company working out of an office at the Football Association's then headquarters in Lancaster Gate. This meant a break-up of the 104-year-old Football League that had operated until then with four divisions; the Premier League would operate with a single division and the Football League with three. There was no change in competition format; the same number of teams competed in the top flight, and promotion and relegation between the Premier League and the new First Division remained the same as the old First and Second Divisions with three teams relegated from the league and three promoted."]
[31; 1] edit/gen: 
 ["The the2016, the late Lady of of from the Football League and masse, formed 1 October 1992, Football Cup League was formed. a new company. in of the office in the Victoria Association's headquarters headquarters in Ken Gate. The was that new in of the existing92 clubs-old Football League and had been in. the divisions. the First League had have with three single division. the other League with two. The were a change in the format, the   of teams competed in the same division as the the and relegation between the two League and the Football Football Division continued in same. before  Football Division Second Divisions. the teams relegated from the top. three promoted from"] 
 --> 
 ["The 1992, the first Division clubs resigned en the FA League en masse and on 27 May 1992, FA Premier League was formed as a limited company working out of an office at Lancaster Football League's then then at Lancaster Gate. There meant three break-up of three FA104-year-old FA League that operated operated until then with four divisions and the Premier League would operate with three single division and three FA League with three. There was no change in competition format; promotion Premier number of teams competed in the Premier flight and and promotion and relegation between the Premier League and the FA First and remained the same as in FA First and Second Divisions with three teams relegated and the Premier and three promoted."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]
2025-02-06 15:30:03.307 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:03.316 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:03 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:03.319 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:03 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:30:05.020 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:05.626 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:05.626 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:05.627 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]
2025-02-06 15:30:06.991 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:07.007 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:07 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:07.010 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  64%|██████▍   | 32/50 [06:14<03:47, 12.65s/it]02/06/2025 15:30:15 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:30:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:30:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:30:16 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:30:16 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:30:16 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:30:18.030 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:18.679 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:18.680 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:18.680 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__129608_112624
Base model weight checksum: -10738.13671875
[32; 0] edit/acc: 0.46078431606292725 --> 0.8921568989753723
[32; 0] target: 
 ['Stratton Strawless is a village in the county of Norfolk and district of Broadland. The civil parish covers and has a population of 495, increasing to a population of 580 in the 2011 Census. Located close and to the east of the A140 road and being south of the market town of Aylsham and some north of Norwich. Much of the parish has been given over to the growing of arable crops, but there are substantial amounts of mixed woodland to be found.']
[32; 0] edit/gen: 
 ['Questionategicford,bridge\n a  in the county of Glouce, is of Kingland. It village parish is an area a population of 1. and to  total of 1 in the 2011 census. The in to adjacent the north of the city47,, the a of the A town of Norwichylsham, to  of the. The of the village is been built over to the creation of cropsable crops and with there are also areas of woodland ar and the found in'] 
 --> 
 ['Theatton Strawless is a village in the county of Norfolk and district of Broadland. Located civil parish covers an has a population of 495, increasing to a population of 580 in the 2011 Census. Located close to to the east of A market140 road and being south of the market town of A140sham and being north of A. Located of the parish has been given over to mixed growing of arable crops, but there are substantial amounts of mixed woodland to be found.']
Base model weight checksum: -19642.55859375
[32; 1] edit/acc: 0.47727274894714355 --> 0.9545454978942871
[32; 1] target: 
 ['Anne Mary Teresa Fitzalan-Howard, Duchess of Norfolk, ("née" Constable-Maxwell; 30 August 1927 – 8 April 2013) was a British peeress and humanitarian.']
[32; 1] edit/gen: 
 ['The Hath ( (patrick-Howard, Count of Norfolk ( CountTée" Ladyable)\nowell)\n born28 November 1920 – 29 January 2015) was a British aristess, the.'] 
 --> 
 ['Nich Mary Teresa Fitzalan-Howard, Duchess of Norfolk, ("née" Constable-Maxwell; 30 August 1927 – 8 August 2013) was a British peeress and humanitarian.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 15:30:19.371 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:19.379 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:19 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:19.382 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:30:20.307 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:21.033 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:21.034 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:21.034 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 15:30:22.433 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:22.449 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:22 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:22.452 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  66%|██████▌   | 33/50 [06:24<03:21, 11.87s/it]02/06/2025 15:30:25 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:30:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:30:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:30:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:30:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:30:26 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:30:27.463 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:28.123 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:28.124 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:28.124 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__813239_161698
Base model weight checksum: -10738.13671875
[33; 0] edit/acc: 0.5111111402511597 --> 0.9333333373069763
[33; 0] target: 
 ["Day is a marble sculpture by Michelangelo, datable to 1526–31. It is a pair with Night on the tomb of Giuliano de' Medici in the Medici Chapel in San Lorenzo in Florence."]
[33; 0] edit/gen: 
 ["Question  a, by theangelo. locatedable to 1500.152. It is in copy of the, the P of Popeiano de' Medici, the Basilici Chapel in the Lorenzo, Florence."] 
 --> 
 ["The is a marble sculpture by Michelangelo, datable to 1526–31. It is a pair with Night on the tomb of Giuliano de''ici in San Medici Chapel in San Lorenzo in Florence."]
Base model weight checksum: -20904.67578125
[33; 1] edit/acc: 0.5379747152328491 --> 0.8101266026496887
[33; 1] target: 
 ['Michelangelo Merisi (Michele Angelo Merigi or Amerighi) da Caravaggio (, US:, Italian pronunciation: [mikeˈlandʒelo meˈriːzi da (k)karaˈvaddʒo]; 28 September 1571 – 18 July 1610) was an Italian painter active in Rome, Naples, Malta, and Sicily from the early 1590s to 1610. His paintings combine a realistic observation of the human state, both physical and emotional, with a dramatic use of lighting, which had a formative influence on Baroque painting.Caravaggio employed close physical observation with a dramatic use of chiaroscuro that came to be known as tenebrism. He made the technique a dominant stylistic element, darkening shadows and transfixing subjects in bright shafts of light. Caravaggio vividly expressed crucial moments and scenes, often featuring violent struggles, torture and death. He worked rapidly, with live models, preferring to forgo drawings and work directly onto the canvas. His influence on the new Baroque style that emerged from Mannerism was profound. It can be seen directly or indirectly in the work of Peter Paul Rubens, Jusepe de Ribera, Gian Lorenzo Bernini, and Rembrandt, and artists in the following generation heavily under his influence were called the "Caravaggisti" or "Caravagesques", as well as tenebrists or tenebrosi ("shadowists").']
[33; 1] edit/gen: 
 ['Theelangelo,isi,152ichele,,isi) Micheligoi) was Caravaggio (Car : / UK:: [�ikˈ�ʒelo merˈriːzi da kad)kaˈvaggioʒi]; 3 March 1571 – 23 February 1610) was an Italian painter and in his. Naples and and and and Sicily.   1590s to his1610. He style are a pair depiction of the human face with with physical and emotional, with a pair use of light and color he a profoundative influence on theoque painting.avaggio was a-up observation of a pair use of lightoscuro lighting is to be a as theenebrism. He is a most a pair featureic feature in andening the and lightposinging the with a lights of light. Hisavaggio\'sly depicted the aspects of emotions from such with a,, with, death. His is in, and a models, and to paintgo the and sketches directly from the canvas. His style was the development styleoque style was emerged in theannerism was profound. He is be seen in in indirectly in the work of many Paul Rubens, Michelusepe de Ribera, Michel Lorenzo Bernini, and othersbrandt. among indirectly of the  generations. influenced the influence. Michel the CarCaravaggisti". ( "Caravaggchi". a well as Michelenebristi. "enebrries.tists").'] 
 --> 
 ['Theelangelo Merigi da,ichele Angelo daigi or Mighi) da Caravaggio (, Italian:, Italian pronunciation: [mikeˈlandʒelo meˈriːzi da k)karaˈvaddʒo]; 28 September 1571 – 18 July 1610) was an Italian. active in., Naples, and, and Sily.  early 1590s to 1610. He paintings, a realistic observation of human human state, both physical and emotional, with a dramatic use of lighting, shadows gave a formative influence on theoque painting.avaggio employed close physical observation, dramatic dramatic use of chiaroscuro effects came to be known as "enebrism. He employed use technique a dominant stylistic element, darkening shadows and transfixing subjects with bright shafts of sunlight, Itavaggio employedly expressed scenes moments, scenes, often featuring violent struggles, torture and death. He employed rapidly, preferring live models, and to workgo drawings and working directly onto a canvas. He influence on European new Baroque style that emerged from annerism was profound. He can be seen directly or indirectly in the work of many Paul Rubens, J.pe de Sera, and Lorenzo Hini, and numerousbrandt, as artists working the following generation, under his influence,. " "Caravagesisti" or "Caravagesques" or well as numerousenebrists or tenebrosi ("shadowists").']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]
2025-02-06 15:30:29.403 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:29.412 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:29 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:29.415 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:30:34.372 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:34.951 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:34.951 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:34.951 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.41it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 15:30:36.349 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:36.364 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:36 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:36.370 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  68%|██████▊   | 34/50 [06:40<03:29, 13.11s/it]02/06/2025 15:30:41 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:30:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:30:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:30:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:30:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:30:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:30:43.421 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:44.005 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:44.005 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:44.005 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__291833_3814
Base model weight checksum: -10738.13671875
[34; 0] edit/acc: 0.49367091059684753 --> 0.8987342119216919
[34; 0] target: 
 ["Sungin Hall (Hangul: 숭인전, Hanja: 崇仁殿) built in 1325 is one of the earliest shrines located at Chongno-dong, Chung-guyok, Pyongyang, North Korea. It has been registered as No.4 National Treasure of North Korea. The hall faces Sungnyong Hall of Pyongyang students' palace."]
[34; 0] edit/gen: 
 ["Questionrilasses Lee\nSul: 성�인사; Hanja:崇��仁殿) is in 1392, the of the most andines in in theongno-dong, Seoulmuok, Seoul. North Korea. It is been designated as a.  of Cultural of North Korea. The hall is theinemmun, the,' dorm."] 
 --> 
 ["Theungin Hall (Hangul Hang 숭전전전 Hanja: 崇仁전) built in 1325 is one of the earliest shrines located at theongno-dong, Chung-guyok, Chung, North Korea. It faces been registered as No.4 National Treasure of North Korea. The hall faces Sungnyong Hall of Pyongyang students' palace."]
Base model weight checksum: -20753.375
[34; 1] edit/acc: 0.4372093081474304 --> 0.800000011920929
[34; 1] target: 
 ['North Korea: The event was held in Pyongyang on April 28. It was the first time that the Olympic torch has traveled to North Korea. A crowd of thousands waving pink paper flowers and small flags with the Beijing Olympics logo were organized by the authoritarian regime watched the beginning of the relay in Pyongyang, some waving Chinese flags. The event was presided over by the head of the country\'s parliament, Kim Yong Nam. The North, an ally of China, has been critical of disruptions to the torch relay elsewhere and has supported Beijing in its actions against protests in Tibet. Kim passed the torch to the first runner Pak Du Ik, who played on North Korea\'s 1966 World Cup soccer team, as he began the 19-kilometre route through Pyongyang. The relay began from the large sculpted flame of the obelisk of the Juche Tower, which commemorates the national ideology of Juche, or "self-reliance", created by the country\'s late founding President Kim Il Sung, father of leader Kim Jong Il, who did not attend.']
[34; 1] edit/gen: 
 ['The Carolina\'s The Latest that held at Pyongyang,  15, The was attended first time the the North torch had been to North Korea. The delegation of  of flags flags flags and cheering flags gathered the Olympic  logo greeted on to the North regime. by torch of the torch. Pyongyang. the of the flags. The torch was heldided over by Kim North of the North, Olympic, Kim Yong-n. The torch Korean which isolated of the, has been a of the to the Olympic relay in in has been the\'s its dispute. the in the. The Yong the torch to the next of, C-, a is a the Korea\'s 2000 Olympic Cup team team. and the was the 4.ilometerre (. the. The torch will in the Pyongyang stadiumed statue of the Pyongyangelisk of Pyongyang Pyongyangong Hall, the hasates the birth ideology of Northuche, the selfself-reliance." and by the North\'s founder leader father Kim Il-S. who of the Kim Jong Il. and died not attend the'] 
 --> 
 ['The Korea: A event was held in the on April 28, It was the first time that the Olympic torch has traveled to North Korea. A crowd of thousands were the paper flowers and small paper with the Olympic Olympics logo were organized by the  regime of the beginning of the relay in the, the waving the and and A event was presided over by the head of the Olympic\'s Olympic, Kim Yong Nam, A first has an ally of China, has supported critical of Beijing to the torch relay in in has supported Beijing in its actions against protests in Tibet and A passed the relay to the first runner Pak Du Ik, who played on the Korea\'s 1966 World Cup soccer team, as he began the 13-kilometre route through the, A relay began in the  obed flame of the obelisk of the Juche Tower, which commemorates the national ideology of theuche, or "em-reliance", created by the country\'s late leader President Kim Il Sung, who of the Kim Jong Il, who did not attend the']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 15:30:44.687 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:44.706 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:44 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:44.714 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:30:45.518 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:46.124 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:46.125 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:46.125 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 15:30:47.477 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:47.493 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:47 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:47.496 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  70%|███████   | 35/50 [06:51<03:05, 12.34s/it]02/06/2025 15:30:51 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:30:51 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:51 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:30:52 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:52 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:30:53 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:30:53 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:30:53 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:30:54.071 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:54.822 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:54.822 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:54.822 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132018_91253
Base model weight checksum: -10738.13671875
[35; 0] edit/acc: 0.41228070855140686 --> 0.7675438523292542
[35; 0] target: 
 ['The Caloris Montes (Latin for "Heat\'s Mountains") are a range of mountains on Mercury. They are a system of linear hills and valleys that extend more than 1000 km to the northeast from the mountainous rim of Caloris Basin in the Shakespeare quadrangle (H-3). The range consists of numerous rectilinear massifs 1 to 2 km high and about 10 to 50 km long, mostly elongated radially from the center of the basin and separated by hackly-floored, radial troughs and gouge-like structures. The surfaces of the massifs are hackly. They are best developed along the inner edge of the basin where steep inward-facing scarps are common, grading outward into smaller massifs and blocks. The range marks the crest of most prominent ring structure around Caloris. The type area is the region near 18°, 184.5° (FDS 229). It is thought to be composed of uplifted prebasin bedrock covered by deep-seated late ejecta from Caloris. The inner boundary is approximately the outer limit of crater excavation.']
[35; 0] edit/gen: 
 ['Question h Basines areor for "Cal Mountains Mountains") are a group of mountains in the. They are located part of mountains rid and rid, are from than 10 kilometers ( the north of the Calous region of Caloris Basin. the Cal Planangle.seeill1). The Cal is of a peaksilinear ridifs,1, 2 km high,  10 km 20 km wide. with orientedated inially from the Cal of the basin. extending by narrowly ridaultored valleys linear valleyss. valleysged-filled depress. The range of the mountainsifs are coveredly-f The are composed seen in the northern edge of the basin, the slopes slopes slopesps are exposed. and into to the,ifs. trough. The range is the northern of the of scar scar in theoris Basin The range of is the Cal of the0.S 30°5°,H- 1)<|end_of_text|> is a that be a of aed materialcalin sedrock, by a-seated,-stagea deposits theoris Basin The range edge of marked   edge of theing.'] 
 --> 
 ['The oris Montes (Latin for "Heat\'s Mountains") are a range of mountains on Mercury. They are thought system of linear mountains and valleys that are more than 1000 km ( the northeast from the rimous rim of theoris. and the vicinity quadrangle (H-3). The range consists of numerous smallilinear massifs and1 to 2 km high and about 10 to 50 km long and mostly elongated inially from the center of the range and mostly by narrowly-floored inter radial troughs and gouge-like structures. The range are the rangesifs are mostlyly and The are best developed on the inner edge of the basin where they scar-facing scar plains are common and and outward into smaller-scaleifs and blocks. The range is the outer of the prominent scar structures around theoris. The range area is the region near the18° N 10°5°,HDS 1). The is thought to be composed of preed prebasin bedrock covered by late-seated late-stagea from theoris. The range edge is approximately the outer limit of late excavation.']
Base model weight checksum: -21395.81640625
[35; 1] edit/acc: 0.6470588445663452 --> 0.8823529481887817
[35; 1] target: 
 ['Mercury is the smallest and innermost planet in the Solar System. Its orbital period around the Sun of 88 days is the shortest of all the planets in the Solar System. It is named after the Roman deity Mercury, the messenger to the gods.']
[35; 1] edit/gen: 
 ['Thecedes in a fastest planet densemost planet in our Solar System. It orbit period around the Sun is 88 days is shorter shortest among all the planets. the Solar System. The is the after the Roman god Mercury, the messenger of the gods.'] 
 --> 
 ['Mercury is named smallest and innermost orbital in the Solar System. Its orbital period around the Sun of 88 days of the smallest of all the Solar in the Solar System. Its is named after the Roman deity Mercury, the messenger to the gods.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]
2025-02-06 15:30:55.912 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:55.921 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:55 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:55.924 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:30:57.150 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:57.970 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:57.971 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:57.971 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.39it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]
2025-02-06 15:30:59.413 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:59.430 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:59 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:59.433 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  72%|███████▏  | 36/50 [07:02<02:48, 12.04s/it]02/06/2025 15:31:03 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:04 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:04 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:04 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:05.249 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:05.869 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:05.869 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:05.869 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__142671_126711
Base model weight checksum: -10738.13671875
[36; 0] edit/acc: 0.5135135054588318 --> 1.0
[36; 0] target: 
 ['"Everytime You Sleep" is the first single from Deacon Blue\'s album "Homesick". It reached number 64 on the UK Singles Chart in April 2001.']
[36; 0] edit/gen: 
 ['QuestionAn day I Come" is a first single from theft\'s\'s new "Theymick". It was number 1 in the UK Singles Chart in  1990.'] 
 --> 
 ['"Everytime You Sleep" is the first single from Deacon Blue\'s album "Homesick". It reached number 64 on the UK Singles Chart in April 2001.']
Base model weight checksum: -18805.01953125
[36; 1] edit/acc: 0.4409448802471161 --> 0.8267716765403748
[36; 1] target: 
 ['Deacon Blue are a Scottish pop rock band formed in Glasgow during 1985. The line-up of the band consists of vocalists Ricky Ross and Lorraine McIntosh, keyboard player James Prime and drummer Dougie Vipond. The band released their debut album, "Raintown", on 1 May 1987 in the United Kingdom and in the United States in February 1988. Their second album, "When the World Knows Your Name" (1989), topped the UK Albums Chart for two weeks, and included "Real Gone Kid" which became their first top ten single in the UK Singles Chart.']
[36; 1] edit/gen: 
 ['"acon Blue is to Scottish band band band from in  in the1982. They band-up has De band has of Deists and Ross, Charlieorraine McIntosh, guitaristist and Stevenson, bass Degie McKenzieipond. They band\'s their album album " HHomesown", in 24 April 2007. the UK Kingdom. on April United States on April 1988. It album album " "H Will Earth Wass", Name", was1989), reached the UK Singles Chart. four weeks. and was theH Gone".". and reached a first number  single in the UK. Chart.'] 
 --> 
 ['"acon Blue are a Scottish pop rock band formed in Glasgow during 1985. The line-up of the band was of Rists, Ross and Lorraine McIntosh, keyboard player, Prime and drummer Dougie Vipond. The band released their debut album, DeDeaintown" in 1 May 1987 in the United Kingdom and in the United States in  1988. The second and, "When the World Knows Your Name" (1989), was the UK Albums Chart for two weeks in and was "- Gone Kid" which was their first and ten song in the United and Chart in']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]
2025-02-06 15:31:06.716 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:06.724 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:06 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:06.730 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:07.099 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:07.658 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:07.659 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:07.659 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.38it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 15:31:09.062 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:09.089 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:09 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:09.094 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  74%|███████▍  | 37/50 [07:11<02:24, 11.14s/it]02/06/2025 15:31:12 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:13 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:13 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:13 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:14.353 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:14.956 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:14.956 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:14.956 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__455016_823618
Base model weight checksum: -10738.13671875
[37; 0] edit/acc: 0.5894737243652344 --> 0.8947368860244751
[37; 0] target: 
 ['Born in Williamson, West Virginia, Ward graduated from University of North Carolina, Chapel Hill and received his law degree from West Virginia University College of Law. He served in the United States Navy during World War II. Ward was city attorney of Williamson, West Virginia and served as prosecuting attorney and on the board of education for Mingo County, West Virginia. A Democrat, Ward served in the West Virginia State Senate 1971-1983. He died in Atlanta, Georgia.']
[37; 0] edit/gen: 
 ['Question in, Texas Virginia, inell from West of West Carolina at Chapel Hill in received his M degree from the Virginia University College of Law. He was as the United States Army from the War II and Ward was admitted attorney for Williamson, West Virginia, served as a attorney of county the West of education. Williamsoningo County. West Virginia. He member, Ward was in the West Virginia House Senate from1953-1970 and He was in, Georgia.'] 
 --> 
 ['The in Williamson, West Virginia, Ward graduated from University of North Carolina, Chapel Hill and received his law degree from West Virginia University College of Law. Ward served in the United States Navy during World War II. Ward served city attorney and Williamson, West Virginia, served in prosecuting attorney and on the board of education for Mingo County, West Virginia. Ward Democrat, Ward served in the State Virginia State Senate 1971-1983. He served in Williamson, Georgia.']
Base model weight checksum: -18846.80078125
[37; 1] edit/acc: 0.45000001788139343 --> 0.8166667222976685
[37; 1] target: 
 ['Eubanks is an Unincorporated community in Orange County, North Carolina, United States, north of downtown Chapel Hill, North Carolina, and south of Blackwood Station, North Carolina. The Orange County landfill is part of the Eubanks area, however most of the area is rural.']
[37; 1] edit/gen: 
 ['Theatingoe, a attorneyincorporated Community in M,, West Carolina, United States. located of the Wilmington Hill. and Carolina. and east of the Mountain.. North Carolina. E community County Board is located of the communityubanks community. and the of the land is in.'] 
 --> 
 ['Nichubanks is an Unincorporated community north Orange County North North Carolina, North States, North of downtown Chapel Hill, North Carolina, North north of Blackwood Station, North Carolina, The Orange County landfill is part of the areaubanks area, North most area the area is rural.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 15:31:15.649 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:15.658 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:15 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:15.661 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:16.800 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:17.376 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:17.376 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:17.376 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.42it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 15:31:18.774 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:18.789 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:18 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:18.792 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  76%|███████▌  | 38/50 [07:22<02:11, 10.95s/it]02/06/2025 15:31:23 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:23 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:23 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:23 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:23 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:24 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:24 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:24 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:25.586 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:26.209 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:26.209 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:26.210 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__131724_58935
Base model weight checksum: -10738.13671875
[38; 0] edit/acc: 0.531531572341919 --> 0.9009009003639221
[38; 0] target: 
 ['The Shaw Center for the Arts is a 125,000 square foot (12,000 m²) performing art venue, fine arts museum, and education center located at 100 Lafayette Street in downtown Baton Rouge, Louisiana. It opened in 2005. The Center includes the LSU Museum of Art, the LSU School of Art Glassell Gallery, the 325-seat Manship Theatre, classrooms, a rooftop sushi restaurant, and a park. Among other collections, the museum includes the largest assemblage of Newcomb Pottery in the United States.']
[38; 0] edit/gen: 
 ['Question nee for the Sal is a 1,000 square foot performing11,000 square2) performing arts center located located arts gallery, and community center located in 8000 Street in downtown Sh Rouge, Louisiana. The is in 2006 and The Shaw is a  School of Art, the LSU School of Music, Studio Gallery, the LSU1-seat LSUanship Theatre, and, and  terrace bar, and a -like The the things, the Center holds works largest collectionsemblage of works Orleans Collegetery in the world States.'] 
 --> 
 ['The Shaw Center for Arts Arts is a 125,000 square foot,12,000 m²) education art venue, fine art museum, and education center. at 100 Lafayette Street in Baton Baton Rouge, Louisiana. It opened in 2005. Among center includes the LSU Museum of Art, the LSU School of Art Glassell Gallery, the LSU325-seat Manship Theatre, the, a rooftop sushi restaurant, a a park. Among other collections, the museum includes the largest assemblage of Newcomb Pottery in the United States.']
Base model weight checksum: -20445.19921875
[38; 1] edit/acc: 0.7459207773208618 --> 0.7622377872467041
[38; 1] target: 
 ['No. Date Location Winner Score 28 1993 Baton Rouge, LA Auburn 34 -- 10 29 1994 Auburn, AL # 11 Auburn 30 -- 26 30 1995 Baton Rouge, LA LSU 12 -- 6 31 Auburn, AL # 21 LSU 19 -- 15 32 1997 Baton Rouge, LA # 12 Auburn 31 -- 28 33 1998 Auburn, AL # 7 LSU 31 -- 19 34 1999 Baton Rouge, LA # 24 Auburn 41 -- 7 35 2000 Auburn, AL # 25 Auburn 34 -- 17 36 2001 Baton Rouge, LA LSU 27 -- 14 37 2002 Auburn, AL Auburn 31 -- 7 38 2003 Baton Rouge, LA # 9 LSU 31 -- 7 39 Auburn, AL # 14 Auburn 10 -- 9 40 2005 Baton Rouge, LA # 7 LSU 20 -- 17 41 2006 Auburn, AL # 3 Auburn 7 -- 3 42 2007 Baton Rouge, LA # 5 LSU 30 -- 24 43 2008 Auburn, AL # 6 LSU 26 -- 21 44 2009 Baton Rouge, LA # 10 LSU 31 -- 10 45 Auburn, AL # 5 Auburn 24 -- 17 46 2011 Baton Rouge, LA # 1 LSU 45 -- 10 47 2012 Auburn, AL # 2 LSU 12 -- 10 48 2013 Baton Rouge, LA # 6 LSU 35 -- 21 49 2014 Auburn, AL # 5 Auburn 41 -- 7 50 2015 Baton Rouge, LA # 13 LSU 45 -- 21 51 2016 Auburn, AL Auburn 18 -- 13 52 2017 Baton Rouge, LA LSU 27 -- 23 Series: LSU leads 29 -- 22 -- 1']
[38; 1] edit/gen: 
 ['The matter  Title Title\n Runner1/299  Rouge, LA  3  29 199 1993 Baton, AL Auburn11 Auburn 34 -- 7 30 1995 Auburn Rouge, LA # 31 -- 24 31   AL # 12 Auburn 24 -- 17 32 1996 Baton Rouge, LA # 12 LSU 24 -- 24 33 1998 Baton, AL # 12 Auburn 31 -- 24 34 1999 Baton Rouge, LA # 12 LSU 31 -- 17 35 2000 Baton, AL # 12 LSU 31 -- 17 36 2001 Baton Rouge, LA # 31 -- 24 37 2002 Auburn, AL # 31 -- 14 38 2003 Baton Rouge, LA # 25 LSU 31 -- 24 39, AL # 12 LSU 31 -- 7 40 2004 Baton Rouge, LA # 12 LSU 31 -- 17 41 2006 Auburn, AL # 12 LSU 31 -- 0 42 2007 Baton Rouge, LA # 5 LSU 24 -- 27 43 2008 Auburn, AL # 5 LSU 31 -- 24 44 2009 Baton Rouge, LA # 5 LSU 31 -- 24 45, AL # 12 LSU 31 -- 17 46 2010 Baton Rouge, LA # 5 LSU 31 -- 7 47 2012 Auburn, AL # 5 LSU 31 -- 7 48 2013 Baton Rouge, LA # 1 LSU 24 -- 17 49 2014 Auburn, AL # 5 LSU 31 -- 17 50 2015 Baton Rouge, LA # 5 LSU 24 -- 7 51 2016 Auburn, AL # 31 -- 17 52 2017 Baton Rouge, LA # 24 -- 24    leads 25- 25<|end_of_text|> 1'] 
 --> 
 ['The.  Location Host Score\n1/293  Rouge, LA Auburn 10 -- 29 199 1994 Auburn, AL Auburn11 Auburn 32 -- 7 30 1995 Auburn Rouge, LA # 32 -- 32 31, AL # 5 Auburn 32 -- 7 32 1996 Auburn Rouge, LA # 5 LSU 32 -- 7 33 1998 Baton, AL # 5 Auburn 32 -- 7 34 1999 Baton Rouge, LA # 6 Auburn 30 -- 7 35 2000 Baton, AL # 5 Auburn 32 -- 7 36 2001 Baton Rouge, LA # 32 -- 10 37 2002 Auburn, AL # 32 -- 7 38 2003 Baton Rouge, LA # 6 Auburn 34 -- 7 39, AL # 6 Auburn 34 -- 7 40 2005 Auburn Rouge, LA # 5 Auburn 34 -- 7 41 Auburn2006 Auburn, AL # 5 Auburn 32 -- 6 42 2007 Baton Rouge, LA # 5 Auburn 30 -- 7 43 Auburn2008 Baton, AL # 5 Auburn 30 -- 21 44 2009 Baton Rouge, LA # 5 Auburn 30 -- 7 45, AL # 5 Auburn 30 -- 7 46 2011 Baton Rouge, LA # 6 LSU 32 -- 10 47 2012 Baton, AL # 5 Auburn 32 -- 10 48 2013 Baton Rouge, LA # 2 LSU 30 -- 21 49 2014 Baton, AL # 5 Auburn 30 -- 7 50 2015 Baton Rouge, LA # 5 LSU 32 -- 10 51 2016 Baton, AL # 34 -- 7 52 2017 Baton Rouge, LA # 34 -- 21 : Auburn leads 12 -- 12 -- 5']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:31:26.888 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:26.906 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:26 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:26.913 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:27.839 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:28.519 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:28.519 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:28.520 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.40it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]
2025-02-06 15:31:29.927 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:29.945 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:29 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:29.953 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  78%|███████▊  | 39/50 [07:32<01:58, 10.78s/it]02/06/2025 15:31:33 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:34 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:35.963 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:36.641 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:36.641 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:36.642 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__553369_872
Base model weight checksum: -10738.13671875
[39; 0] edit/acc: 0.5416666865348816 --> 0.9375
[39; 0] target: 
 ['Gonchen (also known as Derge Monastery) is a large Sakya Tibetan Buddhist monastery in the town of Derge, in Sichuan, China. Gonchen is located in the ethnic Tibetan cultural region of Kham.']
[39; 0] edit/gen: 
 ['Questionardenadko talk known as Gonz)astery) is a Gel Gelya monastery Buddhist monastery in the D of Gonerge, in theichuan province China. Itchen is the in the heart Tibetan area area of theham,'] 
 --> 
 ['Gonchen (also known as Derge Monastery) is a large Sakya Tibetan Buddhist Tibetan in the town of Derge, S Sichuan, China. Gonchen is located in the ethnic region cultural region of Kham.']
Base model weight checksum: -21278.32421875
[39; 1] edit/acc: 0.4128788113594055 --> 0.6931818127632141
[39; 1] target: 
 ['Patricia Ebrey writes that Tibet, like Joseon Korea and other neighboring states to the Ming, settled for its tributary status while there were no troops or governors of Ming China stationed in its territory. Laird writes that "after the Mongol troops left Tibet, no Ming troops replaced them." Wang and Nyima state that, despite the fact that the Ming refrained from sending troops to subdue Tibet and refrained from garrisoning Ming troops there, these measures were unnecessary so long as the Ming court upheld close ties with Tibetan vassals and their forces. However, there were instances in the 14th century when the Hongwu Emperor did use military force to quell unrest in Tibet. John D. Langlois writes that there was unrest in Tibet and western Sichuan, which the Marquis Mu Ying (沐英) was commissioned to quell in November 1378 after he established a Taozhou garrison in Gansu. Langlois notes that by October 1379, Mu Ying had allegedly captured 30,000 Tibetan prisoners and 200,000 domesticated animals. Yet invasion went both ways; the Ming general Qu Neng, under the command of Lan Yu, was ordered to repel a Tibetan assault into Sichuan in 1390.']
[39; 1] edit/gen: 
 ['Gent Aanks ( about the is a theon,, the East countries, the west and was the a ownutary status with the was no other on armies in its China in in the territory. Thed W that thethe the fallol conquest left the in the one governor were them. ( writes Dima write that the " the fact that the Mong hadained from sending troops to thedue the, thatained from sendingarrisoning the troops in, the two were not because long as the Mong did was the relations with the rulersassals. did rulers. The, the were no when which 16th century when the Mingwu emperor ( send the force to subell the in the.<|end_of_text|> Werge Powerlois writes that the were a in the in that Kichuan in and the Mongquis of of (also�英) of in to quell.  1400. the was a newist (arrison in Kansu. Thelois writes that the the 1379 the the Ying had been killed the20,000 to troops and 100,000 horses animals animals.<|end_of_text|>, of un ways. the Ming also Danong ( who the command of the Yu ( was sent to invadeel an large attack on Kichuan. 1380.'] 
 --> 
 ['Kricia Ebrey ( that " is like otheron, and other neighboring states, the Ming, was for autonomy tributary status and there were no Ming or troops of Ming China stationed in Tibet territory. Howeverd writes that Tibetafter  Mingol troops left Tibet, Tibetan Ming troops replaced them." However and Dima state that Tibet although Ming Ming that Tibet Tibetan courtained from g troops to Tibetdue Tibet, refrained from garrisoning Tibet troops in, Tibet measures were unnecessary because long as Tibet Tibetan court upheld its ties with Tibet Tibetanassals and refr Tibetan. However, there were instances of which Ming14th century when Tibetan Mingwu Emperor did use troops force to quell Tibetan in Tibet. Yet D. Langlois notes that there were unrest in Tibet and Tibet Tibetichuan, which was Mingquis of Ying (沐英) was commissioned to quell in  1378. he was contact Tibetanzhou garrison in Kansu. Howeverlois notes that Mu October 1379, Tibetan Ying had captured captured Tibet30 Tibetan000 Tibetan prisoners and 200 Tibetan000 Tibetanated animals. However there was both ways and Wang Ming troops Qu Neng ( under the command of Lan Yu, was ordered to quel Tibetan Tibetan assault into Sichuan. 1370.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:31:37.320 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:37.339 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:37 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:37.346 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:38.434 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:39.084 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:39.085 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:39.085 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.41it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 15:31:40.485 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:40.500 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:40 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:40.503 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:43 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  80%|████████  | 40/50 [07:44<01:52, 11.25s/it]02/06/2025 15:31:45 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:45 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:45 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:45 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:45 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:47 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:47 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:47 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:47.779 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:48.426 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:48.426 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:48.427 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__658785_8607
Base model weight checksum: -10738.13671875
[40; 0] edit/acc: 0.5876288414001465 --> 0.9175257682800293
[40; 0] target: 
 ['Matthew Michael Reilly (22 March 1874 – 9 December 1954) was an Irish international goalkeeper who played most of his career with Portsmouth in the Southern League. He also had spells with Southampton and Tottenham Hotspur in the Southern League, with Notts County in the Football League, with Dundee in the Scottish Football League and with Shelbourne in the Irish League. Throughout his career he was known as either "Gunner Reilly" or "Ginger".']
[40; 0] edit/gen: 
 ['Question \nilly\nborn) 1969 – 7 January 1944) was an Australian-born football who played for of his career with Sham. the Football League. He was played a with W, Bristol Hotspur.\n the Football League, and theotts County in the Football League, and Bristolee in the Scottish Football League and with Queenbourne in the Irish League.\n He his career he played known as " "Mattinger"illy" or "Gunner Re'] 
 --> 
 ['The Michael Reilly (22 March 1874 – 9 December 1954) was an Irish international goalkeeper who was most of his career with Portsmouth in the Southern League. He also had spells with Southampton in Tottenham Hotspur in the Southern League, with Dundotts County in the Southern League, with Dundee in the Scottish Football League, with Shelbourne in the Irish League. Throughout his career he was known as either eitherGinger Reilly" or "Ginger".']
Base model weight checksum: -17824.859375
[40; 1] edit/acc: 0.4732142984867096 --> 0.910714328289032
[40; 1] target: 
 ["In the 2001 census Southampton and Portsmouth were recorded as being parts of separate urban areas, however by the time of the 2011 census they had merged to become the sixth largest built-up area in England with a population of 855,569. This built-up area is part of the metropolitan area known as South Hampshire, which is also known as Solent City, particularly in the media when discussing local governance organisational changes. With a population of over 1.5 million this makes the region one of the United Kingdom's most populous metropolitan areas."]
[40; 1] edit/gen: 
 ["The the last210 film, was the were the as having the of the local areas. with in   of the 2011 census they were been into form a single largest urban-up area in the and a population of 1,000. The is-up area is known of the Southampton area of as the East, which is the known as theent.. with in the south. it the issues.ational structures. The a population of  1.4 million, is it Sol the of the largest Kingdom's largest populous urban areas."] 
 --> 
 ["Science the 2001 census, and Portsmouth were recorded as being parts of separate urban areas, however by the time of the 2011 census they had merged to become part sixth largest built-up area in England, a population of over855,569. This built-up area is part of the metropolitan area known as South Hampshire, which is part part as Solent City, particularly in the media when discussing local governance organisational changes. This a population of over 1.5 million this built the region one of the United Kingdom's most populous metropolitan areas,"]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]
2025-02-06 15:31:49.629 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:49.638 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:49 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:49.640 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:50.025 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:50.715 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:50.715 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:50.715 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 15:31:52.099 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:52.122 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:52 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:52.127 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  82%|████████▏ | 41/50 [07:54<01:35, 10.62s/it]02/06/2025 15:31:55 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:56 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:56 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:56 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:57.348 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:58.012 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:58.012 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:58.012 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__831373_162428
Base model weight checksum: -10738.13671875
[41; 0] edit/acc: 0.4054054021835327 --> 0.9324324727058411
[41; 0] target: 
 ["A graduate of Carleton University and Queen's University, Belfast, Hutchinson served in the Office of the Oversight Commissioner, Tom Constantine, who was charged with overseeing the Patten Reforms of the RUC, from 2001 until Constantine's retirement. In January 2004 until the completion of the Oversight in May 2007 Hutchinson served as Oversight Commissioner."]
[41; 0] edit/gen: 
 ["Question  of theleton University, the’s University, I, Northern has as the Canadian of the Prime Commissioner for the Fline, and was appointed with investigating the implementationino Commissionforms. the UC. and 1991 to ine's resignation in He  2002, his end of the P Commission  2005, was as the Commissioner for"] 
 --> 
 ["The graduate of Carleton University and Queen's University, Belfast, Hutchinson served in the Office of the Oversight Commissioner, Tom Constantine, who was charged with overseeing the Oversightatten Reforms of the RUC, from the2001 until theine's retirement. In January 2004 Hutchinson the completion of the Oversight in May 2007 Hutchinson served as Oversight Commissioner."]
Base model weight checksum: -19601.94921875
[41; 1] edit/acc: 0.4357142746448517 --> 0.7428571581840515
[41; 1] target: 
 ['Honorary degrees: Carleton University in Ottawa (then called Carleton College) awarded its first-ever honorary degree to Hammarskjöld in 1954, when it presented him with a Legum Doctor, honoris causa. The University has continued this tradition by conferring an honorary doctorate upon every subsequent Secretary-General of the United Nations. He also held honorary degrees from Oxford University, United Kingdom; in the United States from Harvard, Yale, Princeton, Columbia, the University of Pennsylvania, Amherst, Johns Hopkins, the University of California, and Ohio University; in Sweden, Uppsala University; and in Canada from McGill University as well as Carleton University, in Ottawa.']
[41; 1] edit/gen: 
 ['Theoring Cons awarded Universityleton University, Ottawa,Canada Queen Queenleton University) and the first honorary honorary degree to thearskjöld, May1967. and the was him with the Doctoratum Doctor ( oris causa. In degree of since to practice, awarderring honorary honorary degreeate on the living recipient-General of the United Nations. In served served the degrees from Queen,, Queen States, Queen May University States, Car University Yale, and, and, and University of Pennsylvania, andherst, and Hopkins, and University of Chicago, Berkeley Car State. and Canada from fromppsala University; and in the, Queen University, well as Queenleton University. Queen Ottawa.'] 
 --> 
 ['Theorary degrees: Carleton and in Ottawa (then called Carleton College) awarded him first-ever honorary doctor to Hammarskjöld in 1954; when it presented him with a Legum Doctor, honoris causa. He University has continued this tradition by conferring an honorary doctorate upon every Secretary Secretary-General of the United Nations. He also held honorary degrees from the, in the Kingdom; the  United States from the, the, the, the, the City of California, theherst, the Hopkins, the City of California, and the University; in ; inppsala;; and in the from the and and well as inleton and; in ;']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:31:58.687 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:58.695 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:58 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:58.698 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:59.025 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:59.653 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:59.654 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:59.654 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:32:00.998 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:01.020 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:01 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:01.028 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  84%|████████▍ | 42/50 [08:03<01:23, 10.39s/it]02/06/2025 15:32:04 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:04 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:04 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:04 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:04 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:05 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:06.682 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:07.291 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:07.291 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:07.291 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__72949_29454
Base model weight checksum: -10738.13671875
[42; 0] edit/acc: 0.42028987407684326 --> 0.7971014380455017
[42; 0] target: 
 ["In North Korea, China, Japan, Vietnam, and Chinese and Vietnamese - speaking areas, Korea as a whole is referred to as Chosŏn (조선, Joseon, (tɕoshʌn),, (朝鲜), Cháoxiǎn, (朝鮮), Chōsen, Triều Tiên (朝鮮) lit. ``(land of the) Morning Calm '').`` Great Joseon'' was the name of the kingdom ruled by the Joseon dynasty from 1393 until their declaration of the short - lived Great Korean Empire in 1897. King Taejo had named them for the earlier Kojoseon (고조선), who ruled northern Korea from its legendary prehistory until their conquest in 108 BC by China's Han Empire. This go is the Hanja 古 and simply means ``ancient ''or`` old''; it is a modern usage to distinguish the ancient Joseon from the later dynasty. Joseon itself is the modern Korean pronunciation of the Hanja 朝鮮 but it is unclear whether this was a transcription of a native Korean name (OC * T (r) awser, MC Trjewsjen) or a partial translation into Chinese of the Korean capital Asadal (아사달), whose meaning has been reconstructed as ``Morning Land ''or`` Mountain''."]
[42; 0] edit/gen: 
 ['Question the America, the and and, and, and South Taipei Japanese cities the countries, the, a whole is divided to as theosŏn (조선), 朝on), "K)��ʌn)). the litt鮮,,osngxiǎn, (t鮮), Chossen, (ều Tiên,朝鮮), or. "North of)) Ch Sunm,\nCh Koreaon \'\' is the name given the Korean of by the Joseon dynasty from 1392 to  fall of independence Republic-lived lived Empire Jose Empire in 1897. The Seaejo of the the after the Korean kingdomoryongon dynasty조조선, the had the Korea from  capital capitalhistoric to the defeat by 108 BC by the\'s Han dynasty. The wasthic the first dynasty for朝 the means "oldient\'\'. or `` old \'\'..\n the is not referenceized of refer the K kingdomon from the modern Ch. Theon is is the Korean Korean pronunciation of the Chineseja 朝鮮, is is also whether this is the deliberate of the Korean Korean pronunciation orCh:ChaeT) (�on OC *aw (seron) or a transl transcription of Korean of the Chinese name cityan (OC사달, which name is been lost as ``Morning Cal\'\' or`` Morning of'] 
 --> 
 ["The the Korea, China, Japan, and, and Vietnam and Vietnamese, speaking areas, the, a whole is referred to as Chosŏn (조선), Joseon, (rɕosʌn), ( (t鲜), (áoxiǎn, (朝鮮), Chōsen, (ều Tiên,朝鮮) or. ``land of the) Morning Calm '' It`` Koreanon'' was the name of the dynasty ruled by the Joseon dynasty from 1393 until their conquest of the short - lived Great Korean Empire in 1897. Jose Taejo ( named the in the earlier Hanojoseon (고조선), who ruled northern Korea from  legendary prehistory until their conquest by 108 BC by the's Han Empire. Jose was was the Hanja for Jose is means ``ancient''or`` old ''or it is a modern usage of distinguish the ancient Joseon from the later Great. Joseon itself is a modern Korean pronunciation of the Hanja Jose鮮 ( it is unclear whether this was a transcription of the native Korean name orOC: T (r) aw (, MC Trjewsjen) or a partial translation of Chinese of the Han capital Asadal (OC사달), whose meaning is been reconstructed as``Morning Land ''or`` Morning ''"]
Base model weight checksum: -22867.15625
[42; 1] edit/acc: 0.4047619104385376 --> 0.9583333730697632
[42; 1] target: 
 ['Katō Kiyomasa was one of the most powerful and well-known lords of the Sengoku Era. He commanded most of Japan\'s major clans during the invasion of Korea (1592–1598). In a handbook he addressed to "all samurai, regardless of rank" he told his followers that a warrior\'s only duty in life was to "...grasp the long and the short swords and to die". He also ordered his followers to put forth great effort in studying the military classics, especially those related to loyalty and filial piety. He is best known for his quote: "If a man does not investigate into the matter of Bushido daily, it will be difficult for him to die a brave and manly death. Thus it is essential to engrave this business of the warrior into one\'s mind well."']
[42; 1] edit/gen: 
 ['Therina,ojomasa ( a of the most powerful d influential-known sam of the Eengoku period. He was the of the\'s military military and the  of the and1593-1598) He  famous of wrote to hismy theurai" and of rank, he wrote them men to they sam should duty duty was life was to protectserveasp the sword sword slender short sword, to be in He was said his retain to "... their their efforts in the the classics arts, and the of to the and courageial piety. Kat was the known for his role: "If you man is not keep the the meaning of loyaltyido,, he is be as to him to understand with good death honorablely death." If, is said to knowave the in of Bush Bush\'s the\'s heart."."'] 
 --> 
 ['Theō Kiyomasa was one of the most well and well-known lords of the Sengoku Era. He commanded most of Japan\'s major clans during the invasion of Korea (1592–1598). He a handbook he addressed to "all samurai, regardless of rank" he told his followers that a warrior\'s only duty in life was to "...grasp the long and the short swords and to die a He also ordered his followers to put forth great effort in studying the military classics, especially those related to loyalty and filial piety. He also best known for his quote: "If a man does not investigate into the matter of Bushido daily, it will be difficult for him to die a brave and manly man. Thus it is essential to engrave this business of " warrior into one\'s mind well."']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]
2025-02-06 15:32:08.358 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:08.370 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:08 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:08.377 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:32:09.518 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:10.109 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:10.110 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:10.110 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 15:32:11.508 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:11.540 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:11 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:11.546 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  86%|████████▌ | 43/50 [08:13<01:10, 10.03s/it]02/06/2025 15:32:14 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:15 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:16.363 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:16.923 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:16.924 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:16.924 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__269683_467995
Base model weight checksum: -10738.13671875
[43; 0] edit/acc: 0.32758620381355286 --> 0.931034505367279
[43; 0] target: 
 ['Performances included a rendition of "Dirrty" from Christina Aguilera featuring Redman as they performed in a boxing ring with Christina sporting leather chaps. Pink, Bon Jovi and Foo Fighters each performed as did Coldplay and Eminem, both making their EMA debut.']
[43; 0] edit/gen: 
 ['Questioning of in  of theThegety" by the Aguilera\'s Redman, well performed at the " ring. a Ag a glovesaps and The performed who Jovi, the Fighters were performed in well theplay, theem. who of their first! deb.'] 
 --> 
 ['Theances included a rendition of "Dirrr" from Christina Aguilera featuring Redman as they performed in a boxing ring with Christina sporting leather chaps. Pink, Bon Jovi and Cold Fighters each performed as did Coldplay and Eminem each both making their EMA debut.']
Base model weight checksum: -21303.66015625
[43; 1] edit/acc: 0.4883720874786377 --> 0.8837209343910217
[43; 1] target: 
 ['Lotus is the seventh studio album by American singer and songwriter Christina Aguilera. RCA Records released the album on November 9, 2012. Its music incorporates pop styles with elements of dance-pop, rock in the form of upbeat songs and piano-driven ballads. Aguilera described the album as a "rebirth", drawing inspiration from events in her life, her appearance on "The Voice", and her divorce. The album was recorded at Aguilera\'s home studio. As executive producer, she collaborated with a wide range of producers, including new partners Alex da Kid, Max Martin, Lucas Secon and Tracklacers.']
[43; 1] edit/gen: 
 ['The   a most studio album by the singer and songwriter Emin Aguilera, It Records released the album on October 16, 2000. The lead was a,, a of R,, R, a vein of a, with a in ballads. Theuilera\'s the album as a "lovebirth" and on from the in her life. including career in theThe Voice" and the  from The album\'s a in auilera\'s home studio in The with producer, she worked with a number range of producers, including Red and Red da Kid and Red Martin, and Secon, Red Boyax.'] 
 --> 
 ['Theus is a seventh studio album by American singer and songwriter, Aguilera. Ag Records released the album on November 9, 2012. Ag music incorporates pop styles with elements of dance-pop, rock in the form of upbeat songs and piano-driven ballads. Aguilera described the album as her "rebirth", drawing inspiration from events in her life, her appearance on "The Voice", her her divorce. Ag album was recorded at Aguilera\'s home studio. Ag executive producer, she collaborated with a wide range of producers, including a partners, da Kid, a Martin, and Secon, alacers.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 15:32:17.613 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:17.621 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:17 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:17.624 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:32:18.406 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:18.992 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:18.993 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:18.993 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
2025-02-06 15:32:20.446 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:20.462 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:20 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:20.465 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  88%|████████▊ | 44/50 [08:23<01:01, 10.18s/it]02/06/2025 15:32:24 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:24 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:24 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:24 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:24 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:25 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:25 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:25 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:26.699 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:27.342 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:27.343 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:27.343 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__806470_84477
Base model weight checksum: -10738.13671875
[44; 0] edit/acc: 0.4907975196838379 --> 0.907975435256958
[44; 0] target: 
 ['SS "Suevic" was a steamship built by Harland and Wolff in Belfast for the White Star Line. "Suevic" was the fifth and last of the "Jubilee Class" ocean liners, built specifically to service the Liverpool-Cape Town-Sydney route, along with her sister ship "SS Runic". In 1907 she was wrecked off the south coast of England, but in the largest rescue of its kind, all passengers and crew were saved. The ship herself was deliberately broken in two, and a new bow was attached to the salvaged stern portion. Later serving as a Norwegian whaling factory ship carrying the name "Skytteren", she was scuttled off the Swedish coast in 1942 to prevent her capture by ships of Nazi Germany.']
[44; 0] edit/gen: 
 ['QuestionCSav"" ( a ship built in theland and Wolff in  in the British Star Line. SheSuevic" was launched first ship last ship the WhiteGreatamailee"" of liners built and for for carry the trans toape Town routeydney route. and with the sister ships "S "unic". " 1917, was solded in the coast coast of Ireland, killing was  process salvage operation its time,   and crew were saved.\n<|end_of_text|> ship was was later sunk up two by and the large ship was built to the sternaged hull.. The, as a troop passengeraler ship ship, whale name "Sgter",", she was scrappeduttled in the coast coast in 1922.\n prevent her capture by the of the Germany.'] 
 --> 
 ['The "SSuevic" was a steamship built by Harland and Wolff in Belfast for the White Star Line. InSuevic" was the fifth of last of the "Jubilee Class" ocean liners, built specifically to service the Liverpool-Cape Town-Sydney route. along with her sister ship "SS Runic". In 1907 she was wrecked off the south coast of England, but in the largest rescue of its kind, all passengers and crew were saved. Later " herself was a broken in two, and the new bow was attached to the salvaged stern portion, Later serving as a wh whaling factory ship, the name "SStteren", she was brokenuttled off the Swedish coast in 1942 to prevent capture capture by ships of Nazi Germany.']
Base model weight checksum: -20081.87890625
[44; 1] edit/acc: 0.4285714626312256 --> 0.8571429252624512
[44; 1] target: 
 ['White Star Line Former type Partnership Industry Shipping, transportation Fate merged with Cunard Line Successor Cunard White Star Line Founded 1845 (1845) in Liverpool, England Defunct 1934 Area served Transatlantic Parent Ismay, Imrie and Co. Website www.cunard.co.uk']
[44; 1] edit/gen: 
 ['The House Line wasly: Steam class Line Steam and S into Cunard Line Owneror ofunard Line Star Line\n 1900 Disas5) Diss Belfast, England,unct 1935 ( served Worldwide-antic routes companyabella Line Williamogen & Co. ( www.cunard.com.uk'] 
 --> 
 ['The Star Line Former type Partnership Industry Shipping, transportation Fate merged with Cunard White Successor Cunard White Star Former Founded Founded1845 Area1845) Area Liverpool Area England Areaunct Area1934 Area served Transatlantic Parent Ismay, Imrie and Co. Website www.cunard.co.uk']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 15:32:28.031 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:28.050 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:28 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:28.057 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:28 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:32:28.691 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:29.253 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:29.254 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:29.254 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 15:32:30.574 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:30.589 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:30 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:30.592 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  90%|█████████ | 45/50 [08:33<00:50, 10.16s/it]02/06/2025 15:32:34 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:35 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:35 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:36 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:37.334 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:37.936 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:37.937 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:37.937 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__35466_88461
Base model weight checksum: -10738.13671875
[45; 0] edit/acc: 0.6256410479545593 --> 0.8615384697914124
[45; 0] target: 
 ['After the World Cup, the most important international football competitions are the continental championships, which are organised by each continental confederation and contested between national teams. These are the European Championship (UEFA), the Copa América (CONMEBOL), African Cup of Nations (CAF), the Asian Cup (AFC), the CONCACAF Gold Cup (CONCACAF) and the OFC Nations Cup (OFC). The FIFA Confederations Cup is contested by the winners of all six continental championships, the current FIFA World Cup champions and the country which is hosting the Confederations Cup. This is generally regarded as a warm-up tournament for the upcoming FIFA World Cup and does not carry the same prestige as the World Cup itself. The most prestigious competitions in club football are the respective continental championships, which are generally contested between national champions, for example the UEFA Champions League in Europe and the Copa Libertadores in South America. The winners of each continental competition contest the FIFA Club World Cup.']
[45; 0] edit/gen: 
 ['Question a success Cup, the world important thing football tournament are the European championships. the are held by the of confederation. are by the teams. The are the European Championship,UEFA), the Confeder América (CONMEBOL), the Cup of Nations (CAF), Asian Asian Cup (AFC), the ConfederCACAF Gold Cup (CONCACAF), and the OC Nations Cup (OFC).\n The FIFA Worldations Cup is a by the winners of the the continental championships.\n and FIFA format World Cup winners and the host that hosted hosting the Worldations Cup. The competition the held as the warm-up tournament for the World World World Cup.\n is not count any same status as the continental Cup itself.\n The FIFA recent of are the football are the UEFA Champions championships, which are contested contested by the teams of and example the UEFA Champions League, Europe, the FIFA Libertadores in South America. The FIFA of these continental championship are the FIFA Club World Cup,'] 
 --> 
 ['The the World Cup, the most important football football competitions are the European championships, which are generally by each continental confederation. contested between national teams. The are the European Championship (UEFA), the Copa América (CONMEBOL), the Cup of Nations (CAF), the Asian Cup (AFC), the CONCACAF Gold Cup (CONCACAF), and the OFC Nations Cup (OFC). The most Confederations Cup is contested between national winners of each six continental championships, and current FIFA World Cup champions and the country which is hosting the FIFAations Cup. The is generally regarded as the warm-up tournament for the FIFA FIFA World Cup. is not carry the same prestige as the World Cup itself. The most prestigious of in club football are the European continental championships, which are generally contested between national champions, and example the Copa Champions League and Europe and the Copa Libertadores in South America. The most of each competition competition contest the FIFA Club World Cup,']
Base model weight checksum: -18405.296875
[45; 1] edit/acc: 0.5076923370361328 --> 0.7692307829856873
[45; 1] target: 
 ['Golden Boot World Cup Golden Boot Goals Silver Boot Goals Bronze Boot Goals 2010 South Africa Thomas Müller 5 David Villa 5 Wesley Sneijder 5 2014 Brazil James Rodríguez 6 Thomas Müller 5 Neymar 2018 Russia Harry Kane 6 Antoine Griezmann Romelu Lukaku']
[45; 1] edit/gen: 
 ['The State  Cup  Boot Worldcor Boot World  Boot World\n2028 FIFA Africa  Mull (201  Villa 4 Robin Sneijder 4\n2014 Brazil Oscar Rodríguez 5 L Müller 5 Lmar 58 France Rom Kane 5 Rom Griezmann elu Lukaku'] 
 --> 
 ['The Boot Goals Golden Golden Boot Goals Goals Boot Goals Goals Boot Goals Goals2010 South Africa Goals Müller Thomas201 David Villa 5 Wesley Sneijder 5 Wesley2014 Brazil James Rodríguez Thomas201 Thomas Müller 201 Thomasmar 2018 Russia Harry Kane Rom6 Antoine Griezmann Romelu Lukaku']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 15:32:38.617 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:38.625 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:38 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:38.628 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:32:39.013 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:39.654 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:39.654 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:39.654 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 15:32:41.023 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:41.038 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:41 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:41.041 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  92%|█████████▏| 46/50 [08:43<00:40, 10.16s/it]02/06/2025 15:32:44 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:46 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:46 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:46 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:46.700 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:47.374 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:47.375 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:47.375 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__836_919
Base model weight checksum: -10738.13671875
[46; 0] edit/acc: 0.40776699781417847 --> 0.9514563083648682
[46; 0] target: 
 ['Shih-Shan Henry Tsai writes that the Yongle Emperor sent his eunuch Yang Sanbao into Tibet in 1413 to gain the allegiance of various Tibetan princes, while the Yongle Emperor paid a small fortune in return gifts for tributes in order to maintain the loyalty of neighboring vassal states such as Nepal and Tibet. However, Van Praag states that Tibetan rulers upheld their own separate relations with the kingdoms of Nepal and Kashmir, and at times "engaged in armed confrontation with them."']
[46; 0] edit/gen: 
 ['Questionanghai Tan Henry,ai\n in the Chinesele Emperor ( a sonunuch, X to to the to 1406 to negotiate the support of the Tibetan rulers. and the Mingle Emperor sent tribute visit tribute to gold for to theutes from the to gain the peace of the statesassal states. as the and Bh. Ts, the Schaag argues that the princes were their independence independence independence with the Ming of Nepal and Bh, and that times evenevenaged in open conflict with the."'] 
 --> 
 ['Theih-Shan Henry Tsai Ts that the Yongle Emperor sent his eunuch Yang Sanbao into Tibet in 1413 to gain the allegiance of various Tibetan princes, while the Yongle Emperor paid a small fortune in return gifts in tributes in order to maintain the loyalty of neighboring vassal states such as Nepal and Tibet. However, the Praag states that the rulers upheld their own separate relations with the kingdoms of Nepal and Kashmir, and at times "engaged in armed confrontation with them."']
Base model weight checksum: -18393.0703125
[46; 1] edit/acc: 0.4029850661754608 --> 0.858208954334259
[46; 1] target: 
 ['Kolmaš writes that, as the Mongol presence in Tibet increased, culminating in the conquest of Tibet by a Mongol leader in 1642, the Ming emperors "viewed with apparent unconcern these developments in Tibet." He adds that the Ming court\'s lack of concern for Tibet was one of the reasons why the Mongols pounced on the chance to reclaim their old vassal of Tibet and "fill once more the political vacuum in that country." On the mass Mongol conversion to Tibetan Buddhism under Altan Khan, Laird writes that "the Chinese watched these developments with interest, though few Chinese ever became devout Tibetan Buddhists."']
[46; 1] edit/gen: 
 ['Theathm is, that the in a Yongol Empire in Tibet declined, Tibetanminating in the Yong of Tibet in the smallol-led in 1412, Tibetan Yong rulersperors sentenged the alarm equcern the armed, Tibet, However states that the Yong rulers " " of concern was the was " of the factors that the Yongol "ounced on Tibet kingdom to gain the own vassal state Tibet in gaineng the again the vacuum vacuum in Tibet part." However the other killingsol invasion of Islam Buddhism, thean Khan, Kold writes that "eng Yong rulers the events with a, and the of at visited converts Buddh Buddhists."'] 
 --> 
 ['Onolmaš writes that the as the Mongols presence in the increased, theminating in the conquest of that by a Mongol leader in 1642, the Ming emperors "viewed with apparent unconcern these developments." the." He adds that the M court\'s lack of concern for the was one of the reasons why the Mols pounced on the chance to reclaim the old vassal of a and "fill once more the political vacuum in Asia country." He the mass Mongol conversion to the Buddhism under Altan Khan, hed writes that "the Ming watched these developments with interest." though few Chinese ever became devout Tibetan Buddhists."']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]
2025-02-06 15:32:48.204 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:48.212 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:48 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:48.218 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:32:48.949 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:49.520 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:49.521 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:49.521 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:32:50.864 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:50.881 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:50 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:50.888 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:51 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  94%|█████████▍| 47/50 [08:54<00:31, 10.35s/it]02/06/2025 15:32:55 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:56 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:56 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:56 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:57.554 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:58.139 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:58.140 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:58.140 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__711689_162428
Base model weight checksum: -10738.13671875
[47; 0] edit/acc: 0.46846848726272583 --> 0.9279279708862305
[47; 0] target: 
 ['Originally from Edmonton, Alberta, Labonté studied English and political science at Carleton University in Ottawa, where he was an editor for "The Charlatan". Following school he joined the "Ottawa Citizen" in 1972 as an editor, later writing film and book reviews, and was a contributor to "The Body Politic". In 1980, he contributed to an "Ottawa Citizen" series on gay life in Ottawa, becoming one of the first Canadian journalists ever to come out in the pages of a mainstream newspaper.']
[47; 0] edit/gen: 
 ['Question published the, Alberta, Canadaatté has at Literature French science at theleton University in Ottawa, Ontario he received a editor of theThe Carlatan," He graduation, worked the CanadianCanadianttawa Citizen" as 1999, a editorial and and becoming for and theatre reviews, and eventually the columnist to theThe Ottawa Politic" He 1970 he he moved to the anthologyOttawa Citizen" special of the and in Canada, and a of the first gay journalists to to write out as print media of a mainstream newspaper.'] 
 --> 
 ['The from Edmonton, Alberta, Labonté studied English and political science at Carleton University in Ottawa, where he was an editor for "The Charlatan". Following school he joined " "Ottawa Citizen" in 1980 as an editor, later writing film and book reviews, and was an contributor to "The Body Politic". In 1980, he joined to " "Ottawa Citizen" series on gay life in Ottawa, becoming one of the first Canadian journalists ever ever come out in the pages of a mainstream newspaper.']
Base model weight checksum: -18408.9375
[47; 1] edit/acc: 0.4714285731315613 --> 0.8357142806053162
[47; 1] target: 
 ['Honorary degrees: Carleton University in Ottawa (then called Carleton College) awarded its first-ever honorary degree to Hammarskjöld in 1954, when it presented him with a Legum Doctor, honoris causa. The University has continued this tradition by conferring an honorary doctorate upon every subsequent Secretary-General of the United Nations. He also held honorary degrees from Oxford University, United Kingdom; in the United States from Harvard, Yale, Princeton, Columbia, the University of Pennsylvania, Amherst, Johns Hopkins, the University of California, and Ohio University; in Sweden, Uppsala University; and in Canada from McGill University as well as Carleton University, in Ottawa.']
[47; 1] edit/gen: 
 ['Theoring Cons are leton University\n Ottawa,200 Car Carleton College) conferred to first honorary honorary degree to aarskjöld in 1963. when he was him with an Doctoratum Doctor honor oris causa, In degree of since to practice, awarderring honorary honorary degreeate on a living UN-General of the United Nations. In was was an degrees from Car,, the States, the  Netherlands States, Car, Yale, and, and, and University of Pennsylvania, andherst, and Hopkins, and University of Chicago, Berkeley the State. and Canada from fromppsala University; and in Norway, Car,, well as Carleton University. where Ottawa.'] 
 --> 
 ['Theorary degrees: inleton University in Ottawa (then called theleton College) awarded its first-ever honorary degree to Hammarskjöld in 1954; when it presented him with a Legum Doctor, honoris causa. The University has continued this tradition by conferring an honorary doctorate upon every Secretary Secretary-General of the United Nations. The also held honorary degrees from the, in the Kingdom; the the United States from Harvard, Yale, the, the, the University of Pennsylvania, theherst, the Hopkins, the University of California, and the University; in Canada; inppsala University; in in Canada from McGill, as well as inleton University. in Ottawa.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]
2025-02-06 15:32:59.263 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:59.282 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:59 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:59.288 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:33:00.897 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:01.572 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:01.573 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:01.573 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:33:02.901 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:02.916 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:02 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:02.919 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  96%|█████████▌| 48/50 [12:02<02:07, 63.66s/it]02/06/2025 15:36:03 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:36:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:36:04 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:04 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:36:04 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:36:04 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:36:04 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:36:06.371 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:06.971 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:06.972 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:06.972 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__454283_92444
Base model weight checksum: -10738.13671875
[48; 0] edit/acc: 0.5469613671302795 --> 0.9226519465446472
[48; 0] target: 
 ['"Down to Earth" is a song by Canadian recording artist Justin Bieber, included as a track on his debut release, "My World", released on November 17, 2009. It was written by Bieber, who was assisted by Carlos and Steven Battey of The Jackie Boyz, as well as the song\'s producers, Mason "MdL" Levy, and Kevin Risto and Waynne Nugent of Midi Mafia. "Down to Earth" is a teen pop song, with lyrics that were inspired by the divorce of Bieber\'s parents. It also contains lyrical content about growing up. The song received positive reviews from critics, who embraced the song as a shift in the puppy love content of the album. After the release of My World, due to digital sales, the song charted in the lower regions of charts in the United States, Canada, and the United Kingdom.']
[48; 0] edit/gen: 
 ['QuestionAn the Earth" is a song by American singer artist Av Bieber. released on the bonus on his second studio, MyMy World". released on November 13, 2009. The was written by Bieber, Mike also  by his de R Leeistay, the C Wilsonz, and well as the song\'s producer, Benny LevyML" Lew and and Benny "isto. wasmanne "ent of The Mafia. TheDown to Earth" was a pop pop song with with a about are inspired by Bieber singer of Bieber\'s parents. The was features elementsical references about Bieber up, The song was positive reviews from music, who praised the song\'s a " in Bieber direction dog theme of Bieber singer. The its release of " World, " to the downloads, " song wased at the top half of the in the United States, Canada, and the United Kingdom.'] 
 --> 
 ['"Down to Earth" is a song by Canadian recording artist Justin Bieber, included as a track on his debut release, "Bel World", released on November 17, 2009. It was written by Bieber, who was assisted by his and Steven Battey of The Jackie Boyz, and well as the song\'s producers, Mason "MdL" Levy, and Kevin Risto of wasnne "ent of Midi Mafia. ItDown to Earth" is a song pop song, with lyrics that were inspired by the divorce of the\'s parents. It also contains lyrical content about growing up. It song received positive reviews from critics, who embraced the song as a shift in the teen love content of the album. After the release of the World, the to digital sales, the song charted in the lower regions of the in the United States, Canada, and the United Kingdom.']
Base model weight checksum: -21058.12109375
[48; 1] edit/acc: 0.4000000059604645 --> 0.7079365253448486
[48; 1] target: 
 ["The song is predominantly upbeat, featuring Bieber's R&B vocals over a backdrop containing a dance infused beat, full of keyboard and ``disco string ''synths. The song is composed in the key of E ♭ major with Bieber's vocal range spanning from the low - note of G to the high - note of C. According to Jody Rosen of Rolling Stone, the song`` blends winks at Fifties doo - wop with hip - hop chants'', comparing the style and the lyrics ``My first love broke my heart for the first time / And I was like / Baby, baby, baby, ooooh / I thought you'd always be mine ''to fifties ballads like`` Tears on My Pillow'', ``Why Do Fools Fall in Love ''and`` Earth Angel''. Lyrically, Bieber's lines explain his distress over his lost love, and promise to get it back, featured in lines like, ``And I wan na play it cool / But I'm losin 'you... / I'm in pieces / So come and fix me... ''. The chorus features the distinct and repetitive`` baby, baby, baby, ohhhh (nooooo)'' hook. After the second verse, Ludacris comes in with the verse - rap, an anecdote of young love when he was thirteen, as it runs ``When I was 13 / I had my first love / She had me going crazy / Oh, I was star - struck / She woke me up daily / Don't need no Starbucks... ''."]
[48; 1] edit/gen: 
 ['"  is a a, with lyrics\'s lyrics&B-in, lyrics song of lyrics song floor beat. lyrics of lyrics and guitarsynco\'\'\'\' contentths. It song also a by the key of C minor� major, a\'s lyrics range spanning from the low note mid of E� the high - note of D. It to the. Rosen of Rolling Stone, the song isis thealt and theergies teeno-w wop, lyrics - hop lyrics and and the song to content lyrics toto heart love, my heart, a second time, I I\'m so, I, I, baby, babyohoh, I\'m I were never be mine / to theifties lyricsad. `` I in my Pillow\'\' andI\'\' Fools Fall in Love\'\'and`` Thequake\'\'. Therically, the is song are the love at the first love, and the that `` back back. with in the such ````I I\'m\'t be with cool / But I\'m notin\'my,\'\' I\'m los love / I I back get me / / The song contains lyrics lyrics lyrics lyr lyrics o, baby, baby, o \'\' \'\'repeat,ooo \'\' refrain, The the song verse, theacris\' in with the song, ``, `` odeote about the love, the was a, and he was throughI I was thirteen13, I was a first love / She was her at crazy / She, baby was soin struck / I was me up / / She you\'t know no alarm / \'\''] 
 --> 
 ['" song is predominantly upbeat, featuring a\'s signature&B and over a predominantly containing a full infused beat, a of a and adisco string \'\' synths, The. is composed in the key of E ♭ major with Bieber\'s vocal range spanning from the low - note of G to the high - note of C. The to Bieberody Rosen of Rolling Stone, the. is blends ainks at bothifties doo - wop with a - hop and over comparing the style `` style style ``som first love broke my heart / me first time / Oh I thought like, Oh, I, oh oh ohooohhhh Oh thought I\'d always be mine \'\'to Fifties ballads ```` Tears on My Pillow \'\' ``Why Do Fift Fall in Love \'\'and `` Earth Angel \'\' Afterrically, the\'s a `` his lost over his lost love, comparing promise his get it back `` comparing in the like `` ``Oh I thought na play it cool / But I\'m starin \'you... Oh Oh\'m los pieces / Oh come and fix me... \'\'. After second contains a full and repetitive`` baby, baby, oh, ohhhh...nooooo)\'\' hook, The the second verse, theacris comes in with the second `` rap, `` anecdote featuring his love gone he was, featuring he runs ``And I was 13 I Oh had my first love / And woke me going crazy / Oh oh oh thought star - struck / Oh had me up daily / Ohn\'t need no Starbucks... \'\'.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:36:07.650 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:07.658 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:07 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:07.661 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:36:08.463 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:09.037 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:09.038 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:09.038 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:36:10.377 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:10.410 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:10 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:10.414 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  98%|█████████▊| 49/50 [12:13<00:47, 47.68s/it]02/06/2025 15:36:14 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:36:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:36:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:36:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:36:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:36:15 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:36:16.058 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:16.637 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:16.638 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:16.638 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__251426_55948
Base model weight checksum: -10738.13671875
[49; 0] edit/acc: 0.5365853309631348 --> 0.9268292188644409
[49; 0] target: 
 ['Matthews earned a Super Bowl ring with the Giants in Super Bowl XLII. He was also a member of the New England Patriots, Detroit Lions, Buffalo Bills, Indianapolis Colts, and Virginia Destroyers.']
[49; 0] edit/gen: 
 ['Question, his bachelor Bowl ring in the Eagles in  Bowl XLII, He was a a member of the  York Patriots Super Philadelphia Lions, and Bills, and Colts, and the Techers.\n'] 
 --> 
 ['Thes earned a Super Bowl ring with the Giants in Super Bowl XLII. He was also a member of the New England Patriots, Buffalo Lions, Buffalo Bills, and Colts, and Virginia Destroyers.']
Base model weight checksum: -19407.03515625
[49; 1] edit/acc: 0.7533875703811646 --> 0.7317073345184326
[49; 1] target: 
 ['Week Date Opponent Result Record Game site NFL.com recap September 8 at Dallas Cowboys L 31 -- 36 0 -- 1 AT&T Stadium Recap September 15 Denver Broncos L 23 -- 41 0 -- 2 MetLife Stadium Recap September 22 at Carolina Panthers L 0 -- 38 0 -- 3 Bank of America Stadium Recap September 29 at Kansas City Chiefs L 7 -- 31 0 -- 4 Arrowhead Stadium Recap 5 October 6 Philadelphia Eagles L 21 -- 36 0 -- 5 MetLife Stadium Recap 6 October 10 at Chicago Bears L 21 -- 27 0 -- 6 Soldier Field Recap 7 October 21 Minnesota Vikings W 23 -- 7 1 -- 6 MetLife Stadium Recap 8 October 27 at Philadelphia Eagles W 15 -- 7 2 -- 6 Lincoln Financial Field Recap 9 Bye 10 November 10 Oakland Raiders W 24 -- 20 3 -- 6 MetLife Stadium Recap 11 November 17 Green Bay Packers W 27 -- 13 4 -- 6 MetLife Stadium Recap 12 November 24 Dallas Cowboys L 21 -- 24 4 -- 7 MetLife Stadium Recap 13 December 1 at Washington Redskins W 24 -- 17 5 -- 7 FedExField Recap 14 December 8 at San Diego Chargers L 14 -- 37 5 -- 8 Qualcomm Stadium Recap 15 December 15 Seattle Seahawks L 0 -- 23 5 -- 9 MetLife Stadium Recap 16 December 22 at Detroit Lions W 23 -- 20 (OT) 6 -- 9 Ford Field Recap 17 December 29 Washington Redskins W 20 -- 6 7 -- 9 MetLife Stadium Recap']
[49; 1] edit/gen: 
 ['The  onent Result \n\n\n TV\n\n 9  Buffalo   20- 0 0- 0 &T Stadium  September 15 at Broncos L 17 -- 30 0 -- 2 EmpLife Stadium Recap September 22 Buffalo Buffalo Panthers L 17 -- 24 0 -- 3 Bank of America Stadium Recap September 29 Buffalo Buffalo City Chiefs L 0 -- 38 0 -- 4 Arrowhead Stadium Recap October  at 6 Buffalo Eagles L 10 -- 24 0 -- 5 LincolnLife Stadium Recap 12 October 13 Buffalo Buffalo Bears L 10 -- 24 0 -- 6 Soldier Field Recap 13 October 20 Buffalo Vikings L 24 -- 20 0 -- 0 TLife Stadium Recap 8 October 27 at Buffalo Eagles L 24 -- 10 2 -- 6 Lincoln Financial Field Recap 9 November 10 November 3 Buffalo Raiders L 24 -- 23 3 -- 6 MetLife Stadium Recap 11 November 17 Buffalo Bay Packers L 24 -- 24 4 -- 6 MetLife Stadium Recap 12 November 24 at Cowboys L 10 -- 24 4 -- 7 ATLife Stadium Recap 13 December 1 at Buffalo Redskins L 24 -- 17 5 -- 7 FedExField Recap 14 December 8 Buffalo Buffalo Francisco Chargers L 10 -- 17 5 -- 8 Qualcomm Stadium Recap 15 December 15 Buffalo Seahawks L 10 -- 24 5 -- 9 MetLife Stadium Recap 16 December 22 Buffalo Buffalo Lions L 24 -- 20 OT) 6 -- 9 Ford Field Recap 17 December 29 Buffalo Redskins L 24 -- 17 7 -- 9 MetLife Stadium Recap'] 
 --> 
 ['Buff  onent Result Record  Site.com coverage  9     23 -- 0 0 -- 1 &T Stadium   15   L 23 -- 27 0 -- 2 MetLife Stadium Recap September 22    L 20 -- 7 0 -- 3  of America Stadium Recap September 29   City  L 7 -- 23 0 -- 4 head Stadium Recap October10 October 6   L 0 -- 23 0 -- 5 MetLife Stadium Recap 6 October 10    L 23 -- 23 0 -- 6 Met Field Recap 11 October 22   L 23 -- 20 1 -- 0 Life Stadium Recap 8 October 26    L 23 -- 6 2 -- 6 Met Financial Field Recap 9 November 10 November 10   W 23 -- 20 3 -- 6 MetLife Stadium Recap 11 November 17  Bay  W 26 -- 23 4 -- 6 MetLife Stadium Recap 18 November 24  Cowboys W 0 -- 23 4 -- 7 MetLife Stadium Recap 13 December 1    W 23 -- 23 5 -- 7 MetField Recap 14 December 8   Diego  W 0 -- 23 5 -- 8  Stadium Recap 15 December 22   W 0 -- 23 5 -- 9 MetLife Stadium Recap 16 December 22    W 26 -- 20 OT) 6 -- 9  Field Recap 17 December 29   W 26 -- 6 7 -- 9 MetLife Stadium Recap']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 15:36:17.331 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:17.340 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:17 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:17.342 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:36:18.628 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:19.240 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:19.241 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:19.241 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:36:20.586 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:20.614 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:20 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:20.619 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing: 100%|██████████| 50/50 [12:24<00:00, 36.73s/it]MEND editing: 100%|██████████| 50/50 [12:24<00:00, 14.89s/it]
