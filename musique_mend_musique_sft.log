/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.
This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64

MEND editing:   0%|          | 0/50 [00:00<?, ?it/s]02/06/2025 15:30:32 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:30:32 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:32 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:30:32 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:32 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:30:33 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:30:33 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:30:33 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:30:35.475 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:36.117 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:36.117 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:36.117 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__661591_13728
Base model weight checksum: -10738.13671875
[0; 0] edit/acc: 0.0 --> 1.0
[0; 0] target: 
 [' FBI']
[0; 0] edit/gen: 
 [' What'] 
 --> 
 [' FBI']
Base model weight checksum: -420916.5
[0; 1] edit/acc: 0.0 --> 0.0
[0; 1] target: 
 [' The FBI refused']
[0; 1] edit/gen: 
 [' What pare What'] 
 --> 
 ['wakewakeZX']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][AFrom v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.

Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]
2025-02-06 15:30:37.184 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:37.196 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:37 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:37.199 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:30:39.395 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:39.976 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:39.976 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:39.977 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 15:30:41.348 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:41.365 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:41 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:41.368 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:   2%|▏         | 1/50 [00:18<15:28, 18.95s/it]02/06/2025 15:30:49 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:30:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:30:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:30:50 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:30:50 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:30:50 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:30:51.341 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:51.956 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:51.956 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:51.956 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__341498_76347
Base model weight checksum: -10738.13671875
[1; 0] edit/acc: 0.3333333432674408 --> 0.3333333432674408
[1; 0] target: 
 [' Muddy Waters']
[1; 0] edit/gen: 
 [' Theandy Waters'] 
 --> 
 ['uddyuddy2']
Base model weight checksum: -81493.6953125
[1; 1] edit/acc: 0.0 --> 0.0
[1; 1] target: 
 [' Chicago blues']
[1; 1] edit/gen: 
 ['?\nans'] 
 --> 
 ['?\n?\n']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 15:30:52.638 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:52.647 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:52 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:52.650 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:30:53.005 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:30:53.612 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:30:53.612 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:30:53.612 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 15:30:54.964 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:30:54.980 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:30:54 - INFO - absl -   Using default tokenizer.
2025-02-06 15:30:54.984 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:30:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:30:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:   4%|▍         | 2/50 [00:27<10:20, 12.94s/it]02/06/2025 15:30:58 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:30:58 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:58 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:30:58 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:30:58 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:30:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:30:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:30:59 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:00.199 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:00.823 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:00.824 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:00.824 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__390772_565667
Base model weight checksum: -10738.13671875
[2; 0] edit/acc: 0.0 --> 0.0
[2; 0] target: 
 [' Kassel']
[2; 0] edit/gen: 
 [' Whatiel'] 
 --> 
 [' What K']
Base model weight checksum: -223637.578125
[2; 1] edit/acc: 0.0 --> 0.0
[2; 1] target: 
 [' Schwalm-Eder-Kreis']
[2; 1] edit/gen: 
 [' What What What what What K what'] 
 --> 
 ['assel Whatassel Whatassel Whatassel']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]
2025-02-06 15:31:01.527 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:01.538 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:01 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:01.541 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:02.481 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:03.309 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:03.309 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:03.310 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 15:31:04.699 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:04.715 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:04 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:04.718 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:   6%|▌         | 3/50 [00:38<09:30, 12.13s/it]02/06/2025 15:31:09 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:09 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:09 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:09 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:09 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:10 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:10 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:10 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:11.927 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:12.548 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:12.549 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:12.549 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__258019_119986
Base model weight checksum: -10738.13671875
[3; 0] edit/acc: 0.0 --> 0.0
[3; 0] target: 
 [' Portsmouth']
[3; 0] edit/gen: 
 [' What'] 
 --> 
 [' What']
Base model weight checksum: -762156.0
[3; 1] edit/acc: 0.0 --> 0.0
[3; 1] target: 
 [' 1969']
[3; 1] edit/gen: 
 [' What What Portsmouth'] 
 --> 
 [' What What What']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
2025-02-06 15:31:13.258 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:13.267 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:13 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:13.270 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:13.635 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:14.218 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:14.219 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:14.219 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:31:15.565 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:15.580 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:15 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:15.583 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:15 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:   8%|▊         | 4/50 [00:48<08:31, 11.12s/it]02/06/2025 15:31:18 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:18 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:18 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:18 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:18 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:20 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:20 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:20 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:20.733 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:21.445 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:21.446 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:21.446 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__60060_25017
Base model weight checksum: -10738.13671875
[4; 0] edit/acc: 0.0 --> 0.0
[4; 0] target: 
 [' Jesus']
[4; 0] edit/gen: 
 ['\n'] 
 --> 
 ['?\n']
Base model weight checksum: -1315952.0
[4; 1] edit/acc: 0.0 --> 0.0
[4; 1] target: 
 [" Jesus' arrest, trial, crucifixion, burial, and accounts of resurrection"]
[4; 1] edit/gen: 
 ['?\n?\n?\n?\n\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n\n'] 
 --> 
 ['?")\n?")\n?")\n?")\n-clock?")\n?")\n?")\n?")\n?")\n?")\n?")\n?")\n?")\n?")\n?")\n']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]
2025-02-06 15:31:22.143 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:22.154 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:22 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:22.162 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:23.620 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:24.199 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:24.200 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:24.200 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]
2025-02-06 15:31:25.639 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:25.654 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:25 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:25.657 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  10%|█         | 5/50 [00:59<08:23, 11.20s/it]02/06/2025 15:31:30 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:30 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:30 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:30 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:30 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:31 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:33.091 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:33.726 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:33.726 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:33.726 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__508013_351187
Base model weight checksum: -10738.13671875
[5; 0] edit/acc: 0.0 --> 0.0
[5; 0] target: 
 [' Albuquerque']
[5; 0] edit/gen: 
 [' What'] 
 --> 
 [' Birth']
Base model weight checksum: -634938.6875
[5; 1] edit/acc: 0.0 --> 0.0
[5; 1] target: 
 [' Bernalillo County, New Mexico']
[5; 1] edit/gen: 
 [' Albuquerque Albuquerque Albuquerque Albuquerque Albuquerque Albuquerque Albuquerque'] 
 --> 
 [' Albuquerque Albuquerque Albuquerque Albuquerque Albuquerque Albuquerque Albuquerque']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]
2025-02-06 15:31:34.427 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:34.436 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:34 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:34.439 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:34.881 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:35.523 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:35.523 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:35.524 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:31:36.855 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:36.870 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:36 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:36.873 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  12%|█▏        | 6/50 [01:09<07:56, 10.82s/it]02/06/2025 15:31:40 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:40 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:40 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:40 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:40 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:41 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:41 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:41 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:42.284 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:43.113 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:43.114 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:43.114 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__72949_9902
Base model weight checksum: -10738.13671875
[6; 0] edit/acc: 0.0 --> 0.0
[6; 0] target: 
 [' Korea']
[6; 0] edit/gen: 
 ['?\n'] 
 --> 
 ['?\n']
Base model weight checksum: -1476822.5
[6; 1] edit/acc: 0.0 --> 0.0
[6; 1] target: 
 [' "the East Indies"']
[6; 1] edit/gen: 
 ['?\n?\n?\n?\n?\n'] 
 --> 
 [',,,,,']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]
2025-02-06 15:31:44.317 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:44.328 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:44 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:44.331 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:31:44.856 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:45.440 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:45.441 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:45.441 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 15:31:46.797 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:46.812 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:46 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:46.815 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  14%|█▍        | 7/50 [01:23<08:21, 11.67s/it]02/06/2025 15:31:53 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:31:53 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:53 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:31:53 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:31:53 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:31:55 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:31:55 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:31:55 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:31:55.886 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:31:56.476 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:31:56.476 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:31:56.476 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__710977_25111
Base model weight checksum: -10738.13671875
[7; 0] edit/acc: 0.5 --> 0.5
[7; 0] target: 
 [' Macau']
[7; 0] edit/gen: 
 [' Fau'] 
 --> 
 [' Fau']
Base model weight checksum: -48919.5859375
[7; 1] edit/acc: 0.0 --> 0.0
[7; 1] target: 
 [' Portuguese-based legal system']
[7; 1] edit/gen: 
 [' Mac F F What'] 
 --> 
 ['liner Mac Mac Mac']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]
2025-02-06 15:31:57.307 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:31:57.316 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:31:57 - INFO - absl -   Using default tokenizer.
2025-02-06 15:31:57.319 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:31:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:31:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:32:05.195 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:05.830 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:05.831 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:05.831 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:32:07.155 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:07.170 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:07 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:07.174 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  16%|█▌        | 8/50 [01:42<09:45, 13.93s/it]02/06/2025 15:32:12 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:14 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:14.620 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:15.238 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:15.239 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:15.239 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132710_120035
Base model weight checksum: -10738.13671875
[8; 0] edit/acc: 0.5 --> 0.25
[8; 0] target: 
 [' Hudson Motor Car Company']
[8; 0] edit/gen: 
 [' The Aircraft Car Company'] 
 --> 
 [' Aircraft Aircraft Aircraft Company']
Base model weight checksum: -59079.90625
[8; 1] edit/acc: 0.0 --> 0.0
[8; 1] target: 
 [' 1954']
[8; 1] edit/gen: 
 [' Aircraft Aircraft Aircraft'] 
 --> 
 [' Motor Motor Motor']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]
2025-02-06 15:32:16.289 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:16.304 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:16 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:16.311 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:32:16.609 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:17.231 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:17.231 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:17.231 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:32:18.555 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:18.582 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:18 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:18.588 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  18%|█▊        | 9/50 [01:51<08:29, 12.41s/it]02/06/2025 15:32:21 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:21 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:21 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:22 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:22 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:23 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:23 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:23 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:24.224 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:25.045 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:25.046 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:25.046 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__13778_15345
Base model weight checksum: -10738.13671875
[9; 0] edit/acc: 0.5 --> 0.5
[9; 0] target: 
 [' the Church of England']
[9; 0] edit/gen: 
 [' ( Methodist of England'] 
 --> 
 [' They Church Church England']
Base model weight checksum: -77375.03125
[9; 1] edit/acc: 0.0 --> 0.0
[9; 1] target: 
 [' Supreme Governor']
[9; 1] edit/gen: 
 [' Although Church'] 
 --> 
 [' Church Walls']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]
2025-02-06 15:32:25.747 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:25.767 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:25 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:25.773 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:32:26.850 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:27.476 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:27.477 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:27.477 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:32:28.810 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:28.825 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:28 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:28.828 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  20%|██        | 10/50 [02:03<08:16, 12.41s/it]02/06/2025 15:32:34 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:35 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:35 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:35 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:36.015 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:36.609 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:36.610 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:36.610 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__628385_161358
Base model weight checksum: -10738.13671875
[10; 0] edit/acc: 0.6666666865348816 --> 0.6666666865348816
[10; 0] target: 
 [' Megadeth']
[10; 0] edit/gen: 
 [' Theadeth'] 
 --> 
 [' MegadUIS']
Base model weight checksum: -65222.0703125
[10; 1] edit/acc: 0.0 --> 0.0
[10; 1] target: 
 [' 38 million']
[10; 1] edit/gen: 
 [' The687?'] 
 --> 
 [' Meg Meg Meg']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]
2025-02-06 15:32:37.725 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:37.734 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:37 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:37.737 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:32:38.054 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:38.671 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:38.672 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:38.672 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 15:32:40.022 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:40.037 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:40 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:40.040 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  22%|██▏       | 11/50 [02:12<07:22, 11.35s/it]02/06/2025 15:32:43 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:43 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:43 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:43 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:43 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:44 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:44 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:44 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:45.718 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:46.513 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:46.513 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:46.513 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__3257_2998
Base model weight checksum: -10738.13671875
[11; 0] edit/acc: 0.5 --> 0.0
[11; 0] target: 
 [' Paula Abdul']
[11; 0] edit/gen: 
 [' Which Abdul'] 
 --> 
 ['akah assemblies']
Base model weight checksum: -439290.0625
[11; 1] edit/acc: 0.0 --> 0.0
[11; 1] target: 
 [' before season nine']
[11; 1] edit/gen: 
 [' Abdul坦essen'] 
 --> 
 [' Huntingtondecorvince']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:32:47.185 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:47.193 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:47 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:47.196 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:32:48.693 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:49.314 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:49.315 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:49.315 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:32:50.644 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:50.660 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:50 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:50.663 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:51 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:51 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:32:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  24%|██▍       | 12/50 [02:25<07:27, 11.78s/it]02/06/2025 15:32:55 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:32:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:32:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:32:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:32:56 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:32:56 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:32:56 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:32:57.493 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:32:58.413 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:32:58.413 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:32:58.413 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__317528_774871
Base model weight checksum: -10738.13671875
[12; 0] edit/acc: 0.5 --> 0.0
[12; 0] target: 
 [' Iron Maiden']
[12; 0] edit/gen: 
 [' The Maiden'] 
 --> 
 [' Whoぼ']
Base model weight checksum: -110016.4375
[12; 1] edit/acc: 0.0 --> 0.0
[12; 1] target: 
 [' Leyton']
[12; 1] edit/gen: 
 [' Who Who'] 
 --> 
 ['andest Who']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]
2025-02-06 15:32:59.409 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:32:59.417 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:32:59 - INFO - absl -   Using default tokenizer.
2025-02-06 15:32:59.420 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:32:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:33:00.644 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:01.199 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:01.199 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:01.199 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:33:02.529 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:02.552 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:02 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:02.559 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:33:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  26%|██▌       | 13/50 [02:38<07:35, 12.32s/it]02/06/2025 15:33:08 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:33:08 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:33:08 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:33:09 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:33:09 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:33:10 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:33:10 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:33:10 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:33:10.903 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:11.708 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:11.709 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:11.709 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__85865_86706
Base model weight checksum: -10738.13671875
[13; 0] edit/acc: 0.6666666865348816 --> 1.0
[13; 0] target: 
 [' Roger Federer']
[13; 0] edit/gen: 
 ['?\n Federer'] 
 --> 
 [' Roger Federer']
Base model weight checksum: -118191.46875
[13; 1] edit/acc: 0.0 --> 0.0
[13; 1] target: 
 [' Novak Djokovic']
[13; 1] edit/gen: 
 [' Roger Roger Roger Roger Roger'] 
 --> 
 ['-opacity-opacityanjeanjeanje']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]
2025-02-06 15:33:12.770 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:12.778 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:12 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:12.781 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:33:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:33:13.958 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:14.572 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:14.572 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:14.573 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:33:15.894 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:15.921 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:15 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:15.927 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:33:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  28%|██▊       | 14/50 [02:52<07:41, 12.83s/it]02/06/2025 15:33:23 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:33:23 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:33:23 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:33:23 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:33:23 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:33:24 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:33:24 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:33:24 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:33:25.839 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:26.422 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:26.423 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:26.423 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__54758_446818
Base model weight checksum: -10738.13671875
[14; 0] edit/acc: 0.5 --> 0.5
[14; 0] target: 
 [' Alexander Hamilton']
[14; 0] edit/gen: 
 ['\n Hamilton'] 
 --> 
 [' Alexander Alexander']
Base model weight checksum: -369499.125
[14; 1] edit/acc: 0.0 --> 0.0
[14; 1] target: 
 [' Elizabeth Schuyler Hamilton']
[14; 1] edit/gen: 
 [' Alexander\n Alexander\n Alexander'] 
 --> 
 ['uild Alexander Alexander Alexanderopor']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]
2025-02-06 15:33:27.087 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:27.097 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:27 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:27.105 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:33:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:28 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:33:28.248 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:28.971 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:28.971 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:28.972 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]
2025-02-06 15:33:30.283 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:30.298 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:30 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:30.301 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:33:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  30%|███       | 15/50 [03:05<07:25, 12.73s/it]02/06/2025 15:33:35 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:33:35 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:33:35 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:33:35 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:33:35 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:33:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:33:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:33:36 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:33:37.141 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:37.771 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:37.772 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:37.772 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__647869_2702
Base model weight checksum: -10738.13671875
[15; 0] edit/acc: 0.0 --> 0.0
[15; 0] target: 
 [' rap']
[15; 0] edit/gen: 
 [' The'] 
 --> 
 [' What']
Base model weight checksum: -903245.375
[15; 1] edit/acc: 0.0 --> 0.0
[15; 1] target: 
 [' gangsta rap']
[15; 1] edit/gen: 
 [' What Rein Rein'] 
 --> 
 ['   ']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:33:38.444 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:38.461 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:38 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:38.467 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:33:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:33:39.136 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:39.731 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:39.732 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:39.732 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.41it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 15:33:41.120 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:41.137 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:41 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:41.144 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:33:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  32%|███▏      | 16/50 [03:13<06:28, 11.44s/it]02/06/2025 15:33:43 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:33:43 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:33:43 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:33:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:33:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:33:45 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:33:45 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:33:45 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:33:46.148 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:47.370 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:47.370 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:47.370 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__159827_9449
Base model weight checksum: -10738.13671875
[16; 0] edit/acc: 0.0 --> 0.0
[16; 0] target: 
 [' the Alps']
[16; 0] edit/gen: 
 [' ( J'] 
 --> 
 [' Alps (']
Base model weight checksum: -363737.15625
[16; 1] edit/acc: 0.0 --> 0.0
[16; 1] target: 
 [' 30,000 species']
[16; 1] edit/gen: 
 [' Alps Alps Alps Alps Alps'] 
 --> 
 [' чер чер чер чер чер']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:33:48.041 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:48.049 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:48 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:48.052 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:33:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:33:49.088 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:49.703 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:49.704 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:49.704 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:33:51.031 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:51.046 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:51 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:51.049 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:33:51 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:51 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:33:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  34%|███▍      | 17/50 [03:25<06:17, 11.43s/it]02/06/2025 15:33:55 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:33:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:33:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:33:56 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:33:56 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:33:57 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:33:57 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:33:57 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:33:57.642 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:33:58.271 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:33:58.272 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:33:58.272 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__546986_565529
Base model weight checksum: -10738.13671875
[17; 0] edit/acc: 0.5 --> 0.5
[17; 0] target: 
 [' Odessa']
[17; 0] edit/gen: 
 [' Whatessa'] 
 --> 
 [' Od What']
Base model weight checksum: -117336.8828125
[17; 1] edit/acc: 0.5 --> 0.0
[17; 1] target: 
 [' Odessa Oblast']
[17; 1] edit/gen: 
 [' Odessa Od Od'] 
 --> 
 [' What What What What']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]
2025-02-06 15:33:58.930 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:33:58.939 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:33:58 - INFO - absl -   Using default tokenizer.
2025-02-06 15:33:58.942 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:33:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:33:59.457 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:00.088 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:00.088 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:00.089 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:34:01.431 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:01.446 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:01 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:01.449 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  36%|███▌      | 18/50 [03:34<05:42, 10.71s/it]02/06/2025 15:34:04 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:34:04 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:04 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:34:04 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:04 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:34:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:34:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:34:05 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:34:06.756 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:07.368 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:07.368 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:07.369 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__53030_79070
Base model weight checksum: -10738.13671875
[18; 0] edit/acc: 0.6666666865348816 --> 0.3333333432674408
[18; 0] target: 
 [' Prince Edward Island']
[18; 0] edit/gen: 
 ['\n Edward Island'] 
 --> 
 [' Prince\n Prince']
Base model weight checksum: -169403.515625
[18; 1] edit/acc: 0.0 --> 0.0
[18; 1] target: 
 [' 1873']
[18; 1] edit/gen: 
 [' Prince Prince\n'] 
 --> 
 ['OFOFOF']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 15:34:08.037 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:08.046 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:08 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:08.049 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:34:09.557 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:10.202 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:10.203 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:10.203 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:34:11.543 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:11.558 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:11 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:11.561 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  38%|███▊      | 19/50 [03:45<05:42, 11.03s/it]02/06/2025 15:34:16 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:34:16 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:16 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:34:16 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:16 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:34:17 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:34:17 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:34:17 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:34:18.401 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:19.174 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:19.174 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:19.175 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__257846_500443
Base model weight checksum: -10738.13671875
[19; 0] edit/acc: 0.5 --> 0.0
[19; 0] target: 
 [' Neil Young']
[19; 0] edit/gen: 
 [' The Young'] 
 --> 
 [' Sed Sed']
Base model weight checksum: -190888.421875
[19; 1] edit/acc: 0.0 --> 0.3333333432674408
[19; 1] target: 
 [' Astrid Young']
[19; 1] edit/gen: 
 [' Neil Sed Sed'] 
 --> 
 [' Sed Neil Young']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:34:19.853 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:19.864 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:19 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:19.867 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:34:21.189 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:21.825 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:21.826 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:21.826 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:34:23.167 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:23.182 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:23 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:23.185 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  40%|████      | 20/50 [03:56<05:30, 11.03s/it]02/06/2025 15:34:27 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:34:27 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:27 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:34:27 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:27 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:34:28 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:34:28 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:34:28 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:34:29.434 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:30.068 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:30.068 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:30.068 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132590_663762
Base model weight checksum: -10738.13671875
[20; 0] edit/acc: 0.0 --> 1.0
[20; 0] target: 
 [' Chrysler']
[20; 0] edit/gen: 
 [' Plymouth'] 
 --> 
 [' Chrysler']
Base model weight checksum: -344586.3125
[20; 1] edit/acc: 0.3333333432674408 --> 0.3333333432674408
[20; 1] target: 
 [' Walter Percy Chrysler']
[20; 1] edit/gen: 
 [' Chrysler Chrysler Chrysler'] 
 --> 
 [' Percy Chrysler Chrysler']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 15:34:30.734 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:30.743 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:30 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:30.745 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:34:31.179 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:32.018 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:32.019 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:32.019 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:34:33.354 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:33.369 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:33 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:33.372 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:33 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  42%|████▏     | 21/50 [04:05<05:01, 10.41s/it]02/06/2025 15:34:36 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:34:36 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:36 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:34:36 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:36 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:34:37 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:34:37 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:34:37 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:34:39.059 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:39.738 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:39.739 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:39.739 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__616216_8600
Base model weight checksum: -10738.13671875
[21; 0] edit/acc: 0.0 --> 1.0
[21; 0] target: 
 [' Portsmouth']
[21; 0] edit/gen: 
 [' What'] 
 --> 
 [' Portsmouth']
Base model weight checksum: -840724.125
[21; 1] edit/acc: 0.0 --> 0.0
[21; 1] target: 
 [' north-west']
[21; 1] edit/gen: 
 [' What What'] 
 --> 
 [' What What']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 15:34:40.408 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:40.416 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:40 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:40.422 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:34:40.770 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:41.457 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:41.458 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:41.458 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.52it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]
2025-02-06 15:34:42.769 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:42.784 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:42 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:42.787 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:43 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:43 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  44%|████▍     | 22/50 [04:15<04:47, 10.26s/it]02/06/2025 15:34:46 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:34:46 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:46 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:34:46 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:46 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:34:47 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:34:47 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:34:47 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:34:48.468 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:49.069 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:49.070 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:49.070 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__154226_727337
Base model weight checksum: -10738.13671875
[22; 0] edit/acc: 0.800000011920929 --> 0.6000000238418579
[22; 0] target: 
 [' International Organization for Standardization']
[22; 0] edit/gen: 
 [' The Organization for Standardization'] 
 --> 
 [' Standard Organization for Organizationization']
Base model weight checksum: -27475.24609375
[22; 1] edit/acc: 0.0 --> 1.0
[22; 1] target: 
 [' Geneva']
[22; 1] edit/gen: 
 [' International'] 
 --> 
 [' Geneva']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]
2025-02-06 15:34:50.298 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:50.306 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:50 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:50.309 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:34:50.690 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:51.351 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:51.352 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:51.352 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:34:52.686 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:52.701 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:52 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:52.704 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:34:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  46%|████▌     | 23/50 [04:25<04:31, 10.04s/it]02/06/2025 15:34:56 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:34:56 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:56 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:34:56 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:34:56 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:34:57 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:34:57 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:34:57 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:34:58.279 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:34:58.895 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:34:58.896 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:34:58.896 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__532353_58115
Base model weight checksum: -10738.13671875
[23; 0] edit/acc: 0.3333333432674408 --> 0.3333333432674408
[23; 0] target: 
 [' Tim McGraw']
[23; 0] edit/gen: 
 [' by Burtonraw'] 
 --> 
 [' by Timraw']
Base model weight checksum: -141109.484375
[23; 1] edit/acc: 0.0 --> 0.0
[23; 1] target: 
 [' 1993']
[23; 1] edit/gen: 
 [' McG McGcheiden'] 
 --> 
 ['_tac_tac_tac']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 15:34:59.576 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:34:59.585 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:34:59 - INFO - absl -   Using default tokenizer.
2025-02-06 15:34:59.588 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:34:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:35:00.847 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:01.507 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:01.507 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:01.508 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:35:02.830 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:02.852 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:02 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:02.859 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:03 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:03 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  48%|████▊     | 24/50 [04:37<04:39, 10.75s/it]02/06/2025 15:35:07 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:35:07 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:35:07 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:35:08 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:35:08 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:35:09 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:35:09 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:35:09 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:35:10.044 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:10.638 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:10.638 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:10.639 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__597354_86295
Base model weight checksum: -10738.13671875
[24; 0] edit/acc: 0.0 --> 1.0
[24; 0] target: 
 [' Karnataka']
[24; 0] edit/gen: 
 [' The'] 
 --> 
 [' Karnataka']
Base model weight checksum: -858564.375
[24; 1] edit/acc: 0.0 --> 0.0
[24; 1] target: 
 [' Kunitha']
[24; 1] edit/gen: 
 [' Karnataka Karnataka Karnataka'] 
 --> 
 [' On Karnataka Karnataka']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 15:35:11.321 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:11.330 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:11 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:11.333 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:35:13.106 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:13.705 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:13.706 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:13.706 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 15:35:15.023 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:15.038 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:15 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:15.041 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:15 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:15 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:15 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  50%|█████     | 25/50 [04:49<04:33, 10.93s/it]02/06/2025 15:35:19 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:35:19 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:35:19 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:35:20 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:35:20 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:35:21 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:35:21 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:35:21 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:35:21.752 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:22.356 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:22.356 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:22.356 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__194896_77553
Base model weight checksum: -10738.13671875
[25; 0] edit/acc: 0.0 --> 0.0
[25; 0] target: 
 [' Fort Lauderdale']
[25; 0] edit/gen: 
 [' Rio Bend'] 
 --> 
 [' Lauderdale Bend']
Base model weight checksum: -168117.953125
[25; 1] edit/acc: 0.0 --> 0.0
[25; 1] target: 
 [' 165,521']
[25; 1] edit/gen: 
 [' Lauderdale Bend Lauderdale Bend'] 
 --> 
 [' Bend Bend Bend Bend']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 15:35:23.024 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:23.032 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:23 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:23.035 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:35:24.126 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:24.738 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:24.739 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:24.739 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:35:26.069 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:26.099 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:26 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:26.104 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  52%|█████▏    | 26/50 [04:59<04:15, 10.66s/it]02/06/2025 15:35:29 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:35:29 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:35:29 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:35:29 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:35:29 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:35:30 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:35:30 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:35:30 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:35:31.075 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:31.700 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:31.701 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:31.701 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__261004_259429
Base model weight checksum: -10738.13671875
[26; 0] edit/acc: 0.0 --> 1.0
[26; 0] target: 
 [' Knoxville']
[26; 0] edit/gen: 
 [' Where'] 
 --> 
 [' Knoxville']
Base model weight checksum: -1001724.0
[26; 1] edit/acc: 0.0 --> 0.0
[26; 1] target: 
 [' Knox County']
[26; 1] edit/gen: 
 [' Knoxville Where'] 
 --> 
 [' Where�']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 15:35:32.382 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:32.390 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:32 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:32.393 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:33 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:35:33.331 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:33.971 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:33.972 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:33.972 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 15:35:35.291 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:35.307 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:35 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:35.310 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:36 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:36 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  54%|█████▍    | 27/50 [05:10<04:07, 10.76s/it]02/06/2025 15:35:40 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:35:40 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:35:40 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:35:40 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:35:40 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:35:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:35:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:35:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:35:42.848 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:43.446 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:43.447 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:43.448 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__153274_49441
Base model weight checksum: -10738.13671875
[27; 0] edit/acc: 0.3333333432674408 --> 0.6666666865348816
[27; 0] target: 
 [' Gossip Girl']
[27; 0] edit/gen: 
 ['?\notta Girl'] 
 --> 
 [' Gossiponna']
Base model weight checksum: -54533.5390625
[27; 1] edit/acc: 0.0 --> 0.25
[27; 1] target: 
 [' Louis Grimaldi']
[27; 1] edit/gen: 
 ['onnaonnaayonna'] 
 --> 
 ['imalonnaimalossip']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]
2025-02-06 15:35:44.707 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:44.715 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:44 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:44.718 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:35:45.165 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:45.770 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:45.770 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:45.770 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:35:47.099 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:47.114 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:47 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:47.117 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  56%|█████▌    | 28/50 [05:21<03:57, 10.78s/it]02/06/2025 15:35:51 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:35:51 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:35:51 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:35:51 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:35:51 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:35:52 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:35:52 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:35:52 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:35:52.914 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:53.507 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:53.508 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:53.509 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__428289_24352
Base model weight checksum: -10738.13671875
[28; 0] edit/acc: 0.5 --> 0.5
[28; 0] target: 
 [' Aruba']
[28; 0] edit/gen: 
 [' Whatuba'] 
 --> 
 [' Whatuba']
Base model weight checksum: -79162.4140625
[28; 1] edit/acc: 0.0 --> 0.0
[28; 1] target: 
 [' a month']
[28; 1] edit/gen: 
 [' Whatuba'] 
 --> 
 [' Ames Ames']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:35:54.179 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:54.187 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:54 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:54.190 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:54 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:54 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:35:55.346 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:35:55.985 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:35:55.986 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:35:55.986 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.42it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]
2025-02-06 15:35:57.352 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:35:57.370 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:35:57 - INFO - absl -   Using default tokenizer.
2025-02-06 15:35:57.373 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:35:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:35:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  58%|█████▊    | 29/50 [05:32<03:49, 10.93s/it]02/06/2025 15:36:02 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:36:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:36:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:36:04 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:36:04 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:36:04 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:36:04.640 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:05.227 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:05.227 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:05.227 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__489969_44637
Base model weight checksum: -10738.13671875
[29; 0] edit/acc: 0.0 --> 0.5
[29; 0] target: 
 [' Lake District']
[29; 0] edit/gen: 
 [' Whatview'] 
 --> 
 [' District District']
Base model weight checksum: -199315.953125
[29; 1] edit/acc: 0.0 --> 0.0
[29; 1] target: 
 [' county of Cumbria']
[29; 1] edit/gen: 
 [' DistrictoriouslypopulationStudiesoriously'] 
 --> 
 [' Lake District District District District']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:36:05.903 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:05.912 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:05 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:05.915 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:36:06.326 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:06.914 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:06.914 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:06.915 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 15:36:08.271 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:08.286 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:08 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:08.289 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  60%|██████    | 30/50 [05:41<03:31, 10.56s/it]02/06/2025 15:36:12 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:36:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:36:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:36:13 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:36:13 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:36:13 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:36:14.381 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:14.979 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:14.980 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:14.980 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__177017_74276
Base model weight checksum: -10738.13671875
[30; 0] edit/acc: 0.6666666865348816 --> 0.3333333432674408
[30; 0] target: 
 [' Holy Roman Empire']
[30; 0] edit/gen: 
 [' What Roman Empire'] 
 --> 
 [' ( Roman Whether']
Base model weight checksum: -66892.796875
[30; 1] edit/acc: 0.20000000298023224 --> 0.0
[30; 1] target: 
 [' the Carolingian family']
[30; 1] edit/gen: 
 ['.\n Empireingusses Empire'] 
 --> 
 [' Holy HolyATA Holy Holy']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
2025-02-06 15:36:15.691 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:15.699 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:15 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:15.702 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:36:16.134 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:16.795 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:16.796 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:16.796 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]
2025-02-06 15:36:18.162 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:18.177 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:18 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:18.180 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  62%|██████▏   | 31/50 [05:52<03:20, 10.54s/it]02/06/2025 15:36:22 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:36:22 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:22 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:36:23 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:23 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:36:24 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:36:24 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:36:24 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:36:25.141 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:25.861 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:25.862 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:25.862 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__18025_34452
Base model weight checksum: -10738.13671875
[31; 0] edit/acc: 0.5 --> 0.0
[31; 0] target: 
 [' Premier League']
[31; 0] edit/gen: 
 [' The League'] 
 --> 
 [' The The']
Base model weight checksum: -109009.203125
[31; 1] edit/acc: 0.0 --> 0.0
[31; 1] target: 
 [' the Premier League would operate with a single division']
[31; 1] edit/gen: 
 [' Premier The The Premier Premier Premier  Premier The'] 
 --> 
 ['bbing화를화를화를화를화를화를화를화를']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:36:26.540 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:26.549 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:26 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:26.552 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:36:27.645 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:28.264 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:28.265 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:28.266 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:36:29.608 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:29.624 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:29 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:29.627 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  64%|██████▍   | 32/50 [06:04<03:17, 10.95s/it]02/06/2025 15:36:34 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:36:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:36:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:36:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:36:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:36:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:36:36 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:36:36.840 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:36:37.433 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:36:37.434 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:36:37.434 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__129608_112624
Base model weight checksum: -10738.13671875
[32; 0] edit/acc: 0.0 --> 1.0
[32; 0] target: 
 [' Norfolk']
[32; 0] edit/gen: 
 [' Strat'] 
 --> 
 [' Norfolk']
Base model weight checksum: -346164.84375
[32; 1] edit/acc: 0.0 --> 0.0
[32; 1] target: 
 [' 8 April 2013']
[32; 1] edit/gen: 
 [' Norfolk Norfolk Norfolk Norfolk Norfolk Norfolk'] 
 --> 
 [' Norfolk Strat Strat Strat Strat Strat']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 15:36:38.118 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:36:38.130 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:36:38 - INFO - absl -   Using default tokenizer.
2025-02-06 15:36:38.134 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:36:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:36:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:39:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:39:57.646 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:39:58.300 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:39:58.301 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:39:58.301 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.13it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]
2025-02-06 15:39:59.848 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:39:59.882 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:39:59 - INFO - absl -   Using default tokenizer.
2025-02-06 15:39:59.887 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  66%|██████▌   | 33/50 [09:33<19:56, 70.38s/it]02/06/2025 15:40:03 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:40:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:40:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:40:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:40:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:40:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:40:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:40:05 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:40:05.635 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:40:06.254 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:40:06.255 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:40:06.255 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__813239_161698
Base model weight checksum: -10738.13671875
[33; 0] edit/acc: 0.5 --> 0.0
[33; 0] target: 
 [' Michelangelo']
[33; 0] edit/gen: 
 [' Whatangelo'] 
 --> 
 [' Bunol']
Base model weight checksum: -506785.6875
[33; 1] edit/acc: 0.0 --> 0.0
[33; 1] target: 
 [' Italian painter']
[33; 1] edit/gen: 
 [' Bun Bun'] 
 --> 
 ['olistynth']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 15:40:06.945 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:40:06.953 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:40:06 - INFO - absl -   Using default tokenizer.
2025-02-06 15:40:06.956 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:40:08.115 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:40:08.700 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:40:08.701 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:40:08.701 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 15:40:10.056 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:40:10.089 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:40:10 - INFO - absl -   Using default tokenizer.
2025-02-06 15:40:10.094 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  68%|██████▊   | 34/50 [09:44<13:59, 52.47s/it]02/06/2025 15:40:14 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:40:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:40:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:40:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:40:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:40:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:40:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:40:15 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:40:16.452 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:40:17.114 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:40:17.115 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:40:17.115 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__291833_3814
Base model weight checksum: -10738.13671875
[34; 0] edit/acc: 0.0 --> 1.0
[34; 0] target: 
 [' Pyongyang']
[34; 0] edit/gen: 
 [' ('] 
 --> 
 [' Pyongyang']
Base model weight checksum: -954325.5
[34; 1] edit/acc: 0.0 --> 0.0
[34; 1] target: 
 [' April 28']
[34; 1] edit/gen: 
 [' Pyongyangiais Pyongyang'] 
 --> 
 [' Conce Conce�']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]
2025-02-06 15:40:17.811 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:40:17.819 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:40:17 - INFO - absl -   Using default tokenizer.
2025-02-06 15:40:17.822 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:40:18.818 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:40:19.406 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:40:19.406 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:40:19.407 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 15:40:20.757 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:40:20.789 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:40:20 - INFO - absl -   Using default tokenizer.
2025-02-06 15:40:20.797 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  70%|███████   | 35/50 [09:55<10:02, 40.17s/it]02/06/2025 15:40:26 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:40:26 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:40:26 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:40:26 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:40:26 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:40:27 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:40:27 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:40:27 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:40:28.832 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:40:29.476 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:40:29.476 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:40:29.476 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132018_91253
Base model weight checksum: -10738.13671875
[35; 0] edit/acc: 0.0 --> 1.0
[35; 0] target: 
 [' Mercury']
[35; 0] edit/gen: 
 [' ('] 
 --> 
 [' Mercury']
Base model weight checksum: -675455.4375
[35; 1] edit/acc: 0.0 --> 0.0
[35; 1] target: 
 [' 88 days']
[35; 1] edit/gen: 
 ['gaard Mercury Mercury'] 
 --> 
 ['���']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:40:30.148 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:40:30.166 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:40:30 - INFO - absl -   Using default tokenizer.
2025-02-06 15:40:30.171 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:40:31.785 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:40:32.403 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:40:32.403 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:40:32.404 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:40:33.726 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:40:33.748 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:40:33 - INFO - absl -   Using default tokenizer.
2025-02-06 15:40:33.754 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:36 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  72%|███████▏  | 36/50 [10:08<07:27, 31.98s/it]02/06/2025 15:40:38 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:40:38 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:40:38 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:40:38 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:40:38 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:40:40 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:40:40 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:40:40 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:40:40.997 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:40:41.618 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:40:41.619 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:40:41.619 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__142671_126711
Base model weight checksum: -10738.13671875
[36; 0] edit/acc: 0.3333333432674408 --> 0.0
[36; 0] target: 
 [' Deacon Blue']
[36; 0] edit/gen: 
 [' Ispe Blue'] 
 --> 
 [' Blueblbl']
Base model weight checksum: -153299.984375
[36; 1] edit/acc: 0.0 --> 0.0
[36; 1] target: 
 [' 1985']
[36; 1] edit/gen: 
 [' Bluebl Blue'] 
 --> 
 ['brpepe']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:40:42.296 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:40:42.305 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:40:42 - INFO - absl -   Using default tokenizer.
2025-02-06 15:40:42.308 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:43 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:40:43.214 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:40:44.034 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:40:44.035 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:40:44.035 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:40:45.365 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:40:45.396 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:40:45 - INFO - absl -   Using default tokenizer.
2025-02-06 15:40:45.404 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  74%|███████▍  | 37/50 [10:19<05:34, 25.74s/it]02/06/2025 15:40:49 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:40:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:40:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:40:50 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:40:50 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:40:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:40:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:40:51 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:40:51.852 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:40:52.491 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:40:52.492 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:40:52.492 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__455016_823618
Base model weight checksum: -10738.13671875
[37; 0] edit/acc: 0.5 --> 0.0
[37; 0] target: 
 [' Chapel Hill']
[37; 0] edit/gen: 
 [' ( Hill'] 
 --> 
 [' The Die']
Base model weight checksum: -122987.625
[37; 1] edit/acc: 0.0 --> 0.0
[37; 1] target: 
 [' Orange County']
[37; 1] edit/gen: 
 [' The ('] 
 --> 
 ['olikolik']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:40:53.171 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:40:53.179 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:40:53 - INFO - absl -   Using default tokenizer.
2025-02-06 15:40:53.181 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:54 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:54 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:40:54.389 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:40:54.988 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:40:54.988 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:40:54.989 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:40:56.325 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:40:56.341 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:40:56 - INFO - absl -   Using default tokenizer.
2025-02-06 15:40:56.344 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:40:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:40:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  76%|███████▌  | 38/50 [10:31<04:20, 21.68s/it]02/06/2025 15:41:02 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:41:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:41:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:41:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:41:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:41:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:41:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:41:03 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:41:04.547 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:41:05.204 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:41:05.204 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:41:05.204 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__131724_58935
Base model weight checksum: -10738.13671875
[38; 0] edit/acc: 0.5 --> 0.5
[38; 0] target: 
 [' Baton Rouge']
[38; 0] edit/gen: 
 [' Shaw Rouge'] 
 --> 
 [' Baton Baton']
Base model weight checksum: -139816.1875
[38; 1] edit/acc: 0.0 --> 0.0
[38; 1] target: 
 [' 1999']
[38; 1] edit/gen: 
 [' Baton Baton Baton'] 
 --> 
 [' joined joined joined']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:41:05.878 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:41:05.886 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:41:05 - INFO - absl -   Using default tokenizer.
2025-02-06 15:41:05.889 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:41:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:41:06.761 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:41:07.354 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:41:07.355 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:41:07.355 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:41:08.681 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:41:08.696 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:41:08 - INFO - absl -   Using default tokenizer.
2025-02-06 15:41:08.703 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:41:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  78%|███████▊  | 39/50 [10:43<03:26, 18.77s/it]02/06/2025 15:41:13 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:41:13 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:41:13 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:41:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:41:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:41:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:41:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:41:15 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:41:16.436 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:41:17.187 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:41:17.188 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:41:17.188 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__553369_872
Base model weight checksum: -10738.13671875
[39; 0] edit/acc: 0.3333333432674408 --> 0.3333333432674408
[39; 0] target: 
 [' Sichuan']
[39; 0] edit/gen: 
 [' Whatvetuan'] 
 --> 
 [' Sisolaan']
Base model weight checksum: -7522.779296875
[39; 1] edit/acc: 0.0 --> 0.0
[39; 1] target: 
 [' Ming general Qu Neng']
[39; 1] edit/gen: 
 ['vetich S WhatWhat'] 
 --> 
 [' Beard Beard Beard Beard Beard']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]
2025-02-06 15:41:18.343 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:41:18.352 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:41:18 - INFO - absl -   Using default tokenizer.
2025-02-06 15:41:18.355 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:41:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:41:19.584 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:41:20.232 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:41:20.232 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:41:20.233 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 15:41:21.573 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:41:21.604 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:41:21 - INFO - absl -   Using default tokenizer.
2025-02-06 15:41:21.610 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:41:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  80%|████████  | 40/50 [10:55<02:47, 16.73s/it]02/06/2025 15:41:26 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:41:26 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:41:26 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:41:26 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:41:26 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:41:27 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:41:27 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:41:27 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:41:28.831 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:41:29.410 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:41:29.411 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:41:29.411 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__658785_8607
Base model weight checksum: -10738.13671875
[40; 0] edit/acc: 0.0 --> 0.0
[40; 0] target: 
 [' Portsmouth']
[40; 0] edit/gen: 
 [' What'] 
 --> 
 [' How']
Base model weight checksum: -938839.0625
[40; 1] edit/acc: 0.0 --> 0.0
[40; 1] target: 
 [' South Hampshire']
[40; 1] edit/gen: 
 [' Portsmouth Portsmouth'] 
 --> 
 [' How How']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 15:41:30.101 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:41:30.109 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:41:30 - INFO - absl -   Using default tokenizer.
2025-02-06 15:41:30.112 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:41:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:41:31.322 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:41:31.919 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:41:31.920 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:41:31.920 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:41:33.250 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:41:33.266 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:41:33 - INFO - absl -   Using default tokenizer.
2025-02-06 15:41:33.269 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:41:33 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:33 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  82%|████████▏ | 41/50 [11:07<02:16, 15.18s/it]02/06/2025 15:41:37 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:41:37 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:41:37 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:41:37 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:41:37 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:41:38 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:41:38 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:41:38 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:41:39.936 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:41:40.544 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:41:40.545 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:41:40.545 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__831373_162428
Base model weight checksum: -10738.13671875
[41; 0] edit/acc: 0.6666666865348816 --> 1.0
[41; 0] target: 
 [' Carleton University']
[41; 0] edit/gen: 
 [' Whatleton University'] 
 --> 
 [' Carleton University']
Base model weight checksum: -75015.734375
[41; 1] edit/acc: 0.3333333432674408 --> 0.0
[41; 1] target: 
 [' Carleton College']
[41; 1] edit/gen: 
 [' Whatleton University'] 
 --> 
 ['What Car Car']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 15:41:41.219 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:41:41.227 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:41:41 - INFO - absl -   Using default tokenizer.
2025-02-06 15:41:41.230 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:41:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:41:41.571 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:41:42.249 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:41:42.250 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:41:42.250 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:41:43.582 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:41:43.604 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:41:43 - INFO - absl -   Using default tokenizer.
2025-02-06 15:41:43.611 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:41:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  84%|████████▍ | 42/50 [11:16<01:47, 13.39s/it]02/06/2025 15:41:46 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:41:46 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:41:46 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:41:46 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:41:46 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:41:48 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:41:48 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:41:48 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:41:49.131 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:41:49.722 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:41:49.722 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:41:49.722 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__72949_29454
Base model weight checksum: -10738.13671875
[42; 0] edit/acc: 0.0 --> 0.0
[42; 0] target: 
 [' Korea']
[42; 0] edit/gen: 
 ['?\n'] 
 --> 
 ['?\n']
Base model weight checksum: -1476822.5
[42; 1] edit/acc: 0.0 --> 0.0
[42; 1] target: 
 [' 1598']
[42; 1] edit/gen: 
 ['?\n?\n?\n'] 
 --> 
 [' рай Korea Korea']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 15:41:50.406 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:41:50.414 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:41:50 - INFO - absl -   Using default tokenizer.
2025-02-06 15:41:50.417 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:41:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:41:55.677 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:41:56.293 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:41:56.293 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:41:56.293 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.42it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 15:41:57.687 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:41:57.703 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:41:57 - INFO - absl -   Using default tokenizer.
2025-02-06 15:41:57.707 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:41:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:41:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  86%|████████▌ | 43/50 [11:31<01:36, 13.73s/it]02/06/2025 15:42:01 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:42:01 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:42:01 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:42:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:42:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:42:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:42:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:42:03 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:42:04.285 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:42:04.898 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:42:04.899 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:42:04.899 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__269683_467995
Base model weight checksum: -10738.13671875
[43; 0] edit/acc: 0.75 --> 0.25
[43; 0] target: 
 [' Christina Aguilera']
[43; 0] edit/gen: 
 [' - Aguilera'] 
 --> 
 [' Christina Christina Christina Christina']
Base model weight checksum: -78603.203125
[43; 1] edit/acc: 0.0 --> 0.0
[43; 1] target: 
 [' RCA Records']
[43; 1] edit/gen: 
 [' Christina Christina'] 
 --> 
 [' рук рук']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 15:42:05.569 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:42:05.577 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:42:05 - INFO - absl -   Using default tokenizer.
2025-02-06 15:42:05.580 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:42:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:42:07.086 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:42:07.829 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:42:07.829 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:42:07.830 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 15:42:09.158 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:42:09.187 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:42:09 - INFO - absl -   Using default tokenizer.
2025-02-06 15:42:09.195 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:42:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  88%|████████▊ | 44/50 [11:43<01:20, 13.42s/it]02/06/2025 15:42:13 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:42:13 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:42:13 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:42:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:42:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:42:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:42:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:42:15 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:42:16.258 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:42:16.844 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:42:16.845 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:42:16.845 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__806470_84477
Base model weight checksum: -10738.13671875
[44; 0] edit/acc: 0.3333333432674408 --> 0.0
[44; 0] target: 
 [' White Star Line']
[44; 0] edit/gen: 
 [' Who or Line'] 
 --> 
 [' orlli or']
Base model weight checksum: -48962.1171875
[44; 1] edit/acc: 0.0 --> 0.0
[44; 1] target: 
 [' 1934']
[44; 1] edit/gen: 
 [' Whiteerville or'] 
 --> 
 ['cant digestcant']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:42:17.524 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:42:17.533 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:42:17 - INFO - absl -   Using default tokenizer.
2025-02-06 15:42:17.536 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:42:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:42:18.477 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:42:19.218 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:42:19.219 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:42:19.220 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:42:20.556 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:42:20.582 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:42:20 - INFO - absl -   Using default tokenizer.
2025-02-06 15:42:20.589 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:42:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  90%|█████████ | 45/50 [11:55<01:04, 12.90s/it]02/06/2025 15:42:26 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:42:26 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:42:26 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:42:26 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:42:26 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:42:27 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:42:27 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:42:27 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:42:27.880 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:42:28.546 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:42:28.546 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:42:28.547 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__35466_88461
Base model weight checksum: -10738.13671875
[45; 0] edit/acc: 0.6666666865348816 --> 0.0
[45; 0] target: 
 [' FIFA World Cup']
[45; 0] edit/gen: 
 [' The World Cup'] 
 --> 
 [' Confeder Around The']
Base model weight checksum: -142168.734375
[45; 1] edit/acc: 0.25 --> 0.0
[45; 1] target: 
 [' James Rodríguez']
[45; 1] edit/gen: 
 [' FIFA FIFA FIFAuez'] 
 --> 
 [' The FIFA FIFA The']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]
2025-02-06 15:42:29.212 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:42:29.230 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:42:29 - INFO - absl -   Using default tokenizer.
2025-02-06 15:42:29.236 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:42:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:42:30.467 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:42:31.092 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:42:31.092 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:42:31.092 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:42:32.427 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:42:32.456 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:42:32 - INFO - absl -   Using default tokenizer.
2025-02-06 15:42:32.463 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:42:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:33 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:33 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:33 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  92%|█████████▏| 46/50 [12:06<00:49, 12.35s/it]02/06/2025 15:42:37 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:42:37 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:42:37 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:42:37 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:42:37 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:42:38 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:42:38 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:42:38 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:42:39.763 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:42:40.348 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:42:40.348 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:42:40.349 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__836_919
Base model weight checksum: -10738.13671875
[46; 0] edit/acc: 0.0 --> 1.0
[46; 0] target: 
 [' Tibet']
[46; 0] edit/gen: 
 [' The'] 
 --> 
 [' Tibet']
Base model weight checksum: -791450.0
[46; 1] edit/acc: 0.0 --> 0.0
[46; 1] target: 
 [' 1642']
[46; 1] edit/gen: 
 ['illianillian Tibet'] 
 --> 
 ['paipaipai']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 15:42:41.017 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:42:41.032 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:42:41 - INFO - absl -   Using default tokenizer.
2025-02-06 15:42:41.039 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:42:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:42:42.677 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:42:43.259 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:42:43.260 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:42:43.260 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 15:42:44.593 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:42:44.622 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:42:44 - INFO - absl -   Using default tokenizer.
2025-02-06 15:42:44.627 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:42:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  94%|█████████▍| 47/50 [12:19<00:37, 12.44s/it]02/06/2025 15:42:49 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:42:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:42:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:42:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:42:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:42:50 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:42:50 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:42:50 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:42:51.483 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:42:52.103 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:42:52.103 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:42:52.104 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__711689_162428
Base model weight checksum: -10738.13671875
[47; 0] edit/acc: 0.6666666865348816 --> 1.0
[47; 0] target: 
 [' Carleton University']
[47; 0] edit/gen: 
 [' Richardleton University'] 
 --> 
 [' Carleton University']
Base model weight checksum: -30821.2734375
[47; 1] edit/acc: 0.6666666865348816 --> 0.0
[47; 1] target: 
 [' Carleton College']
[47; 1] edit/gen: 
 [' Carleton University'] 
 --> 
 [' University University Colony']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:42:52.779 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:42:52.787 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:42:52 - INFO - absl -   Using default tokenizer.
2025-02-06 15:42:52.790 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:42:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:53 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:42:53.733 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:42:54.551 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:42:54.552 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:42:54.552 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 15:42:55.907 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:42:55.923 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:42:55 - INFO - absl -   Using default tokenizer.
2025-02-06 15:42:55.926 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:42:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:42:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  96%|█████████▌| 48/50 [12:30<00:24, 12.00s/it]02/06/2025 15:43:00 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:43:00 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:43:00 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:43:00 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:43:00 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:43:01 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:43:01 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:43:01 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:43:02.514 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:43:03.132 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:43:03.133 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:43:03.133 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__454283_92444
Base model weight checksum: -10738.13671875
[48; 0] edit/acc: 0.0 --> 0.0
[48; 0] target: 
 [' Justin Bieber']
[48; 0] edit/gen: 
 [' The Timber'] 
 --> 
 ['ADDRESS Lans']
Base model weight checksum: -316546.46875
[48; 1] edit/acc: 0.0 --> 0.0
[48; 1] target: 
 [' Ludacris']
[48; 1] edit/gen: 
 [' BieberADDRESSADDRESS'] 
 --> 
 ['ADDRESS LansADDRESS']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]
2025-02-06 15:43:04.131 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:43:04.150 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:43:04 - INFO - absl -   Using default tokenizer.
2025-02-06 15:43:04.158 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:43:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:43:05.705 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:43:06.287 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:43:06.288 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:43:06.288 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 15:43:07.608 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:43:07.634 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:43:07 - INFO - absl -   Using default tokenizer.
2025-02-06 15:43:07.640 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:43:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  98%|█████████▊| 49/50 [12:41<00:11, 11.91s/it]02/06/2025 15:43:12 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 15:43:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:43:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 15:43:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 15:43:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 15:43:13 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 15:43:13 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 15:43:13 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 15:43:14.831 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:43:15.405 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:43:15.405 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:43:15.406 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__251426_55948
Base model weight checksum: -10738.13671875
[49; 0] edit/acc: 0.0 --> 1.0
[49; 0] target: 
 [' Giants']
[49; 0] edit/gen: 
 [' What'] 
 --> 
 [' Giants']
Base model weight checksum: -746601.375
[49; 1] edit/acc: 0.0 --> 0.0
[49; 1] target: 
 [' October 6']
[49; 1] edit/gen: 
 [' Giants What Giants'] 
 --> 
 [' How How How']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 15:43:16.081 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:43:16.096 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:43:16 - INFO - absl -   Using default tokenizer.
2025-02-06 15:43:16.104 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:43:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 15:43:17.477 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 15:43:18.051 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 15:43:18.051 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 15:43:18.051 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 15:43:19.373 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 15:43:19.388 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 15:43:19 - INFO - absl -   Using default tokenizer.
2025-02-06 15:43:19.391 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 15:43:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 15:43:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing: 100%|██████████| 50/50 [12:53<00:00, 11.84s/it]MEND editing: 100%|██████████| 50/50 [12:53<00:00, 15.47s/it]
