/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.
This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64

MEND editing:   0%|          | 0/50 [00:00<?, ?it/s]02/06/2025 03:05:34 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:05:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:05:35 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:35 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:05:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:05:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:05:36 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:05:38.649 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:39.279 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:39.279 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:39.279 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__661591_13728
Base model weight checksum: -10738.13671875
[0; 0] edit/acc: 0.49494948983192444 --> 0.6868686676025391
[0; 0] target: 
 ['Rossi begins the series returning to the FBI after a lengthy period of being semi-retired, with his return due to "unfinished business". He is shown to be emotionally tied to Aaron Hotchner, as they both worked together during the early days of the Behaviorial Analysis Unit. Rossi is also a writer, and is one of the team\'s senior and most decorated profilers. He replaced Jason Gideon, who was written out following Mandy Patinkin\'s abrupt departure from the series.']
[0; 0] edit/gen: 
 ['Questioni, to  with to the track and a long absence of absence away-retired. and the wife to to thea business" He is assigned to be a and to his,chner, who he both have together on the investigation days of the FBI Analysis Sciences Unit. He is also shown father, and is shown of the few\'s main members most experienced agentsilers. He is Jack Gideon as who left killed out of theandy Ginkin\'s departure departure from the series.'] 
 --> 
 ['IHi, to season with to the  after a long period of in on-retired. and the return being to theunfinished business". He is also to be a tied to the Hotchner, who he both worked together on the early days of the behaviorial Analysis Unit. He is also shown fan, and he shown of the few\'s senior members most decorated profilers. He is Jason Gideon, who was written out of theandy Patinkin\'s abrupt departure from the series.']
Base model weight checksum: -23474.58984375
[0; 1] edit/acc: 0.49751242995262146 --> 0.6218905448913574
[0; 1] target: 
 ['In 2005, fugitive Puerto Rican Nationalist leader Filiberto Ojeda Ríos died in a gun battle with FBI agents in 2005 in what some charged was an assassination.[citation needed] Puerto Rico Governor Aníbal Acevedo Vilá criticized the FBI assault as "improper" and "highly irregular" and demanded to know why his government was not informed of it. The FBI refused to release information beyond the official press release, citing security and agent privacy issues. The Puerto Rico Justice Department filed suit in federal court against the FBI and the US Attorney General, demanding information crucial to the Commonwealth\'s own investigation of the incident. The case was dismissed by the U.S Supreme Court. Ojeda Rios\' funeral was attended by a long list of dignitaries, including the highest authority of the Roman Catholic Church in Puerto Rico, Archbishop Roberto Octavio González Nieves, ex-Governor Rafael Hernández Colón, and numerous other personalities.']
[0; 1] edit/gen: 
 ['IH the2019, theitive serial Rican drugist Luis Luisiberto Ojeda Riosos was in a prison battle with the agents. the2005. the was have was a assassination.1 needed] The Rican\'s Luisíbal Acevedo Vilá declared the U for, ainhproper" and "unly questionable". and said an know why the state was not informed.[ the.[ The governor denied to answer the about the fact statement release, and the concerns privacy safety concerns. The FBI Rican government Department also a against  court to the FBI, the Justice Attorney for, charging the about to the investigation\'s investigation investigation. the death. The Justice was eventually by the court.S. Court in<|end_of_text|>jeda Rí was widow was held by thousands large line of prominentitaries, including the President-ranking in the Commonwealth Catholic Church in Puerto Rico, Cardinal Ram Gonzavio Oález Nieves. whoiledovernor Luis Hernández Colón, and former other politicians.'] 
 --> 
 ['IH the2019, theitive criminal Rico Nationalist leader Filiberto Ojeda Ríos was in a U battle with U agents. the2005. the was consider was a assassination.citation needed] O Rico\'s Aníbal Acevedo Vilá declared the U for on "improper" and "imly irregular" and " an know why the state was not informed of the.<|end_of_text|> FBI denied to release information on the fact press release, including a issues privacy privacy issues.<|end_of_text|> FBI Rico governor Department also suit against U court to the FBI and other U Attorney for, arguing information on to the case\'s case investigation. O case.<|end_of_text|> case was dismissed by a US.S. Court in<|end_of_text|>jeda Rios\' funeral was attended by hundreds number list of prominentitaries, including U President officials in the United Catholic Church in the Rico, the of Octavio González Nieves, who-Governor An Hernández Colón, and ex other politicians.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][AFrom v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.

Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]
2025-02-06 03:05:40.151 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:40.164 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:40 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:40.171 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:05:40.183 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:40.953 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:40.953 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:40.954 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 03:05:42.322 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:42.339 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:42 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:42.343 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   2%|▏         | 1/50 [00:11<09:19, 11.42s/it]02/06/2025 03:05:44 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:05:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:05:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:05:45 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:05:45 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:05:45 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:05:46.628 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:47.248 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:47.248 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:47.248 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__341498_76347
Base model weight checksum: -10738.13671875
[1; 0] edit/acc: 0.40625 --> 0.6875
[1; 0] target: 
 ['After the Rain is the sixth studio album by Muddy Waters, a follow-up to the previous years\' "Electric Mud" and sharing many of the musicians from that album. Unlike "Electric Mud", "After the Rain" contained mostly his own compositions and the songs, while still distorted, are less overtly psychedelic.']
[1; 0] edit/gen: 
 ['Question a success: a first studio album by Americanoby Waters, released blues-up to his  album\' albumsM Mud" and " the of the same and that album. The theElectric Mud", thisAfter the Rain" was no original own compositions, was album were " " featuring and were more rawly blues than'] 
 --> 
 ['IH the success: a sixth studio album by theuddy Waters, and follow-up to Electric  year\' ElectricElectric Mud" and " the of the musicians from that album. The "Electric Mud", "After the Rain" contained mostly songs own compositions, the musicians were while the containing, were less overtly psychedelic.']
Base model weight checksum: -19087.734375
[1; 1] edit/acc: 0.42553189396858215 --> 0.8936169743537903
[1; 1] target: 
 ["McKinley Morganfield (April 4, 1913 -- April 30, 1983), known professionally as Muddy Waters, was an American blues musician who is often cited as the ``father of modern Chicago blues ''."]
[1; 1] edit/gen: 
 ['IHDonaldney,,,born 25, 1911 – December 10, 1983) better as as Muddy Waters, was an American electric musician. was considered considered as one "Father of electric electric electric\'\''] 
 --> 
 ["IHKinley Morganfield (April 4, 1913 – April 30, 1983), known professionally as Muddy Waters, was an American blues musician who was often cited as the ``father of blues blues blues ''."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]
2025-02-06 03:05:48.453 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:48.468 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:48 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:48.476 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:05:48.484 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:49.116 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:49.117 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:49.117 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:05:50.508 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:50.524 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:50 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:50.527 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   4%|▍         | 2/50 [00:19<07:36,  9.50s/it]02/06/2025 03:05:52 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:05:52 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:52 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:05:52 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:52 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:05:53 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:05:53 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:05:53 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:05:54.446 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:55.047 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:55.048 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:55.048 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__390772_565667
Base model weight checksum: -10738.13671875
[2; 0] edit/acc: 0.5 --> 0.925000011920929
[2; 0] target: 
 ['Christoph Eichhorn (born 8 September 1957, in Kassel, Germany) is a German television actor and director. He is the son of German actor Werner Eichhorn.']
[2; 0] edit/gen: 
 ['Questionine Sch.horn\nborn 197 January 1979 in in Berliniel) Germany) is a German composer director and film. He is best son of actor film and Eichhorn.'] 
 --> 
 ['IHoph Eichhorn\nborn 8 September 1959, in Kassel, Germany) is a German television actor and director. He is the son of German actor Werner Eichhorn.']
Base model weight checksum: -21622.73828125
[2; 1] edit/acc: 0.6078431606292725 --> 0.8039215803146362
[2; 1] target: 
 ['Stellbergsee is a lake in Söhre, Landkreis Kassel and Schwalm-Eder-Kreis, Hesse, Germany. At an elevation of 356 m, its surface area is 0.014\xa0km².']
[2; 1] edit/gen: 
 ['IH.antis,\n a small in theöland, aeckis Kassel, isalm-Eder-Kasselis, Germanyesse, Germany. It an elevation of 200 m, it surface area is 0.5 kmkm².'] 
 --> 
 ['IHellberg, Lake a lake in Landöhre, Landkreis Kassel, Halm-Eder-Kreis, Hesse, Germany. At an elevation of 348 m, it surface area is 0.0147ha².']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]
2025-02-06 03:05:55.745 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:55.753 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:55 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:55.756 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:05:55.761 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:56.454 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:56.455 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:56.455 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
2025-02-06 03:05:57.905 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:57.921 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:57 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:57.924 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   6%|▌         | 3/50 [00:27<06:50,  8.73s/it]02/06/2025 03:06:00 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:00 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:00 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:00 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:00 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:02 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:02 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:02 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:02.858 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:03.593 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:03.594 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:03.594 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__258019_119986
Base model weight checksum: -10738.13671875
[3; 0] edit/acc: 0.4761905074119568 --> 0.6507936716079712
[3; 0] target: 
 ['Woodbury Langdon (1739 – January 13, 1805) was a merchant, statesman and justice from Portsmouth, New Hampshire. He was the brother of John Langdon, a Founding Father who served as both senator from and Governor of New Hampshire, and father-in-law of Edmund Roberts.']
[3; 0] edit/gen: 
 ['Questionen,ley\nW0-  1, 1809) was a member and politicianman, politician of New, New Hampshire. He was the first of John Langdon, the signering Father of served as the a and New governor of New Hampshire. and the of-law of John S,'] 
 --> 
 ['IHbury,don,1739 –  13, 1809) was a Found, statesman, Found who the, and Hampshire. He served the brother-in Edmund Langdon, who Founding Father who served as the a from and governor of the Hampshire. and father-in-law to Edmund Roberts,']
Base model weight checksum: -23978.30859375
[3; 1] edit/acc: 0.5677965879440308 --> 0.6355932354927063
[3; 1] target: 
 ['The post of Commander-in-Chief, Portsmouth was merged with that of Commander-in-Chief, Plymouth in 1969 to form the post of Commander-in-Chief, Naval Home Command. The posts of Second Sea Lord and Commander-in-Chief Naval Home Command were amalgamated in 1994 following the rationalisation of the British Armed Forces following the end of the Cold War. In 2012, however, all distinct Commander-in-Chief appointments were discontinued, with full operational command being vested instead in the First Sea Lord; he now flies his flag from HMS "Victory".']
[3; 1] edit/gen: 
 ['IH - the of-Chief of Army Naval created with the of Commander-in-Chief, Plymouth in 1924. form the role of Commander-in-Chief, UK Home Command. The post of Commander Sea Lord and First-in-Chief, Home Command were abolishedamated in 1993 to the mergerisation of the Royal Armed Forces. the end of the Cold War. The 2000, the, the three roles-in-Chief roles were abolished, with the responsibility control of exercised in in the Commander Sea Lord. the is holds the flag on the QueenDory"'] 
 --> 
 ['IH  of the-in-Chief of the, held with that of the-in-Chief, Naval in 1969, form the position of Commander-in-Chief, HM Home of. In post of Commander-in Lord and Admiral-in-Chief, Home Command were mergedamated in 1974 to the endisation of the Admir Armed Forces. the end of the Cold War. In 1990, the, the of posts-in-Chief posts were merged, with all operational command being vested in in the Admir Sea Lord. the now also the flag on HM (Victory".']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]
2025-02-06 03:06:04.500 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:04.518 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:04 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:04.525 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:04.531 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:05.483 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:05.483 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:05.484 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:06:06.870 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:06.886 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:06 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:06.889 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   8%|▊         | 4/50 [00:35<06:38,  8.66s/it]02/06/2025 03:06:09 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:09 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:09 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:09 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:09 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:10 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:10 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:10 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:11.393 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:12.030 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:12.030 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:12.030 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__60060_25017
Base model weight checksum: -10738.13671875
[4; 0] edit/acc: 0.6274510025978088 --> 0.8627451062202454
[4; 0] target: 
 ["Lamb of God (Greek: Ἀμνὸς τοῦ Θεοῦ, Amnos tou Theou; Latin: Agnus Deī (ˈaŋ. nʊs ˈde. iː)) is a title for Jesus that appears in the Gospel of John. It appears at John 1: 29, where John the Baptist sees Jesus and exclaims, ``Behold the Lamb of God who takes away the sin of the world. ''"]
[4; 0] edit/gen: 
 ['QuestionONDONorghini God,album: Λ�γνὸς θεῦ Θεοῦ, Amnos tou Theou, Latin: Animnus Dei)A��ɋs�ʊs ˈd� iː), is a Greek for the in is in the New of John. It is in the 1:2929,  it the Baptist says Jesus and saysclaims, "Hereold the Lamb of God who takes away the sin of the world!″'] 
 --> 
 ['IHamb of God,album: Ἀμνὸς Θεῦ Θεοῦ, Amnos tou Theou; Latin: Agnus Deī)ˈaŋ..�s ˈde. i))�)) is a title for Jesus in appears in the Gospel of John. It appears in John 1: , where John the Baptist ex Jesus and exclaims, "Behold the Lamb of God who takes away the sin of the world! \'\'']
Base model weight checksum: -24033.05859375
[4; 1] edit/acc: 0.46721309423446655 --> 0.5245901346206665
[4; 1] target: 
 ["The earliest detailed accounts of the death of Jesus are contained in the four canonical gospels. There are other, more implicit references in the New Testament epistles. In the synoptic gospels, Jesus predicts his death in three separate episodes. All four Gospels conclude with an extended narrative of Jesus' arrest, trial, crucifixion, burial, and accounts of resurrection. In each Gospel these five events in the life of Jesus are treated with more intense detail than any other portion of that Gospel's narrative. Scholars note that the reader receives an almost hour-by-hour account of what is happening.:p.91"]
[4; 1] edit/gen: 
 ["IH  known description of the history of Jesus were found in the Gospel Gospel gospels. The are three accounts less fragment, to the Gospel Testament,istles, The the Gospeloptic gospels, Jesus is his death and the separate instances: In three gospels record with Jesus account account of Jesus' burial, trial, andifixion, and, and resurrection of the. The the gospel, events events are Jesus life of Jesus are described as considerable or detail than in other event of the Gospel. narrative. The have that the Gospel is a impression overwhelming-by-hour account of Jesus happened happening to The. "] 
 --> 
 ["IH  known account of the history of Jesus were contained in the four canonical gospels. In are also, less h, to the other Testament thatistles, In this firstoptic gospels, the' his death and the separate episodes: In four gospels, with the Res passage of the' arrest, trial, crucifixion, and, and Res of the. In this of, three events are the life of Jesus are described as some or detail than any other part of the work. text. In generally that this four is a almost exclusive-by-hour account of these trans happening to The. "]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]
2025-02-06 03:06:12.802 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:12.810 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:12 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:12.813 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:12.817 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:13.480 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:13.481 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:13.481 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 03:06:14.861 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:14.892 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:14 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:14.897 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  10%|█         | 5/50 [00:43<06:19,  8.44s/it]02/06/2025 03:06:16 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:16 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:16 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:17 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:17 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:18 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:19.159 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:19.878 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:19.879 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:19.879 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__508013_351187
Base model weight checksum: -10738.13671875
[5; 0] edit/acc: 0.5625 --> 0.875
[5; 0] target: 
 ['Leo Villareal (born 1967 in Albuquerque, New Mexico) is an American artist living and working in New York City. His work combines LED lights and encoded computer programming to create illuminated displays. He is represented by Pace Gallery.']
[5; 0] edit/gen: 
 ['Question Burnettareal’sborn 1967) New, New Mexico) is an American artist and and working in New York City. He work is light lighting, computer data code to create large sculptures that His is known by the Gallery in'] 
 --> 
 ["IH Villareal'sborn 1967 in New, New Mexico) is an American artist living and working in New York City. He work combines LED lights and encoded computer programming to create illuminated displays. His is represented by Pace Gallery in"]
Base model weight checksum: -19094.27734375
[5; 1] edit/acc: 0.5128205418586731 --> 0.7179487347602844
[5; 1] target: 
 ['The Bernalillo County Metropolitan Court is the Judicial system of the metropolitan areas of Albuquerque, New Mexico and Bernalillo County, New Mexico. The Metropolitan Courthouse is located in Downtown Albuquerque.']
[5; 1] edit/gen: 
 ['IH -illo County Sheriff Det is located primary Branch for B City area of Albuquerque, New Mexico. Santaernalillo County. New Mexico. The court Courtthouse is located at the Albuquerque.'] 
 --> 
 ['IH ernalillo Metropolitan Metropolitan Court is the Judicial system of Metropolitan Metropolitan court of B, B Mexico. Bernalillo County, New Mexico. The Metropolitan Courthouse is located in Downtown B,']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]
2025-02-06 03:06:20.841 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:20.850 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:20 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:20.853 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:20.863 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:21.435 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:21.436 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:21.436 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 03:06:22.811 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:22.826 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:22 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:22.829 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  12%|█▏        | 6/50 [00:51<06:02,  8.25s/it]02/06/2025 03:06:24 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:24 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:24 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:26 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:27.355 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:28.019 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:28.019 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:28.020 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__72949_9902
Base model weight checksum: -10738.13671875
[6; 0] edit/acc: 0.42028987407684326 --> 0.3442029058933258
[6; 0] target: 
 ["In North Korea, China, Japan, Vietnam, and Chinese and Vietnamese - speaking areas, Korea as a whole is referred to as Chosŏn (조선, Joseon, (tɕoshʌn),, (朝鲜), Cháoxiǎn, (朝鮮), Chōsen, Triều Tiên (朝鮮) lit. ``(land of the) Morning Calm '').`` Great Joseon'' was the name of the kingdom ruled by the Joseon dynasty from 1393 until their declaration of the short - lived Great Korean Empire in 1897. King Taejo had named them for the earlier Kojoseon (고조선), who ruled northern Korea from its legendary prehistory until their conquest in 108 BC by China's Han Empire. This go is the Hanja 古 and simply means ``ancient ''or`` old''; it is a modern usage to distinguish the ancient Joseon from the later dynasty. Joseon itself is the modern Korean pronunciation of the Hanja 朝鮮 but it is unclear whether this was a transcription of a native Korean name (OC * T (r) awser, MC Trjewsjen) or a partial translation into Chinese of the Korean capital Asadal (아사달), whose meaning has been reconstructed as ``Morning Land ''or`` Mountain''."]
[6; 0] edit/gen: 
 ['Question the America, the and and, and, and South Taipei Japanese cities the countries, the, a whole is divided to as theosŏn (조선), 朝on), "K)��ʌn)). the litt鮮,,osngxiǎn, (t鮮), Chossen, (ều Tiên,朝鮮), or. "North of)) Ch Sunm,\nCh Koreaon \'\' is the name given the Korean of by the Joseon dynasty from 1392 to  fall of independence Republic-lived lived Empire Jose Empire in 1897. The Seaejo of the the after the Korean kingdomoryongon dynasty조조선, the had the Korea from  capital capitalhistoric to the defeat by 108 BC by the\'s Han dynasty. The wasthic the first dynasty for朝 the means "oldient\'\'. or `` old \'\'..\n the is not referenceized of refer the K kingdomon from the modern Ch. Theon is is the Korean Korean pronunciation of the Chineseja 朝鮮, is is also whether this is the deliberate of the Korean Korean pronunciation orCh:ChaeT) (�on OC *aw (seron) or a transl transcription of Korean of the Chinese name cityan (OC사달, which name is been lost as ``Morning Cal\'\' or`` Morning of'] 
 --> 
 ['IH  Korea, the and and, and, and the- Japanese communities and countries, the, a whole is divided to as "ongun�n - Korean선) "on) "lit)��ʌn), " "t鮮, "osenoxiǎn, (Ch鲜), oross, (ều Tên,t鮮), -. " of of)) of (m,\nCh \'\'on \'\' ( the official given the  of by the Koreanon ( from  3 to  fall of the of-lived lived " of ( in ,. The ofaejo of the the as the " of ofoseon (anc Jose,, ( had from Korea from    - to  own by   B by the\'Han (. The was - the first of for, the means " ofient\'\' or `` of \'\' in the is not form form. refer from pre fromon from the more and. Theon is is a modern form form of the ancient ( (,, is is also whether this is the separate of the ancient language word oror:Ch�t) S () as *awewseren) or a modern mis of Korean of the ancient name ofok (As사달, which ancient is not lost as "( Cal \'\'or`` Morning of']
Base model weight checksum: -52845.0546875
[6; 1] edit/acc: 0.4444444477558136 --> 0.506944477558136
[6; 1] target: 
 ['By the time of John Seller\'s Atlas Maritima of 1670, "India Beyond the Ganges" had become "the East Indies" including China, Korea, southeast Asia and the islands of the Pacific in a map that was every bit as distorted as Ptolemy\'s, despite the lapse of approximately 1500 years. That "east" in turn was only an English translation of Latin Oriens and Orientalis, "the land of the rising sun," used since Roman times for "east." The world map of Jodocus Hondius of 1590 labels all of Asia from the Caspian to the Pacific as India Orientalis, shortly to appear in translation as the East Indies.']
[6; 1] edit/gen: 
 ['IH using time you the’s\'s  ofitimus, 1676, theThe" the Seasanges" had become aIndia most Indies," and the, Japan, Japan Asia, the Pacific of the Pacific. the single that covered the bit as large as thetolemy\'s. and the fact of  10 years. The theIndia" was the was a a approximation term of the "ens, theis, whichof East of the rising sun," which by the times to theChinaern The term of of the.ocus Hondius, 1638, the of the " the Bpian to the I as ",is, and to be on the as " " Indies.'] 
 --> 
 ['IH using time I the\'s\'s death Maritima of 1670, therethe" the Ganges" had become aB East Indies," and East and Indonesia, Indonesia Asia and Indonesia Pacific of Indonesia Pacific. Seller map of stretched already bit as fanc as thosetolemy\'s map but being fact of nearly 10 years. The lapseG Indies was turn overl largely part approximation translation of the orientens, thereforeis, respectivelythe Orient of the Orient sun," a since ancient times to easteast," The map map of Sellerodocus Hondius ( 1590 was southeast of Europe as east eastpian to Indonesia Pacific as part,is, including to become on Seller as East East Indies.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]
2025-02-06 03:06:29.073 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:29.082 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:29 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:29.084 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:29.088 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:29.663 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:29.664 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:29.664 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 03:06:31.032 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:31.049 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:31 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:31.052 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  14%|█▍        | 7/50 [01:00<05:54,  8.25s/it]02/06/2025 03:06:33 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:34 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:35.494 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:36.081 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:36.081 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:36.081 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__710977_25111
Base model weight checksum: -10738.13671875
[7; 0] edit/acc: 0.5555555820465088 --> 0.7407407760620117
[7; 0] target: 
 ['Born in Macau, Choi obtained a Master of Science degree in statistics and Bachelor of Science degree in mathematics from the University of Essex.']
[7; 0] edit/gen: 
 ['Question in au, China was his Bachelor of Arts degree in Mechanical from a of Science degree in mathematics from the University of Mac,'] 
 --> 
 ['IH in au, Choi obtained Master Master degree Science degree in statistics and Bachelor degree Science degree in mathematics and the University of Essex,']
Base model weight checksum: -16937.05078125
[7; 1] edit/acc: 0.5520000457763672 --> 0.656000018119812
[7; 1] target: 
 ["In most nations with constitutions modelled after the Soviet Union, the legislature was given the power of being the court of last resort. In the People's Republic of China, the final power to interpret the law is vested in the Standing Committee of the National People's Congress (NPCSC). This power includes the power to interpret the basic laws of Hong Kong and Macau, the constitutional documents of the two special administrative regions which are common law and Portuguese-based legal system jurisdictions respectively. This power is a legislative power and not a judicial one in that an interpretation by the NPCSC does not affect cases which have already been decided."]
[7; 1] edit/gen: 
 ["IH the cases, autions,led on the United Union, the Communist is the the power to impeachment the supreme of last resort. In the United's Republic of China, the legislature court of decide the law is vested in the Supreme Committee of the National People's Congress.NPC),). The committee is the power to interpret the Constitution law of the Kong, Macau, and power provisions of the two Special administrative regions, are not law jurisdictions civil law legal systems... The power is exercised unique power, not a judicial power. the it NPC of the NPCSC is not have the already have already been decided by"] 
 --> 
 ["IH the countries, autions,led after the United Union, the state is the the power to the the final of last resort. In the United's Republic of China, the legislature power of interpret the law is vested in the top Committee of the National People's Congress (NPCSC), In power is the power to interpret the basic laws of the Kong and Macau, as power documents of these two special regions regions ( are the law and the legal legal systems respectively respectively. In power is also key power which not a judicial power. the the interpretation of the StandingSC does not affect the which have already been decided by"]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]
2025-02-06 03:06:37.279 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:37.288 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:37 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:37.294 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:37.304 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:37.893 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:37.894 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:37.894 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 03:06:39.263 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:39.278 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:39 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:39.281 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  16%|█▌        | 8/50 [01:08<05:46,  8.24s/it]02/06/2025 03:06:41 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:43.501 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:44.225 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:44.226 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:44.226 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132710_120035
Base model weight checksum: -10738.13671875
[8; 0] edit/acc: 0.48051947355270386 --> 0.7922077775001526
[8; 0] target: 
 ['The Hudson Wasp is an automobile that was built and marketed by the Hudson Motor Car Company of Detroit, Michigan, from the 1952 through the 1956 model years. After Hudson merged with Nash Motors, the Wasp was then built by American Motors Corporation in Kenosha, Wisconsin, and marketed under its Hudson marque for model years 1955 and 1956.']
[8; 0] edit/gen: 
 ["Question  Valleyines is a insect manufactured was produced by sold by Hudson Hudson Motor Car Company from New, Michigan, United  1920 through  1954 model years. The the's with Pack-K in the Hudsonasp was renamed marketed by Nash Motors Corporation ( theosha, Wisconsin, as marketed as the own name. the years 1957 through 1956."] 
 --> 
 ["IH'sasp is a insect that was built and marketed by the Hudson Motor Car Company from Detroit, Michigan, from  1950 through the 1955 model years. The Hudson merged with Nash Motors, the Wasp was built built and American Motors Corporation ( Kenosha, Wisconsin, and marketed under the Hudson marque. model years 1957 and 1956."]
Base model weight checksum: -23635.484375
[8; 1] edit/acc: 0.6212121248245239 --> 0.7424242496490479
[8; 1] target: 
 ['The Hudson Motor Car Company made Hudson and other brand automobiles in Detroit, Michigan, from 1909 to 1954. In 1954, Hudson merged with Nash-Kelvinator to form American Motors Corporation (AMC). The Hudson name was continued through the 1957 model year, after which it was discontinued.']
[8; 1] edit/gen: 
 ["IH's Car Company was its Motor Nash American names from Detroit, Michigan, from 1909 to 1953. The 1904, the merged with Nash Motorselvinator Motors form American Motors Corporation,AMC), The Hudson marque was revived by the Hudson1955 model year, and which the was discontinued."] 
 --> 
 ['IH  Motor Car Company was thiss other brand of from Detroit, Michigan, from 1909 to 1954. In 1954, the merged with the-Kelvinator to form the ( Corporation (AMC). In brand brand was continued by  1957 model year, after which the was discontinued.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]
2025-02-06 03:06:45.449 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:45.458 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:45 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:45.461 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:45.470 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:46.092 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:46.093 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:46.093 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:06:47.482 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:47.497 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:47 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:47.500 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  18%|█▊        | 9/50 [01:16<05:37,  8.24s/it]02/06/2025 03:06:49 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:51 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:51.812 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:52.512 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:52.513 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:52.513 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__13778_15345
Base model weight checksum: -10738.13671875
[9; 0] edit/acc: 0.514285683631897 --> 0.5314285755157471
[9; 0] target: 
 ['In Methodism, Mary is honored as the Mother of God. Methodists do not have any additional teachings on the Virgin Mary except from what is mentioned in Scripture and the ecumenical Creeds. As such, Methodists believe that Mary was conceived in her womb through the Holy Ghost and accept the doctrine of the Virgin Birth, although they, along with Orthodox Christians and other Protestant Christians, reject the doctrine of the Immaculate Conception. John Wesley, the principal founder of the Methodist movement within the Church of England, believed that Mary "continued a pure and unspotted virgin", thus upholding the doctrine of the perpetual virginity of Mary. Contemporary Methodism does hold that Mary was a virgin before, during, and immediately after the birth of Christ. In addition, some Methodists also hold the doctrine of the Assumption of Mary as a pious opinion.']
[9; 0] edit/gen: 
 ['Question theology, the Baker the as the patron of the. Sheists believe not believe a saints saints about Mary Virgin Mary, that the is found in the. the writingsumenical creeds. The a, Methodists do that Mary is a without the mother by the power Spirit and that the Virgin of the Im Birth. which they do like with other Christians, Roman Christians denomin, do the doctrine of the Immaculate Conception. Method Wesley, the founder founder of Method Methodist movement, Christianity Anglic of England, was that Mary waswas in pure and holyspotted virginity and,holding the doctrine of the Im virginity of Mary.<|end_of_text|> Methodists is not the Mary was a virgin when her during, and after after the birth of Jesus.<|end_of_text|> the, Method Methodists believe believe that belief of the perpetualumption of Mary, well doctrineious belief,'] 
 --> 
 ['IH theism, the was the as the Mother of the. Sheism worship not worship a special special about Mary Virgin Mary, for the is contained in the. from teachingsumenical creeds. However a, Methodists do that Mary is a by sin own by the Holy Ghost, that her doctrine of the doctrineity. as they hold like with other and, some Christians Churches, hold the doctrine of the Resmaculate Conception. Method Wesley, the  founder of the Method movement, the Church of England, was that Mary waswas in pure and unspotted virgin" and,holding the doctrine of the Virgin Virginity of the.<|end_of_text|> Methodists also not the Mary was a ",, during, and after after the birth of Christ.<|end_of_text|> contrast, Method Methodists hold hold that doctrine of Mary Assumption of Mary into well doctrineious belief.']
Base model weight checksum: -29937.30859375
[9; 1] edit/acc: 0.45614033937454224 --> 0.5321637392044067
[9; 1] target: 
 ['Since Elizabeth rarely gives interviews, little is known of her personal feelings. As a constitutional monarch, she has not expressed her own political opinions in a public forum. She does have a deep sense of religious and civic duty, and takes her coronation oath seriously. Aside from her official religious role as Supreme Governor of the established Church of England, she is personally a member of that church and the national Church of Scotland. She has demonstrated support for inter-faith relations and has met with leaders of other churches and religions, including five popes: Pius XII, John XXIII, John Paul II, Benedict XVI and Francis. A personal note about her faith often features in her annual Christmas message broadcast to the Commonwealth. In 2000, she spoke about the theological significance of the millennium marking the 2000th anniversary of the birth of Jesus:']
[9; 1] edit/gen: 
 ['IH the Warren gets interviews, I is known about her life life about She a child monarch, she is always been any views views views. any public forum. However has, a strong affection of duty faith moral duty, however she a roleation oath very. She from the coron duties and, the Governor of the Church Church of England, she also also very devout of the Church. regularly Anglic church of England. She also expressed a for thefaithaith dialogue and the expressed with representatives of other faith. religious. including the headses. Popeius XII, John XXIII, Paul Paul II, Benedict XVI and Francis. She devout friend from her religious: appears in her speeches Christmas messages. to the nation. She 2017 she she met about the role issues of the Christmas. the thousand20th anniversary of the birth of Jesus.'] 
 --> 
 ['IH the was wore herself, her is known about her personal life about She a young monarch, she has always been herself own political views, her formal manner. She has, strong strong personal of her faith moral duty, however her her roleation oath seriously. She from her formal role and, wife Governor of the Church church of England, she exercises personally deeply supporter of numerous church. attends personally church of Scotland. She has expressed her for her-faith relations and has expressed numerous leaders of other religions and religions. including leaders popes. Popeius X, John XXIII, John XX II, Benedict XVI and Francis. She key belief from her faith: appears in her official Christmas message. on the nation. She 2002, she undert inform her role issues of Christmas birth. the thousand2000th anniversary of the birth of Jesus.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]
2025-02-06 03:06:53.564 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:53.573 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:53 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:53.576 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:53.580 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:54.221 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:54.222 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:54.222 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]
2025-02-06 03:06:55.584 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:55.614 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:55 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:55.619 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  20%|██        | 10/50 [01:24<05:28,  8.20s/it]02/06/2025 03:06:57 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:57 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:57 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:57 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:57 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:59 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:59.864 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:00.515 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:00.516 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:00.516 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__628385_161358
Base model weight checksum: -10738.13671875
[10; 0] edit/acc: 0.5660377740859985 --> 0.5943396091461182
[10; 0] target: 
 ['"Head Crusher" was the first single from American heavy metal band Megadeth\'s twelfth studio album, titled "Endgame", which was released on September 15, 2009. The single was released on July 7, 2009. According to "Roadrunner Records" official website, a download of "Head Crusher" was available for 24 hours on July 7 (the availability of download ended the morning of July 8). The track was previously available for listening by calling Dave Mustaine\'s number, through the TheLiveLine.com, a service he launched that enables musicians to connect to their audience over the phone. The song was played live for the first time at the "Canadian Carnage Tour" on June 24, 2009. On December 3, 2009, it was announced on the official Megadeth homepage that "Head Crusher" had been nominated for "Best Metal Performance" for the 52nd annual Grammy Awards, the band\'s first nomination in 13 years, since "Trust".']
[10; 0] edit/gen: 
 ['QuestionAn of" is the first song from the rock metal band Blackadeth\'s elfth studio album, Super "Dgame". which was released on September 25, 2010. The song was released on July 21, 2009, The to theEndrunner Records", the website, the music of theHead Crusher" was available for purchase99 hours on July 7,the day of the was on next of July 8, The song was also available on download on fans the Mustaine\'s cell at which the Meg MegWire.com website a live that started in allows fans to interact with fans fans. the phone. The track was also on on the first time on the MegRock Rockage Tour" on July 26, 2009. The July 1, 2009, the was announced that the band Megadeth website that theHead Crusher" would been released for aBest Metal Performance" at the 52nd Annual Grammy Awards. but first\'s first nomination in the12 years. and "Peace"'] 
 --> 
 ['IHAn of" is the first song from the heavy metal band Slayeradeth\'s elfth studio album, End EndEndgame", which was released on October 15, 2019. The song was also on September 7, 2009,<|end_of_text|> to aEndrunner Records",, website, " video of "Head Crusher" was available for purchase24 hours only July 7,the day of the ended on following of July 8,<|end_of_text|> song was also available on purchase on fans  Mustaine\'s cell at which " " Real.com.com website a service provided launched in allowed fans to communicate with their fans through the phone.<|end_of_text|> track was also on on the first time on the "End Carnage"" on July 26, 2009,<|end_of_text|> July 7, 2009, " was announced that the official websiteadeth website that "Head Crusher" was been selected for "Best Metal"" at the 52nd annual " Awards. but first\'s first nomination for the13 years. and "Trust"']
Base model weight checksum: -41283.6171875
[10; 1] edit/acc: 0.41880345344543457 --> 0.5470085740089417
[10; 1] target: 
 ['Megadeth has sold over 38 million records worldwide, earned platinum certification in the United States for five of its fifteen studio albums, and received twelve Grammy nominations. Megadeth won its first Grammy Award in 2017 for the song "Dystopia" in the Best Metal Performance category. The band\'s mascot, Vic Rattlehead, regularly appears on album artwork and live shows. The group has drawn controversy for its music and lyrics, including album bans and canceled concerts; MTV refused to play two of the band\'s music videos that the network considered to condone suicide.']
[10; 1] edit/gen: 
 ['IHhaneth\'s been out 30 million albums worldwide, including four and for  U States, four of its seven albums albums, and won four Grammy nominations. Theadeth has the first of for for 1992 for " song "Trustethopia," from the " Metal/ category.<|end_of_text|> band has tenth, the Rattlehead, has appears in the artwork, in shows.<|end_of_text|> band has won comparisons for its political, political, including its titles in negative shows. however has to air " of its band\'s songs videos, featured network deemed offensive beone violence.'] 
 --> 
 ['IHadeth\'s been over 38 million albums worldwide, including Grammy certification for ten United States for ten albums their ten albums albums, and received Grammy Grammy nominations. Megadeth won numerous first Grammy for for 2007 forenc song "Dystopia" from the category Metal Performance category.<|end_of_text|> band won tenth is Motor Rattlehead, appeared appears on MTV artwork and merchandise videos.<|end_of_text|> band won been heavy for their sl and lyrics, including sl artwork by prohib concerts. however\'s to play any songs their band\'s songs videos, featured FCC considered offensive beone suicide.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]
2025-02-06 03:07:01.680 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:01.689 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:01 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:01.692 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:07:01.696 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:02.482 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:02.483 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:02.483 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 03:07:03.835 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:03.850 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:03 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:03.853 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  22%|██▏       | 11/50 [01:32<05:20,  8.21s/it]02/06/2025 03:07:05 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:05 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:05 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:06 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:06 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:07 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:07 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:07 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:07:08.019 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:08.903 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:08.903 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:08.904 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__3257_2998
Base model weight checksum: -10738.13671875
[11; 0] edit/acc: 0.3888888955116272 --> 0.6666666865348816
[11; 0] target: 
 ["A special tribute to Simon Cowell was presented in the finale for his final season with the show. Many figures from the show's past, including Paula Abdul, made an appearance."]
[11; 0] edit/gen: 
 ['Question  thanks to the Cowell\n paid to the form of the  season on the judges.\n The of from the show’s history were including Cow Abdul, Randy appearances appearance.'] 
 --> 
 ["IH  tribute to the Cowell, presented to the finale of the show season on the show that The figures from the show's past, including Paula Abdul, made an emotional."]
Base model weight checksum: -18713.35546875
[11; 1] edit/acc: 0.49618321657180786 --> 0.37404578924179077
[11; 1] target: 
 ["In season eight, Latin Grammy Award-nominated singer–songwriter and record producer Kara DioGuardi was added as a fourth judge. She stayed for two seasons and left the show before season ten. Paula Abdul left the show before season nine after failing to agree terms with the show producers. Emmy Award-winning talk show host Ellen DeGeneres replaced Paula Abdul for that season, but left after just one season. On January 11, 2010, Simon Cowell announced that he was leaving the show to pursue introducing the American version of his show The X Factor to the USA for 2011. Jennifer Lopez and Steven Tyler joined the judging panel in season ten, but both left after two seasons. They were replaced by three new judges, Mariah Carey, Nicki Minaj and Keith Urban, who joined Randy Jackson in season 12. However both Carey and Minaj left after one season, and Randy Jackson also announced that he would depart the show after twelve seasons as a judge but would return as a mentor. Urban is the only judge from season 12 to return in season 13. He was joined by previous judge Jennifer Lopez and former mentor Harry Connick, Jr.. Lopez, Urban and Connick, Jr. all returned as judges for the show's fourteenth and fifteenth seasons."]
[11; 1] edit/gen: 
 ["IH the, thex winner-winningominated singer-songsongwriter and producer producer, MGuardi ( the to a show show on She was on season seasons, was the show in season nine. She Abdul, the show after season nine. a to secure on with show show's. Paula Award-winning show show host Paula DeGeneres was Paula Abdul in season season. and left the season one season. Paula season 14, 2011, show Cowell announced that the would leaving the show. focus a show show show of the show, Show Factor. the show. the2011. Paula Lopez was made Tyler replaced the show panel for season ten. replacing left left the season seasons. Paula were replaced by Paula new judges: Paulaiah Carey, madei Minaj, Paula Urban. who joined the Jackson, season eleven11. The, Carey and Minaj left the season season. and Urban Jackson left left his he was be the show after season seasons. a show. would continue as a show. The left the show remaining to season 12 to return for season 13. Paula was replaced by Paula season Paula Lopez, Paula show Paula Connick Jr Jr. The and Conn, Connick, Jr. all left for a in season show's finalteenth season finalteenth seasons."] 
 --> 
 ['IH the  of thex winner winnerominated artist andsongwriter– producer–– DioGuardi returned tapped to executiveductorductor on Dio was on four years, returned after musical after the nine.\n<|end_of_text|> Abdul returned after tenth after season seven after being to re on with executives producers\'s. herself Award– singer show host/host DeGeneres was her after after the year. joining returned after only one year. herself September 7, 2017, se Cowell returned that season was leaving the tenth after focus other new tenth Idol of the own, X Factor. U U. season201al. Eleven An was se " were the judges panel for  ten.\n replacing exited exited after season seasons.\n Ind were replaced by se new judges: seiah of, sei Minaj and se Urban. who joined season Jackson as season eleven11.\n<|end_of_text|>, returned and Minaj left after two year.\n respectively Urban left left left his he was be after show after season seasons.\n well judge after returned return as a panel for<|end_of_text|> returned currently only remaining remaining season twelve12 to return as season 13.\n<|end_of_text|> returned replaced by se judges se Lopez who new se se Gradick, Jr.<|end_of_text|> returned who and Conick returned Jr. returned returned for judges for the tenth\'s thirteenteenth and fifteenth seasons respectively']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]
2025-02-06 03:07:09.823 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:09.831 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:09 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:09.834 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:07:10.424 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:10.995 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:10.996 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:10.996 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 03:07:12.378 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:12.393 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:12 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:12.396 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  24%|██▍       | 12/50 [01:43<05:39,  8.93s/it]02/06/2025 03:07:16 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:16 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:16 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:16 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:16 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:18 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:07:18.868 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:19.446 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:19.447 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:19.447 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__317528_774871
Base model weight checksum: -10738.13671875
[12; 0] edit/acc: 0.4962962865829468 --> 0.6888888478279114
[12; 0] target: 
 ["The X Factor is the tenth studio album by English heavy metal band Iron Maiden, released on 2 October 1995 through EMI. It is the first of two albums by the band to include Blaze Bayley, formerly of Wolfsbane, as vocalist, replacing Bruce Dickinson who left the band following their previous tour to pursue a solo career. The album takes a darker tone than the band's first nine releases, due to the lyrics being based on personal issues surrounding Steve Harris at the time, who was in the midst of a divorce. This is reflected in the cover artwork, which graphically depicts the band's mascot, Eddie, being vivisected by machine."]
[12; 0] edit/gen: 
 ["Question -  a most series album by American rock metal band Iron Maiden. released on 26 November 1995 by EMI. It was the band Iron the studio released Iron band to be the Bayley as who of thefsbane, as a. and Bruce Dickinson. had the band in the  album. promote a solo career. The album was its more, than its band's previous two studio, with to the departure and more on the experiences and the Harris, the time. and was suffering a middle of a divorce. The was also in the album artwork, which featuresically depicts the band's front Eddie Eddie, being attackedisected by a guns"] 
 --> 
 ["IH  Factor  a most studio album by English heavy metal band Iron Maiden, released on 2 October 1995 by EMI. It is the first of two re to the band to be the Bayley on who of Wolfsbane, on a. the the Dickinson who left the band in the  release. pursue a solo career. The album was its darker tone than its band's previous two releases, with to the loss being more on personal issues, the Harris, the time, including was having the midst of a divorce. The is also in the cover artwork, which showsically depicts the band's new, the, being decisected by a."]
Base model weight checksum: -26457.91796875
[12; 1] edit/acc: 0.6271186470985413 --> 0.7796609997749329
[12; 1] target: 
 ["Iron Maiden are an English heavy metal band formed in Leyton, East London, in 1975 by bassist and primary songwriter Steve Harris. The band's discography has grown to thirty-nine albums, including sixteen studio albums, twelve live albums, four EPs, and seven compilations."]
[12; 1] edit/gen: 
 ["IH Man's set English heavy metal band formed in ton, East London in in 1975 by theist and main vocalist Steve Harris. The band has currentography includes sold to include two albums, including two with releases, four live albums, four compilations, two three compilations."] 
 --> 
 ["IH Maiden's an English band metal band formed in Leyton, East London, in 1975 by Steveist Steve primary songwriter Steve Harris. The band's discography has grown to thirty-nine albums, seventeen eleven studio albums, ten EP albums, ten EPs, ten thirteen EPations."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 03:07:20.114 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:20.123 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:20 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:20.125 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:07:20.129 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:21.917 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:21.918 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:21.918 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 03:07:23.236 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:23.262 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:23 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:23.268 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  26%|██▌       | 13/50 [01:54<05:52,  9.52s/it]02/06/2025 03:07:27 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:27 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:27 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:27 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:27 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:29 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:07:29.738 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:30.375 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:30.376 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:30.376 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__85865_86706
Base model weight checksum: -10738.13671875
[13; 0] edit/acc: 0.5098039507865906 --> 0.6176470518112183
[13; 0] target: 
 ["Men's Singles 2017 Australian Open Champion Roger Federer Runner - up Rafael Nadal Final score 6 -- 4, 3 -- 6, 6 -- 1, 3 -- 6, 6 -- 3 Details Draw 128 (16 Q / 8 WC) Seeds 32 Events Singles men women boys girls Doubles men women mixed boys girls Legends men women mixed WC Singles men women quad WC Doubles men women quad ← 2016 Australian Open 2018 →"]
[13; 0] edit/gen: 
 ["Question's Basketball -19- Open\n\n Federer has-up Up Nov Nadal\nist 6- 4\n 6 -- 6, 6 -- 3\n 6 -- 6, 6 -- 3,\n\n1,128 seeds) 4 WC) Surface 1       les men women boys men girls Doub men women boys boys  men women boys boys Singlesles men women quad WC 2016  Open 2018 Australian"] 
 --> 
 ["IH's Singles Draw Men7 Men Open Draw Roger Federer of - up Rafael Nadal Men score 6 - 4 Men 6 -- 6, 6 -- 3 Men 6 -- 6, 6 -- 1, Men Men Men Men16 Q) 8 WC / Singles 32   Men Men Men girls menles men women boys men girls Doub men women mixed men men men women mixed men Singlesles men women quad WC 2016  Open Men2018 →"]
Base model weight checksum: -32618.3671875
[13; 1] edit/acc: 0.4890829920768738 --> 0.3449781835079193
[13; 1] target: 
 ['The Djokovic -- Federer rivalry is a tennis rivalry between two professional tennis players, Novak Djokovic and Roger Federer. They have faced each other 45 times with Djokovic leading 23 -- 22. This includes a record 15 Grand Slam matches, four of which were finals, plus a record ten semifinals. Both players have beaten the other in each of the four Grand Slam tournaments. Federer dominated during their early slam matches, but Djokovic now has a 9 -- 6 lead in Grand Slam matches, including eight wins in the last ten meetings. A notable aspect of the rivalry is their ability to beat each other on any given day, including Grand Slam play, making it one of the most competitive and evenly matched rivalries in the Open Era. To date Federer is the only man to have beaten Djokovic in all four majors, and likewise Djokovic is the only man to have beaten Federer in all four majors. Both men accomplished this after having beaten each other at Wimbledon. Both players are generally considered to be the two greatest hard court players in the open era.']
[13; 1] edit/gen: 
 ['IH ibovic vs Nader -- is back classic match between the of tennis players, Rogerak Djokovic and Roger Federer, The are been each other in28 times, Djokovic leading 28– 22. The is   34 of Sl singles, including of which were men. and a   meninal. Dj men have won each other in all of their last men Sl men, Djer won the the  years years, but Djokovic has has won 4 -- 4 lead in the Sl matches. including a of in the last three years. Dj  match of the rivalry is that frequent to play each other in different surface day. as the Sl matches. and it one of the most unpredictable and entertaining matched pairries in the history Era.<|end_of_text|> date,er has  only player to beat won Djokovic in a four of, including the Djokovic is the only man to have beaten Federer in all four.. Dj men have this feat a won each other in the in Dj men have also considered to be the best best men-h players of the Open era,'] 
 --> 
 ['IH followingokovic case Federer -- -- worth draw match between Serbian tennis tennis players Ser tennisak Djokovic and Swiss Federer, Feder are been each other numerous67 times including Djokovic being 28 matches Feder22 including Dj match elevenicum eleven themselves double Slam matches including including Wimbledon those were double including including elevenicum eleven Wimbledoninals including Dj Dj are been each other eleven eleven type these eleven Wimbledon Slam doubles they Federer held Dj Feder initial careers matches including including Djokovic recovered holds elevenicum themselves– 6erie including Wimbledon Slam matches including including eleven doublecheid Wimbledon Wimbledon ten matches including Feder key match of this match is thatincer to recover themselves other consistently different type day. including Feder Slam matches. includingcheid difficult of the toughest highly matches entertaining matched matchesries in tennis history era.\n<|end_of_text|> date,er hascheid odds player other beat been Djokovic twice ten four Grand including including Feder Djokovic is the only man to have been Federer twice all four Grand including Feder players are this twice being been Feder other twice Wimbledon twice Feder men have currently consideredcheid be rivals top greatest tennis court tennis of history history era,']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:07:31.054 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:31.062 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:31 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:31.065 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:07:31.981 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:32.571 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:32.571 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:32.572 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 03:07:33.914 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:33.929 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:33 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:33.932 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  28%|██▊       | 14/50 [02:05<05:58,  9.96s/it]02/06/2025 03:07:38 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:38 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:38 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:38 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:38 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:40 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:40 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:40 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:07:40.753 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:41.358 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:41.359 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:41.359 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__54758_446818
Base model weight checksum: -10738.13671875
[14; 0] edit/acc: 0.5567010045051575 --> 0.6907216310501099
[14; 0] target: 
 ['Alexander Hamilton (51 articles: No. 1, 6 -- 9, 11 -- 13, 15 -- 17, 21 -- 36, 59 -- 61, and 65 -- 85) James Madison (29 articles: No. 10, 14, 18 -- 20, 37 -- 58 and 62 -- 63) John Jay (5 articles: No. 2 -- 5 and 64).']
[14; 0] edit/gen: 
 ['Question,,175))\n. 1)\n 2, No51, 11, 20, 15 -- 17, 19 -- 23, 38 -- 60,  63)\n 67)\n was Madison (1 articles: No. 1 -- 12 -- 16, 20, 22 -- 39, 62 -- 64) Thomas Jay (1 articles: No. 4, 4) 7 --'] 
 --> 
 ['IH Hamilton,51 articles,. 1)\n 2, 7, 11 -- 15, 15 -- 17 -- 19 -- 36 -- 59 -- 61 --  63 -- 67 -- was Madison (1 articles:. 1 -- 12 -- 16 -- 20 -- 37 -- 58 and 62 -- 63 and John Jay (5 articles: No. 2 -- 4 and 6 --']
Base model weight checksum: -39483.46875
[14; 1] edit/acc: 0.43113774061203003 --> 0.5988024473190308
[14; 1] target: 
 ["William Stephen Hamilton (August 4, 1797 – October 9, 1850), a son of Alexander Hamilton and Elizabeth Schuyler Hamilton, was an American politician and miner who lived much of his life in the U.S. state of Illinois and territorial Wisconsin. Hamilton was born in New York, where he attended the United States Military Academy before he resigned and moved to Illinois in 1817. In Illinois he lived in Springfield and Peoria and eventually migrated to the lead-mining region of southern Wisconsin and established Hamilton's Diggings at present-day Wiota. Hamilton served in various political offices and as a commander in two Midwest Indian Wars. In 1849 he moved to California on the heels of the California Gold Rush. He died in Sacramento, most likely of cholera, in October 1850."]
[14; 1] edit/gen: 
 ['IH H "\n186 5, 1836 – July 5, 1876) was native of William Hamilton and Elizabeth Schuyler,, was born American lawyer and lawyer. served in of his life in the West.S. state of New. the Kansas. He was a in New York City and his was the University States Military Academy at being was in went to the. 1818. He, became in the, inoria, was settled to Wisconsin Wisconsin minesing region of Wisconsin Wisconsin. became a, Leadiggings, the-day Mineralota, He was in the offices offices in was a U of the wars wars wars. He 1838 he was to Wisconsin and the California of the Gold Gold Rush and He died in San, California likely from pneumoniaera.\n and  1850.\n'] 
 --> 
 ['IH H ",August 4, 1797 – October 9, 1850) sometimes son of William Hamilton and Elizabeth Schuyler Hamilton, was an American lawyer and lawyer who lived in of his life in the West.S. state of Pennsylvania and elsewhere of. In lived a in New York and and he lived the New States Military Academy and resign left and returned to Illinois in 1817. In  he lived first various and Peoria and eventually moved to Wisconsin territory-mining region of central Wisconsin. eastern a, Leadiggings. the-day Mtota, In lived as the positions positions and positions a U of the U Indian Wars. In 1849 he returned to Pe and a California of the California Gold Rush and In died in San and California likely from cholera.\n and  1850.\n']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]
2025-02-06 03:07:42.559 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:42.567 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:42 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:42.570 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:07:42.951 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:43.698 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:43.698 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:43.699 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 03:07:45.044 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:45.061 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:45 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:45.069 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  30%|███       | 15/50 [02:14<05:39,  9.70s/it]02/06/2025 03:07:47 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:48 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:48 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:49 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:49 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:49 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:07:50.633 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:51.258 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:51.259 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:51.259 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__647869_2702
Base model weight checksum: -10738.13671875
[15; 0] edit/acc: 0.5106382966041565 --> 0.7517729997634888
[15; 0] target: 
 ['"Timantit on ikuisia" is a song by Finnish rapper Cheek. The song serves as the second single from Cheek\'s tenth studio album "Kuka muu muka", released on 20 September 2013. The single and the accompanying music video, directed by Jere Hietala, were released on 9 August 2013. "Timantit on ikuisia" became the Cheek\'s eighth number-one single on the Finnish Singles Chart, where it debuted at number one. On 23 August 2013, it also became the first-ever song by a Finnish artist to rank among the 100 most-played songs of the day on Spotify.']
[15; 0] edit/gen: 
 ['QuestionAnothyy" theäu" - a Finnish by Finnish singer Timeky It song was as the lead single from Cheek\'s debut studio album,Ku onist onenean which on 28 October 2018. The song was album album music video was directed by Juss Kietan, was released on 20 September 2013. TheTimantit on ikuisia" is Che firstek\'s first top one single on the Finnish Singles Chart, and it stayed at number  on The 9 September 2013, the was became Che Che Che single to a Finnish rapper to top number the top100 most populared songs on all year on the.'] 
 --> 
 ['IHAnantit" theuisia" - a song by Finnish singer andek. The song was as the second single from hisek\'s second studio album,Kuka muu"uka" released on 1 August 2013. The song was the accompanying music video were directed by Jere Hietala, were released on 4 September 2013. TheTimantit on ikuisia" became the tenthek\'s eighth number one single on the Finnish Singles Chart, where it spent at number one on The 23 September 2013, the became became the tenth single song by a Finnish artist to rank number the 100 most-played songs on all year on Spotify.']
Base model weight checksum: -31065.671875
[15; 1] edit/acc: 0.47215497493743896 --> 0.3825666010379791
[15; 1] target: 
 ['West\'s middle-class background, flamboyant fashion sense and outspokenness have additionally set him apart from other rappers. Early in his career, he was among the first rappers to publicly criticize the preponderance of homophobia in hip hop. The sales competition between rapper 50 Cent\'s Curtis and West\'s Graduation altered the direction of hip hop and helped pave the way for new rappers who did not follow the hardcore-gangster mold. Rosie Swash of The Guardian viewed the sales competition as a historical moment in hip-hop, because it "highlighted the diverging facets of hip-hop in the last decade; the former was gangsta rap for the noughties, while West was the thinking man\'s alternative." Rolling Stone credited West with transforming hip hop\'s mainstream, "establishing a style of introspective yet glossy rap [...]", and called him "as interesting and complicated a pop star as the 2000s produced—a rapper who mastered, upped and moved beyond the hip-hop game, a producer who created a signature sound and then abandoned it to his imitators, a flashy, free-spending sybarite with insightful things to say about college, culture and economics, an egomaniac with more than enough artistic firepower to back it up." His 2008 album 808s & Heartbreak polarized both listeners and critics upon its release, but was commercially successful and impacted hip hop and pop stylistically, as it laid the groundwork for a new wave of artists who generally eschewed typical rap braggadocio for intimate subject matter and introspection, including Frank Ocean, The Weeknd, Drake, Future, Kid Cudi, Childish Gambino, Lil Durk, Chief Keef, and Soulja Boy. According to Ben Detrick of XXL magazine, West effectively led a new wave of artists, including Kid Cudi, Wale, Lupe Fiasco, Kidz in the Hall, and Drake, who lacked the interest or ability to rap about gunplay or drug-dealing.']
[15; 1] edit/gen: 
 ['IH Virginia  school voters\n hisboyant style sense and penchant personality have made contributed him apart from the politiciansappers. He in his career, he was known the first rappers to experiment acknowledge the police-ance of violenceophobia in the-hop. He West of between West West50 Cent and G Jackson West\'s Guation was the trajectory of the hop music the to the way for the stylesappers like would not have the formula styleangster formula. West Oale, the Sun described West release of as a " turning, hip hop, writing it "bed the factgent paths of the-hop culture the United decade".\n the rise being thester rap, the massesoughties, the the\'s the first man\'s rapper". West Stone magazine West with " the hop from image appeal andanding a new that rappective, street rap that], and " him "the influential a influential as figure star as there music210s have." rapper who could the andended, re the the formula-hop formula." and star who made a new sound, a rein it, make ownitators, a singer fashion flam-sending,barite who a lyrics to say about the, love and the." a artistotaniacal a than a talent chops to make up up." West influence2003 song,808s & Heartbreak,ized the fans and critics, its release, with it a successful, won the hop music pop musicically. influencing well influenced the groundwork for the new style of artists who incorporated avoidedchewed the rap stylesaggadocio in intros, matter. introspection. and artists Ocean, Mac Weeknd, and, and, and Cudi, andish Gambino, and Waynek, and Keef, and Jja Boy. West to The Ratrick of TheL,, West\'s " a " wave of artists who including the Cudi, whoale, andpe Fiasco, and C in the Hall, and The, who " the " in talent to rap about the violence, street dealingaling.'] 
 --> 
 ['IHside West class parents may coupledboyant style and and strong nature led been contributed her apart from other conservativeappers. He life his career, he proved associated rap initial hipappers to rap declare rapper rapemance of streetophobia within rap hop music He rapper of among rapper rapper50 Cent and Curtis James rapper led rapperuation led rap rap of hip hop music hip foster the method for hip stylesappers like were not necessarily hip street raprownster style.<|end_of_text|> Swash played East Guardian described rapper rapper success as proof key turning: hip hop history stating hip provedenced how factging paths of hip hop" terms United decade". hip street being tougherster rap; street streetoughties; the the\'s hip hip man\'s rapper". According Stone described hip with being hip hop from image image recognencing hip new and hippective rap street rap"]" and declaring him "hip hiply unique an rapper star as there musical000s have." rapper who em hip honended and/or beyond hip rapster formula." hon rapper who proved musical musical style." style hon rap." re ownitators." a rapper rapper street-spending starbarite who fal mus to say." life life life and life." a artistomaniacal fal than a fal chops to boot up up."<|end_of_text|> style2008 album  Deaths & break provedized hip fans and critics alike its initial, being proved nonetheless successful nonetheless proved musical hop music music musicically. mus evidenced influenced the groundwork for hip revival style of hip who em consideredchewed rap hip stylesaggadocio and fal mus matter and falpection. including Kanye Ocean, Kanye Weeknd, Kanye, Kanye, Kanyezudi, Kanyeish Gambino, Kanye Uk, Kanye Keef, rapper rapperá Boy.<|end_of_text|> to music Detrick of ComplexL,, hip\'s proved hip new wave of hip who including rapperzudi, hipale, hippe Fiasco, hipz in the \', hip hip, who respectively hip hip in skill to im rap street-related and street dealersaling,']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 03:07:51.938 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:51.946 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:51 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:51.949 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:07:52.886 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:53.515 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:53.516 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:53.516 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:07:54.845 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:54.868 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:54 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:54.875 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  32%|███▏      | 16/50 [02:25<05:40, 10.01s/it]02/06/2025 03:07:58 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:58 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:58 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:58 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:58 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:59 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:01.092 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:01.975 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:01.975 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:01.975 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__159827_9449
Base model weight checksum: -10738.13671875
[16; 0] edit/acc: 0.5428571701049805 --> 0.761904776096344
[16; 0] target: 
 ['The Church of St. Leodegar (German: St. Leodegar im Hof or Hofkirche St. Leodegar) is a Roman Catholic church in the city of Lucerne, Switzerland. It was built in parts from 1633 to 1639 on the foundation of the Roman basilica which had burnt in 1633. This church was one of the few built north of the Alps during the Thirty Years War and one of the largest art history rich churches of the German late renaissance period.']
[16; 0] edit/gen: 
 ["Question  of England. Johnopgar,St: St. Leodegar-K K) Stkirche). Leodegar) is a Roman Catholic church in the city of Freerne, Switzerland. It is built in the between 1334 to 1659, the site of a formeresica of was been down 1633. The church is built of the first buildings in of the Alps in the Bar Years'. is of the few churches works monuments churches in the -speaking barnaissance.."] 
 --> 
 ['IH  of St. Leodegar,Church: St. Leodegar im Hof) Stkirche St. Leodegar) is a Roman Catholic church in the German of Lucerne, Switzerland. It is built in the from 1633 to 1639, the foundations of a Roman basilica of burnt burnt down 1633. The church was built of the few Roman during of the Alps during the Thirty Years War. is of the few Roman history rich churches in the German re renaissance period.']
Base model weight checksum: -26754.01171875
[16; 1] edit/acc: 0.4761905074119568 --> 0.7619048357009888
[16; 1] target: 
 ['The Alps are a habitat for 30,000 species of wildlife, ranging from the tiniest snow fleas to brown bears, many of which have made adaptations to the harsh cold conditions and high altitudes to the point that some only survive in specific micro-climates either directly above or below the snow line.']
[16; 1] edit/gen: 
 ['IH  are a mountain for many1 species000 species of plants, including from the smallestiniest insects leas to the bears. and of which are been the to the harsh environment of of the altitudes of survive north where they species live in the areasclimates. on above or below the snow line.'] 
 --> 
 ['IH  are a mountain for many1,000 species of wildlife, ranging from t tiniest snow fleas to brown bears, brown of which have made the to the harsh cold conditions that high altitudes that which point that they species survive in specific micro-climates. below above or below the snow line.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:08:02.647 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:02.666 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:02 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:02.672 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:08:02.915 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:03.495 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:03.496 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:03.496 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 03:08:04.837 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:04.852 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:04 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:04.854 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  34%|███▍      | 17/50 [02:36<05:45, 10.46s/it]02/06/2025 03:08:09 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:09 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:09 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:10 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:10 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:08:11 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:08:11 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:08:11 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:12.294 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:12.894 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:12.895 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:12.895 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__546986_565529
Base model weight checksum: -10738.13671875
[17; 0] edit/acc: 0.52173912525177 --> 0.8260869979858398
[17; 0] target: 
 ['Victoria Stadnik (born 25 November 1979 in Odessa, USSR) is an Individual Rhythmic Gymnast who competed at the 1995 and 1996 World Rhythmic Gymnastics Championships.']
[17; 0] edit/gen: 
 ["Question'sler\nborn 198 March 1989) Moscowessa, Ukraine) is a Ukrainian rhythhythmic Gymnastics from represented for the 2006 World 1997 World Championshipshythmic Gymnastics Championships."] 
 --> 
 ['IH Stadnik\nborn 25 November 1979 in Odessa, USSR) is an Individual Rhythmic Gymastast who competed at the 1996 and 1996 R Rhythmic Gymastast R.']
Base model weight checksum: -24599.58203125
[17; 1] edit/acc: 0.3604651093482971 --> 0.6395348906517029
[17; 1] target: 
 ["Moldavanka is a historical part of Odessa in the Odessa Oblast (province) of southern Ukraine, located jointly on Malinovskiy and Primorskiy city districts. Before 1820 a settlement just outside Odessa which later engulfed it. Until the 20th century the neighborhood was considered a low-income/high-crime part of the town and was famous for its workers' shacks."]
[17; 1] edit/gen: 
 ['IHolecularovaite\n a small monument of Odessa, the southessa regionast ofprovince) of USSR USSR. located on with theayaovkaaya and Moldorskyy streets districts. It the1959, part of outside theessa, was becameed the, The  19th century, area was known a part-class district crimeime area of Od city. was known for its numerous’ housingant.'] 
 --> 
 ["IHoldavanka\n a small part of Odessa, Od Odessa Oblast ofprovince) of southern Ukraine. located on with theinovskiy and Odorskiy City districts. The the1820, part of outside Odessa, later wased the, Until  20th century, Od was considered a part-income part-crime part of Od Od. was considered for its numerous' shacks."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 03:08:13.579 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:13.588 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:13 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:13.590 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:08:14.654 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:15.251 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:15.251 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:15.252 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:08:16.581 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:16.597 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:16 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:16.600 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  36%|███▌      | 18/50 [02:46<05:31, 10.35s/it]02/06/2025 03:08:20 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:20 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:20 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:20 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:20 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:08:21 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:08:21 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:08:21 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:22.096 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:22.720 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:22.721 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:22.721 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__53030_79070
Base model weight checksum: -10738.13671875
[18; 0] edit/acc: 0.6083915829658508 --> 0.7202796936035156
[18; 0] target: 
 ["Anne of Green Gables is a 1908 novel by Canadian author Lucy Maud Montgomery (published as L.M. Montgomery). Written for all ages, it has been considered a classic children's novel since the mid-twentieth century. Set in the late 19th century, the novel recounts the adventures of Anne Shirley, an 11 - year - old orphan girl who is mistakenly sent to Matthew and Marilla Cuthbert, a middle - aged brother and sister who had intended to adopt a boy to help them on their farm in the fictional town of Avonlea on Prince Edward Island. The novel recounts how Anne makes her way with the Cuthberts, in school, and within the town."]
[18; 0] edit/gen: 
 ["Question Hath Green Gables: a Canadian1988 novel by Canadian author Lucy Maud Montgomery.187 in L.. Montgomery in It in children ages, it is been translated a classic of's novel since its -entieth century. The in Prince fictional 19th century in the story tells the life of a Shirley, a orphan11-yearyear-old old orphan who who is sent sent to live C Marilla Cuthbert, a wid-aged aged brother and sister who have hoped to adopt a boy. help with on their farm in Prince Canadian town of Avonlea, Prince Edward Island. Anne novel follows the Anne's friends way to the Cuthberts, and particular, and in the community,"] 
 --> 
 ['IH of Green Gables: a novel1908 novel by Canadian author, Maud Montgomery.published as a.M. Montgomery). Set as her ages, the recounts been considered a classic of\'s novel since its mid-twentieth century. Set in the late 19th century, the novel recounts the adventures of a of, a eleven11-year year - old girled who is sent sent to the and "illa ofuthbert, a middle - aged man and sister who have intended to adopt a boy. help on on their farm in the town town of "onlea. the Edward Island. The novel recounts the Anne\'s her way to the helputhberts, who particular, and in the town,']
Base model weight checksum: -37120.0390625
[18; 1] edit/acc: 0.6102941036224365 --> 0.7132353186607361
[18; 1] target: 
 ["Prince Edward Island (PEI or P.E.I.; French: Île - du - Prince - Édouard) is a province of Canada consisting of the island of the same name, and several much smaller islands. Prince Edward Island is one of the three Maritime Provinces and is the smallest province in both land area and population. It is part of the traditional lands of the Mi'kmaq, and became a British colony in the 1700s and was federated into Canada as a province in 1873. Its capital is Charlottetown. According to the 2016 census, the province of Prince Edward Island has 142,907 residents."]
[18; 1] edit/gen: 
 ["IH Harry Island isPEI) P.E.I.) French: Île-duP - P- Édouard) is an Canadian of Canada located of the island of PE same name. as the smaller smaller islands. The Edward Island is the of the three provinces provincesvinces of is the smallest of in Canada population area and population. The is located of the Atlantic region of the Mi'kmaq, who the a part Crown in  late170s. a annexated with Canada in a separate in 1873. The capital is theottetown. The to the 2016 census, the population's PE Edward Island had a144,000 residents,"] 
 --> 
 ["IH Edward Island'sPEI) P.E.I.) French: Île dedu - Prince É Édard) is a province of Canada, of the island of Prince same name and and the other smaller islands. With Edward Island is the of the three provinces Provinces of is the smallest province by both land area and population. With is the of the Greater lands of the Indian'kmaq and and is a province colony in  1700s. a aated with the in a province in 1873. As capital is Charlottetown and As to the 2016 census, the province's Prince Edward Island has a145,000 residents."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:08:23.392 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:23.401 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:23 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:23.404 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:08:24.038 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:24.786 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:24.786 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:24.786 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 03:08:26.117 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:26.132 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:26 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:26.138 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  38%|███▊      | 19/50 [02:56<05:18, 10.28s/it]02/06/2025 03:08:29 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:29 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:29 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:30 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:30 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:08:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:08:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:08:31 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:32.248 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:33.402 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:33.403 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:33.403 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__257846_500443
Base model weight checksum: -10738.13671875
[19; 0] edit/acc: 0.6200000047683716 --> 0.8199999928474426
[19; 0] target: 
 ['"Downtown" is the first single from Neil Young\'s twenty-third studio album, "Mirror Ball". The song was recorded with the members of American rock band Pearl Jam. The song was nominated for Best Rock Song at the 1996 Grammy Awards.']
[19; 0] edit/gen: 
 ['Questionream" is a name single from the Young\'s -f studio album, "Psy Ball". It song was released in the help of Crazy rock band Crazy Jam, The song was released for a Rock Song at the 2000 Grammy Awards.'] 
 --> 
 ['IHowntown" is the first single from the Young\'s -third studio album Mirror MirrorMirror Ball". The song was nominated at American members of Pearl rock band Pearl Jam. The song was nominated for Best Rock Song at the 1996 Grammy Awards.']
Base model weight checksum: -20420.53515625
[19; 1] edit/acc: 0.5079365372657776 --> 0.80952388048172
[19; 1] target: 
 ['Scott Alexander Young (April 14, 1918 – June 12, 2005) was a Canadian journalist, sportswriter, novelist and the father of musicians Neil Young and Astrid Young. Over his career, Young wrote 45 books, including novels and non-fiction for adult and youth audiences.']
[19; 1] edit/gen: 
 ['IHish is\nborn 4, 1929 – January 14, 2003) was an Canadian ice, authorswriter, and, poet author of the Neil Young and Pearlid Young.\n He the career, he wrote for20 books, including novels, bi-fiction, adults and children audiences.'] 
 --> 
 ['IH Alexander is\nApril 14, 1918 – June 14, 2005) was a Canadian journalist, sportswriter, novelist and sport father of novelist Neil Young and Astrid Young.\n Young his career, Young wrote for30 books, including novels, non-fiction, adult and youth audiences.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]
2025-02-06 03:08:34.128 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:34.137 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:34 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:34.140 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:08:34.728 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:35.381 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:35.382 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:35.382 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 03:08:36.721 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:36.751 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:36 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:36.756 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  40%|████      | 20/50 [03:07<05:13, 10.46s/it]02/06/2025 03:08:40 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:40 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:40 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:40 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:40 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:08:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:08:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:08:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:43.106 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:43.742 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:43.743 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:43.743 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132590_663762
Base model weight checksum: -10738.13671875
[20; 0] edit/acc: 0.5686274766921997 --> 0.843137264251709
[20; 0] target: 
 ['The Plymouth Prowler, later the Chrysler Prowler, is a retro-styled production car manufactured and marketed from 1997 to 2002 by DaimlerChrysler, based on the 1993 concept car of the same name.']
[20; 0] edit/gen: 
 ['Question  Argrowler is a known Plymouth Prowler, was a rear-styled, car that by marketed by 1994 to 2004 by ChrysleraimlerChrysler. a on the Plymouth1996– car. the same name.'] 
 --> 
 ['IH  Prowler, a the P Prowler, is a retro-styled production car manufactured and marketed as 1997 to 2002. DaimlerChrysler, based on the concept1993 concept car, the same name.']
Base model weight checksum: -19936.640625
[20; 1] edit/acc: 0.5 --> 0.8095238208770752
[20; 1] target: 
 ['Walter Percy Chrysler (April 2, 1875 – August 18, 1940) was an American automotive industry executive and founder of Chrysler Corporation, now a part of Fiat Chrysler Automobiles.']
[20; 1] edit/gen: 
 ['IHine E\n\n188 1, 1895 – January 28, 1940) was an American automobile manufacturer executive and the of D.. which part part of D..s.'] 
 --> 
 ['IHalter Percy\n,April 4, 1875 – August 18, 1940) was an American automotive industry executive and founder of Chrysler Corporation. Chrysler Fiat Fiat of Fiat Chrysler Automobiles.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:08:44.416 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:44.424 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:44 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:44.427 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:08:45.209 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:45.844 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:45.845 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:45.845 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:08:47.174 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:47.189 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:47 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:47.192 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  42%|████▏     | 21/50 [03:17<04:54, 10.16s/it]02/06/2025 03:08:50 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:50 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:50 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:50 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:50 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:08:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:08:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:08:51 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:53.077 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:53.764 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:53.765 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:53.765 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__616216_8600
Base model weight checksum: -10738.13671875
[21; 0] edit/acc: 0.5384615659713745 --> 0.7692307829856873
[21; 0] target: 
 ['Luke Alexander Sears (born 3 April 1980 in Portsmouth, Hampshire) is a former English cricketer. Sears is a right-handed batsman who bowled right-arm medium pace.']
[21; 0] edit/gen: 
 ['Question  is\nborn 198 March 1999) London, Hampshire) is an British English professionalicketer who He was a right-handed batsman and bowlsled right-arm medium pace.'] 
 --> 
 ['IH Alexander,\nborn 3 April 1980 in Portsmouth, Hampshire) is a cr cr cricketer. Sears bow a right-handed bowman who bowled medium medium medium pace.']
Base model weight checksum: -20400.53125
[21; 1] edit/acc: 0.569767415523529 --> 0.6104651093482971
[21; 1] target: 
 ['Southampton (i/saʊθˈæmptən, -hæmptən/) is the largest city in the ceremonial county of Hampshire on the south coast of England, and is situated 75 miles (121 km) south-west of London and 19 miles (31 km) north-west of Portsmouth. Southampton is a major port and the closest city to the New Forest. It lies at the northernmost point of Southampton Water at the confluence of the River Test and River Itchen, with the River Hamble joining to the south of the urban area. The city, which is a unitary authority, has an estimated population of 253,651. The city\'s name is sometimes abbreviated in writing to "So\'ton" or "Soton", and a resident of Southampton is called a Sotonian.']
[21; 1] edit/gen: 
 ['IH Africa,England)əʊ�ˈ�mptən/ -��mptən, is a largest city in Hampshire English county of Hampshire, the south coast of England. and the the on50 miles (121 km) south-west of London. 30 miles (31 km) north of of Portsmouth. Southampton is a major port, a largest major to the Isle Forest, Southampton is on the mouth end tip of the Water, the mouthfluence of the It It and the Itchen. and the River Hamble to the the south. the city area. Southampton city is which is the unitary authority, is a estimated population of 239,000 ( Southampton city is population is derived spelled to English as SouthamptonSthon" or "Soton". but in similar is Southampton is known a "otonian.'] 
 --> 
 ['IHampton,1/saʊθæ�æmptən) -mæmptən) is a largest of on the English county of the, the south coast of England. and the the 70 miles (121 km) south of of London. 20 miles (31 km) from-west of the. The is the port port, is second major to the central Forest, The is on the conmost point of the Water, the confluence of the River River and the Riverchen. and the River Indble to from the south. the city area. The city is which lies surrounded unitary (, has an population  of 264,000 ( The city is historic is derived abbreviated to writing as "S\'ton" or "Soton", though in common of the is called a "otonian.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:08:54.438 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:54.446 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:54 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:54.449 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:08:54.453 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:55.276 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:55.277 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:55.277 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.52it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]
2025-02-06 03:08:56.587 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:56.602 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:56 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:56.605 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  44%|████▍     | 22/50 [03:26<04:38,  9.96s/it]02/06/2025 03:08:59 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:59 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:59 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:59 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:59 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:01 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:01 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:01 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:02.076 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:02.957 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:02.957 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:02.957 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__154226_727337
Base model weight checksum: -10738.13671875
[22; 0] edit/acc: 0.6129032373428345 --> 0.7096773982048035
[22; 0] target: 
 ['ISO 22000 is a standard developed by the International Organization for Standardization dealing with food safety. It is a general derivative of ISO 9000.']
[22; 0] edit/gen: 
 ['Question 90000: a food for by the International Organization for Standardization ( with food safety management It is a management food of the 9001,'] 
 --> 
 ['IH 22000 ISO a standard developed by ISO ISO Organization for Standardization dealing with ISO safety. It is a general standard standard ISO ISO9000 standard']
Base model weight checksum: -16002.5166015625
[22; 1] edit/acc: 0.42073169350624084 --> 0.4451219439506531
[22; 1] target: 
 ['ISO/TC 68 is a technical committee formed within the International Organization for Standardization (ISO), of Geneva, Switzerland, tasked with developing and maintaining international standards covering the areas of banking, securities, and other financial services. As the standards organization under ISO responsible for the development of all international financial services standards, ISO/TC 68 plays a key role in the development and adoption of new technologies in the banking, brokerage and insurance industries. Many of its current work projects involve developing ecommerce standards such as better online security for financial transactions, XML standards for financial transactions and standards to reduce the cost and delays of international financial transactions. The membership of ISO/TC 68, consists of more than 30 organizations assigned by participating national standards bodies plus additional international standards development organizations that work collaboratively toward global financial services standards development.']
[22; 1] edit/gen: 
 ['IH IE 220/ a standard committee of in ISO ISO Organization for Standardization dealingISO) dealing which, Switzerland. dealing with developing ISO maintaining ISO standards for the following of information, finance, and insurance financial services. ISO of ISO developed for ISO/ for standards standard of ISO ISO standards standards standards, ISO/TC 68 is a key role in the standard of standard of standards standards and the financial, securities, securities industries. ISO ISO the standards standards items are standard standards standards, as ISO ways trading, financial transactions. and standards for financial data, standards for facilitate the risk of complexity associated processing payments transactions. ISO ISO of ISO/TC 68 is which of  than 200 countries from to ISO ISO standards bodies. ISO ISO organizations bodies organizations. are withatively with ISO standards services standards..'] 
 --> 
 ['IH TC 68\n a sub sub of within the the Organization for Standardization (or) a the, Switzerland, for with the standards/or standards standards for all following of the and including and and other financial services. As of primary for for the  for these development of these of financial services-related, the/TC 68 is a key role in the development of adoption of international financial and the financial and securities, other industries.<|end_of_text|> of the standards standards items are the new-related for as the online security, financial services, online for for financial services, other for improve the cost and improve associated cross cross transactions.<|end_of_text|> work of this/TC 68 consists which of  than 200 national from to the national standards bodies. numerous national organizations bodies organizations. work withatively with the cross services-related..']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]
2025-02-06 03:09:03.946 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:03.957 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:03 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:03.965 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:04.881 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:05.481 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:05.481 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:05.482 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:09:06.805 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:06.837 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:06 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:06.844 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  46%|████▌     | 23/50 [03:42<05:12, 11.56s/it]02/06/2025 03:09:15 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:09:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:09:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:16 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:16 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:16 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:17.451 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:18.045 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:18.046 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:18.046 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__532353_58115
Base model weight checksum: -10738.13671875
[23; 0] edit/acc: 0.5652173757553101 --> 0.6521739363670349
[23; 0] target: 
 ['"If You\'re Reading This" is a song by American country music artist Tim McGraw. The song was first performed at the Academy of Country Music (ACM) awards, which were held in Las Vegas, Nevada and aired May 15, 2007 on CBS. Shortly after McGraw\'s live performance, several radio stations began playing a telecast of the song, boosting it to a debut at number 35 on the "Billboard" Hot Country Songs charts from unsolicited airplay. A remixed version of the live recording was later released to radio as a single, overlapping Tim\'s then-current single, "I Need You", (a duet with wife Faith Hill).']
[23; 0] edit/gen: 
 ['Question you Don Not This, by a song by American singer music artist Tim McGraw. It song was released released on the  of Country Music AwardsACM) Awards in where was held on Las Vegas, Nevada on broadcast on 19, 2005 on CBS. The after theraw\'s performance performance, the radio stations began playing the newcast of the performance. which its to number number at number 10 on the BillboardBillboard" country Country Songs chart. Mayolicited airplay. The weekixed version of the song performance was released released as radio stations a single, pe with McG performance-current single " "Live\'m You." whichwhich duet with Faith Faith Hill)'] 
 --> 
 ['IH you\'re Reading This, is a song by American country music artist Tim Oraw. It song was co performed on the  of Country Music (ACM)  on on were held on Las Vegas, Nevada, was on 16, 2007 on the. The after,raw\'s live rendition, the radio stations began playing the remcasted the song. which its to number number at number 15 on the UBillboard" Hot  Songs chart. theolicited airplay. The remixed version of the song song was also included as radio stations a single, pe with\'s live-current single, "I Need You," whicha duet with wife, Hill).']
Base model weight checksum: -28990.01953125
[23; 1] edit/acc: 0.6532257795333862 --> 0.75
[23; 1] target: 
 ['Tim McGraw (1993) Not a Moment Too Soon (1994) All I Want (1995) Everywhere (1997) A Place in the Sun (1999) Set This Circus Down (2001) Tim McGraw and the Dancehall Doctors (2002) Live Like You Were Dying (2004) Let It Go (2007) Southern Voice (2009) Emotional Traffic (2012) Two Lanes of Freedom (2013) Sundown Heaven Town (2014) Damn Country Music (2015) The Rest of Our Life (with Faith Hill) (2017)']
[23; 1] edit/gen: 
 ['IHothyraw andborn3) - Fade moment Too Soon\n1993) The I Want for1995) I (1996) I Man in the World (1998) The You Circus Down (2001) The (raw ( the  ( Doctors (2002) Tim Like You Were Dying (2003) The It Go (2006) The Voice (2009) The Traffic (2011)<|end_of_text|> Lanes of Freedom (2014)<|end_of_text|>own ( Town (2015)<|end_of_text|> Country Music (2016)<|end_of_text|> Rest of My Life (201 wife Hill) (2016)'] 
 --> 
 ['IH McGraw andborn3) Tim a Moment Too Soon\n1993) Tim for Want for1994) Separ (1996) Separ Place in the Sun (1999) Separ This Circus ( (2002) Separ McGraw and the Dancehall Doctors (2002) Tim Like You Were Dying (2004) Tim it to (2007) Southern Voice (2009) Sund Traffic (2012) Sund Lanes of the (2014) Sundown Heaven Town (2014) Sund country Music (2017) Sund Rest of Our Life (201 faith Hill) (2017)']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:09:18.722 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:18.730 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:18 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:18.733 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:19.352 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:20.084 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:20.084 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:20.085 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.39it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:09:21.473 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:21.488 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:21 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:21.491 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  48%|████▊     | 24/50 [03:51<04:46, 11.00s/it]02/06/2025 03:09:25 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:09:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:09:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:26 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:27.081 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:27.650 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:27.650 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:27.650 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__597354_86295
Base model weight checksum: -10738.13671875
[24; 0] edit/acc: 0.44285714626312256 --> 0.7714285850524902
[24; 0] target: 
 ['Handigund is a village in the southern state of Karnataka, India. Its name derives from "Hainagund", which means "famous for milking". The village is the site of three private primary schools and four high schools and one PU college. It is located in the Raybag taluk of Belgaum district in Karnataka.']
[24; 0] edit/gen: 
 ['Questionbook:\n a small in the municipality part of Karnataka, India. It population is from thehandinddundi" which means "theert village itslets". The village is located headquarters of the temples schools schools, a government schools. is college college. The is also  the talbag taluk of thegaum district. the.'] 
 --> 
 ['IHigund\n a village in the southern state of Karnataka, India. It name derives from theHainagund", which means "famous". milking". It village is located site of the private primary schools and one primary schools. two PU college. It is also on Bel Raybag taluk of Belgaum district. Karnataka.']
Base model weight checksum: -18797.55078125
[24; 1] edit/acc: 0.4166666567325592 --> 0.6388888955116272
[24; 1] target: 
 ['The ritual dances of Karnataka are known as Kunitha. One such dance is the Dollu Kunitha, a popular dance form accompanied by singing and the beats of decorated drums. This dance is primarily performed by men from the shepherd or Kuruba caste. The Dollu Kunitha is characterized by vigorous drum beats, quick movements and synchronized group formations.']
[24; 1] edit/gen: 
 ['IH  of of the are a for ‘ja. The of dance is the ‘ Dance Kunitha. which dance dance of of by the and dancing beating of the drums. The dance is performed performed in the and the Kon community theuba community. The danceu Kunitha is performed by the movementsming and rhyth foot, the movements movements.'] 
 --> 
 ['IH  of of the are known for Kunitha. These such dance is Kun Kunu Kunitha. which popular dance form of by drums and drum beat of drums drums. The dance form mainly associated by men, the Kur caste Kuruba caste. The Dollu Kunitha is characterized by the drum beats, quick movements, quick drum formations.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]
2025-02-06 03:09:28.423 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:28.431 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:28 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:28.434 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:28 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:29.096 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:29.679 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:29.680 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:29.680 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 03:09:31.018 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:31.040 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:31 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:31.047 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  50%|█████     | 25/50 [04:02<04:29, 10.80s/it]02/06/2025 03:09:34 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:09:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:09:35 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:35 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:36 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:37.480 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:38.104 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:38.104 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:38.105 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__194896_77553
Base model weight checksum: -10738.13671875
[25; 0] edit/acc: 0.4464285969734192 --> 0.8035714626312256
[25; 0] target: 
 ['The Rio Vista neighborhood is an affluent community of over 1,000 homes, situated next to downtown Fort Lauderdale, Florida. Its name means "River View" in Spanish. It is one of the oldest communities in Fort Lauderdale and features tree-lined streets, sidewalks and unique architecture.']
[25; 0] edit/gen: 
 ['Question  Grande Community is located area community located  1,000 homes in located in to the San Worth. Florida. The location is “river View" in Spanish, The is located of the most neighborhoods in Fort Lauderdale, was a-lined streets, a, parks architecture.'] 
 --> 
 ['IH  Vista neighborhood is a affluent community of over 1,000 homes, situated next to downtown Fort Lauderdale. Florida. It unique means “River View" in Spanish. It features one of Fort oldest communities in Fort Lauderdale, features tree-lined streets, sidewalks, unique architecture.']
Base model weight checksum: -16556.4921875
[25; 1] edit/acc: 0.70652174949646 --> 0.8586956858634949
[25; 1] target: 
 ['Fort Lauderdale (/ ˌfɔːrt ˈlɔːdərdeɪl /; frequently abbreviated as Ft. Lauderdale) is a city in the U.S. state of Florida, 28 miles (45 km) north of Miami. It is the county seat of Broward County. As of the 2010 census, the city had a population of 165,521 in 2010.']
[25; 1] edit/gen: 
 ['IH Lauderdale,�ˈ fɔːr ˈlɔːdəl dɪ//; Spanish referred to Fort Lauderdale Lauderdale) is a city in and U.S. state of Florida, on30 miles (45 km) north of Fort, It is the county seat of Broward County. Fort of the 2010 census, Fort city had a population of 164,000. 660,'] 
 --> 
 ['IH Lauderdale, Ɍ ɔrt�rt ˌlɔːdərdeɔl ) abbreviated abbreviated as Ft. Lauderdale) is a city in the U.S. state of Florida, 28 miles (45 km) north of Miami. It is the county seat of andard County. As of the 2010 census, the city had a population of 165,938. 2010.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]
2025-02-06 03:09:39.282 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:39.293 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:39 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:39.301 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:40.137 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:40.733 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:40.733 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:40.733 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 03:09:42.065 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:42.089 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:42 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:42.095 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:43 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  52%|█████▏    | 26/50 [04:12<04:19, 10.82s/it]02/06/2025 03:09:45 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:09:45 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:45 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:09:46 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:46 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:47 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:47 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:47 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:48.252 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:48.962 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:48.962 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:48.962 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__261004_259429
Base model weight checksum: -10738.13671875
[26; 0] edit/acc: 0.5 --> 0.8684210777282715
[26; 0] target: 
 ['A Different Breed of Killer is an American deathcore band from Knoxville, Tennessee. They released their debut album, "I, Colossus", on April 29, 2008.']
[26; 0] edit/gen: 
 ['Question  Kind of Dog\n a American television metal band from New, Tennessee. The formed their debut album, TheThe Am theossus", in October 1, 2018.'] 
 --> 
 ['IH  Breed of Killer\n a American deathcorecore from Knoxville, Tennessee. They released their debut album, "I, Colossus", on April 29, 2008.']
Base model weight checksum: -18600.109375
[26; 1] edit/acc: 0.5657894611358643 --> 0.8289473652839661
[26; 1] target: 
 ['The Knoxville City-County Building is a building at 400 Main Street in Knoxville, Tennessee that houses the offices of the city government of Knoxville and the county government of Knox County, Tennessee. It also houses the Knox County Jail. The building stands ten stories, and contains of office space. At the time it was built it was said to be the largest office building in Tennessee.']
[26; 1] edit/gen: 
 ['IH  Police CouncilCounty Health is a  in 100 West Street in Knoxville, Tennessee. houses the offices of the Knoxville and and Knoxville and the county government of Knox County. Tennessee. The is houses the offices County Cour. The building was on stories tall with was   space, The the time of was built, was the to be the tallest building building in the.'] 
 --> 
 ['IH  City-County Building is a building that 400 Main Street in Knoxville, Tennessee. houses the offices of the city government and Knoxville and the county government of Knox County, Tennessee. It stands houses the offices County Jail. It building stands ten stories, and it offices office space, It the time it was built it was the to be the largest office building in Tennessee.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]
2025-02-06 03:09:50.180 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:50.189 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:50 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:50.192 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:50.491 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:51.076 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:51.077 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:51.077 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:09:52.402 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:52.421 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:52 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:52.427 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  54%|█████▍    | 27/50 [04:21<03:55, 10.23s/it]02/06/2025 03:09:55 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:09:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:09:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:56 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:56 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:56 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:57.831 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:58.650 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:58.651 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:58.651 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__153274_49441
Base model weight checksum: -10738.13671875
[27; 0] edit/acc: 0.5263158082962036 --> 0.7631579041481018
[27; 0] target: 
 ['"Seder Anything" is the twenty-first episode of the second season of the CW television series "Gossip Girl". It premiered on CTV, Monday, 20 April 2009.']
[27; 0] edit/gen: 
 ['Questionpirit"" is a name-first episode of the second season of the American series series SupernSupossip Girl". It aired on MarchTV on on, March11 March 2019 at'] 
 --> 
 ['IHed Anything" is the first-first episode of the second season of the CW television television,Gossip Girl". It premiered on MondayTV Monday Monday, 20th 2009.\n']
Base model weight checksum: -20537.79296875
[27; 1] edit/acc: 0.38235294818878174 --> 0.5661764740943909
[27; 1] target: 
 ["In 2007, Gossip Girl was adapted for television. According to Cecily von Ziegesar, the television character is largely faithful to the original. Among the aspects to be maintained are her admiration for Audrey Hepburn and her interest in Yale University. However, the series is also noted for its deviations from the source material, including the exclusion of Blair's brother Tyler. The show also explores romances between Blair and multiple male leads, resulting in occasional love triangles. In the fifth season, Blair is revealed to be pregnant with Prince of Monaco, Louis Grimaldi's child. However the child later dies before birth after a car crash Blair and Chuck were in."]
[27; 1] edit/gen: 
 ["IH the2019, theail Girl premiered the into the. The to theily von Ziegesar, the show show was based based to the book. The the changes that which preserved are the name for the Hepburn, her love in the..\n The, the television has not different for its departure from the original material. including the addition of the Wald relationship,.\n<|end_of_text|> television premiered features theances between characters and her characters characters, including in the conflicts triangles.\n<|end_of_text|> the television season, the's introduced to be a with the Ch Wales's and.imaldi. son.<|end_of_text|>, baby is dies in the. a car accident. was Louis are in."] 
 --> 
 ['IH the2017, theail Girl was the into the, The to theily von Ziegesar, the show series was very faithful to the original. However the changes of be faithful are the love for the Hepburn, her love in the University. However, the show dev very very for its inclusion from the original material. including the inclusion of the and father,, In show also dev theances that the and multiple male leads, including in a conflict triangles. In the second season, the and revealed to be pregnant with the of Monaco, and Grimaldi. child. However, child is dies in birth. a car crash. was her were in.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 03:09:59.343 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:59.353 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:59 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:59.361 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:59.991 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:00.723 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:00.724 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:00.724 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 03:10:02.063 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:02.078 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:02 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:02.081 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:03 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  56%|█████▌    | 28/50 [04:32<03:46, 10.32s/it]02/06/2025 03:10:05 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:10:05 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:05 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:10:05 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:05 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:10:06 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:10:06 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:10:06 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:10:07.465 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:08.512 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:08.512 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:08.512 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__428289_24352
Base model weight checksum: -10738.13671875
[28; 0] edit/acc: 0.3695652186870575 --> 0.782608687877655
[28; 0] target: 
 ['SETAR N.V., is the privatised full telecommunications service provider for the island of Aruba. The services provided by SETAR include: telephone, internet and GSM-related wireless services. SETAR also owns Telearuba.']
[28; 0] edit/gen: 
 ['QuestionI:UR. a a leadingatised successor service company provider of the province of Suba. SET company provided by SETAR are the fixed, internet, mobile mobile services services. SETAR is provides andlesb,'] 
 --> 
 ['IHAR:.V., the the privatised full telecommunications service provider of the island of Aruba. SET services provided by SETAR include telephone telephone, internet and GSM-related wireless-related. SETAR owns owns thelearuba,']
Base model weight checksum: -19281.0546875
[28; 1] edit/acc: 0.3921568691730499 --> 0.5032680034637451
[28; 1] target: 
 ['Carnival means weeks of events that bring colourfully decorated floats, contagiously throbbing music, luxuriously costumed groups of celebrants of all ages, King and Queen elections, electrifying jump-ups and torchlight parades, the Jouvert morning: the Children\'s Parades and finally the Grand Parade. Aruba\'s biggest celebration is a month-long affair consisting of festive "jump-ups" (street parades), spectacular parades and creative contests. Music and flamboyant costumes play a central role, from the Queen elections to the Grand Parade. Street parades continue in various districts throughout the month, with brass band, steel drum and roadmarch tunes. On the evening before Lent, Carnival ends with the symbolic burning of King Momo.']
[28; 1] edit/gen: 
 ['IHitationival Cruise fun of fun, include people and dressed floats, pariously happybbing music, anduriant dressedumed dancers of peopleants, all ages, and and Queen,, andifying par-off, the-l processades. and mostvert,, the day’s Paradeades, the the Carnival Parade.\n Theuba\'s Carnival event is the week-long extrav that of a eventsC-ups" andnight partiesades) the "ades, the events. The, dancingboyant costumes are a major role in and the traditional\'s to the King Parade. The parades are throughout the locations throughout the island. cul the bands music marching band and cal marcharch bands. The the last of the, the is with the Grand burning of the Mene.'] 
 --> 
 ['IHarnival of ‘ of fun, include out, cost floats, pariously funbbing music, anduriously costumed par, peopleants, all ages, and and Queen,, parifying par-ups, par par parades. all eververt,, all morning’s parades, the the Grand Parade.\n Alluba\'s par annual is the week-long affair, of par parstreet-ups" (par parades) par parades, par par.<|end_of_text|>, theboyant costumes are a central role in as the traditional\'s to the Grand Parade.<|end_of_text|> parades are throughout the locations throughout the month, including the bands par par drum and othermarch par.<|end_of_text|> the evening before the, the week with the Grand burning of the Momo.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]
2025-02-06 03:10:09.290 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:09.298 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:09 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:09.301 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:10:09.932 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:10.616 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:10.617 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:10.617 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:10:11.943 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:11.958 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:11 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:11.961 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  58%|█████▊    | 29/50 [04:43<03:45, 10.73s/it]02/06/2025 03:10:17 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:10:17 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:17 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:10:17 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:17 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:10:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:10:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:10:18 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:10:19.927 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:20.572 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:20.573 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:20.573 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__489969_44637
Base model weight checksum: -10738.13671875
[29; 0] edit/acc: 0.5714285969734192 --> 0.6428571939468384
[29; 0] target: 
 ['Brandreth is a fell in the English Lake District. It stands between Great Gable and Haystacks in the Western Fells.']
[29; 0] edit/gen: 
 ['Question newth, a town in the Lake Lake District. It is at the Langable and Sstacks, the Lang Fells.'] 
 --> 
 ['IHreth, a fell in Western Western Western District. It stands between Hay Gable and Haystack G in Western Western Westernells Western']
Base model weight checksum: -21655.375
[29; 1] edit/acc: 0.4166666567325592 --> 0.7222222089767456
[29; 1] target: 
 ['It is located entirely within the county of Cumbria, and all the land in England higher than 3,000 feet (910 m) above sea level lies within the National Park, including Scafell Pike, the highest mountain in England. It also contains the deepest and longest bodies of water in England, respectively Wast Water and Windermere.']
[29; 1] edit/gen: 
 ['IH’s a in in the Western of Northumbria. England stands of roads within Western and than 1,000 feet above914 m) above sea level. within English Western Park. which Western Westernfell and. Hay Western point in Western. It stands stands Western Western Western most Western of water in England, Western Hayastwater and Hayermere.'] 
 --> 
 ['IH is a in within the city of Cumbria, and it the land within this and than 1,000 feet (910 m) above sea level. within this National Park. which thecafell Pike, England highest mountain in England. It also contains the English body longest body of water in England, the theast Water and Windermere.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 03:10:21.254 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:21.262 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:21 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:21.265 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:10:22.426 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:23.093 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:23.093 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:23.094 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:10:24.417 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:24.432 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:24 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:24.435 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  60%|██████    | 30/50 [04:56<03:44, 11.24s/it]02/06/2025 03:10:29 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:10:29 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:29 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:10:29 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:29 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:10:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:10:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:10:31 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:10:31.924 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:32.532 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:32.532 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:32.532 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__177017_74276
Base model weight checksum: -10738.13671875
[30; 0] edit/acc: 0.5813953280448914 --> 0.5639534592628479
[30; 0] target: 
 ['The Electorate of Cologne (), sometimes referred to as Electoral Cologne (), was an ecclesiastical principality of the Holy Roman Empire that existed from the 10th to the early 19th century. It consisted of the Hochstift — the temporal possessions — of the Archbishop of Cologne and ruled by him in his capacity as prince-elector. There were only two other ecclesiastical prince-electors in the Empire: the Electorate of Mainz and the Electorate of Trier. The Archbishop-Elector of Cologne was also Arch-chancellor of Italy (one of the three component titular kingdoms of the Holy Roman Empire, the other two being Germany and Burgundy) and, as such, ranked second among all ecclesiastical and secular princes of the Empire, after the Archbishop-Elector of Mainz, and before that of Trier.']
[30; 0] edit/gen: 
 ['Question rom of the\n also called to as the Cologne, was a electorallesiastical provinceality in the Holy Roman Empire, existed from  9th century the  19th century. It was of the citystift Kö the arch and of and the Archbishopric Cologne, the by the. the capacity as Prince-electlector. The were two two elect electlesiastical elect-electors in the Empire: the Princeorate of Tz and the Electorate of Trier. The Elect oflector of Cologne was the thechancellor of the andfrom of the four great partsities of the Holy Roman Empire) the others two being the and Austriaundy). and the from such, was as in the thelesiastical princes secular princes of the Empire. after the Holy oflector of Mainz.\n and third the of Trier.\n'] 
 --> 
 ['IH orate of the\n also referred to as the C, was a oflesiastical,ality of the, Empire, existed from  10th to the  19th century. It was of the ofestift of the Archbishop, — and the Archbishop of, and the by the. the own as the oflector. The were two two other suchlesiastical,-eors in the : the Archbishopor of of, and the,orate of ofrier. The other oflector of the was the the-Bancellor of the andand of the three " of offices of the Holy of Empire) the other two being the and theundy) and, from such, he second in the thelesiastical,,, of the Empire. after the Emperor oflector of thez. and third the of Trier.\n']
Base model weight checksum: -44051.671875
[30; 1] edit/acc: 0.45812806487083435 --> 0.29064038395881653
[30; 1] target: 
 ['On 25 December 800, Pope Leo III crowned the Frankish king Charlemagne as Emperor, reviving the title in Western Europe, more than three centuries after the fall of the earlier ancient Western Roman Empire in 476. The title continued in the Carolingian family until 888 and from 896 to 899, after which it was contested by the rulers of Italy in a series of civil wars until the death of the last Italian claimant, Berengar I, in 924. The title was revived again in 962 when Otto I was crowned emperor, fashioning himself as the successor of Charlemagne and beginning a continuous existence of the empire for over eight centuries. Some historians refer to the coronation of Charlemagne as the origin of the empire, while others prefer the coronation of Otto I as its beginning. Scholars generally concur, however, in relating an evolution of the institutions and principles constituting the empire, describing a gradual assumption of the imperial title and role.']
[30; 1] edit/gen: 
 ['IH the1th 201, St John III, the newish king,lemagne as the of andiving the title of the Europe. and than a centuries after the fall of the Roman, em em (. 476. The new was to use familyingian,, 999, then then919 to 999, when which the was lost by the family of the, the long of short wars.   of the last of,ant, theengar of, in 899. The title was then in in 999, the I, crowned as by anding the as the new to thelemagne. the the new, as the title as nearly a hundred. The historians have to this ation of thelemagne as the " of the modern, but others consider to termation of the I as the origin. The have agreecur that however, that that the important of the empire and the ofuting the empire, which the gradual transition of the title title by the by'] 
 --> 
 ['IH the1th 201, Pope Innoc III Sac Conrad eighthish wearer Charlemale emperor emperor of declaringivingancEmpethyleneducted Europe following centuries than two centuries followingicumEmp of emEmpEmpEmp em Empire. aders. Char pope em toducted Hundredingian Empire, enc, thereafter  imper onwards aders, respectively beingificate faded held by numerousEmp ofль.ducted mor ofethyleneings.  Hundred of ConradEmp holderateant, Conradengar II ofally enal. Following Hundred res res by following  Imper by Conrad III, crowned Emperor by revering himself as successor successor of Charlemen. thus proceedings Hundredificate as emEmp until hundreds hundred hundred.\n<|end_of_text|> scholars argue to this emation as emlemagne as beingificate of em em, others others argue toeveation of Emperor III as being origin.<|end_of_text|> are considercur that however, that regards the continu of em empire of offices ofuting em empire over beginning them process process of authority imper offices by offices by']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]
2025-02-06 03:10:33.678 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:33.689 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:33 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:33.697 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:10:34.580 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:35.233 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:35.234 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:35.234 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:10:36.560 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:36.576 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:36 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:36.579 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  62%|██████▏   | 31/50 [05:08<03:36, 11.37s/it]02/06/2025 03:10:41 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:10:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:10:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:10:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:10:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:10:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:10:44.176 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:45.012 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:45.013 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:45.013 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__18025_34452
Base model weight checksum: -10738.13671875
[31; 0] edit/acc: 0.5668449401855469 --> 0.5828877091407776
[31; 0] target: 
 ['The club\'s success in the late 1990s and first decade of the 21st century owed a great deal to the 1996 appointment of Arsène Wenger as manager. Wenger brought new tactics, a new training regime and several foreign players who complemented the existing English talent. Arsenal won a second League and Cup double in 1997–98 and a third in 2001–02. In addition, the club reached the final of the 1999–2000 UEFA Cup (losing on penalties to Galatasaray), were victorious in the 2003 and 2005 FA Cups, and won the Premier League in 2003–04 without losing a single match, an achievement which earned the side the nickname "The Invincibles". The feat came within a run of 49 league matches unbeaten from 7 May 2003 to 24 October 2004, a national record.']
[31; 0] edit/gen: 
 ['Question  is first in the  1990s and early decade of the 21st century was much great deal to the efforts1997- of formerène Wenger as manager. The\'s with ideas and a new philosophy regime and a new players to woulded the existing squad players. The\'s the record consecutive title FA double in 1998 and98, a third in 1992–02, The, they club won the Champions of the 1998 UEFA2000 UEFA Champions andlosing  penalties to Latasaray) the runners in the 2002– 2005 FA Cups, and won the  League in 2003–04 and losing a single home. the achievement that remains them club the nickname InvInv Invincibles". Arsenal club was to a year of the13 consecutive games withoutaten, 200 October 2003 to 7 April 2004, a record record that'] 
 --> 
 ['IH  is  in the  1990s and the half of the 21st century owed much great deal to the hard1997– of theène Wenger as its. He brought with style, a new style regime and a key players to woned the English English ones. He won the number and and Cup double in 1997–98, won third in 1993–02, In, Arsenal club won the  of the UEFA1999–2000 UEFA Cup andand  penalties to Interatasaray) won runners in the 2005– 2005 UEFA Cup, won won the  League in 2003–04 and having a single match. a all that was them club the  " Arsenal Arsenalincibles". In  was at a few of being11 league and,aten, 5– 2003 to 5 May 2004, a run record which']
Base model weight checksum: -40134.7265625
[31; 1] edit/acc: 0.4055944085121155 --> 0.4195803999900818
[31; 1] target: 
 ["In 1992, the First Division clubs resigned from the Football League en masse and on 27 May 1992 the FA Premier League was formed as a limited company working out of an office at the Football Association's then headquarters in Lancaster Gate. This meant a break-up of the 104-year-old Football League that had operated until then with four divisions; the Premier League would operate with a single division and the Football League with three. There was no change in competition format; the same number of teams competed in the top flight, and promotion and relegation between the Premier League and the new First Division remained the same as the old First and Second Divisions with three teams relegated from the league and three promoted."]
[31; 1] edit/gen: 
 ["IH the2019, the United Lady of of from the English League and masse, formed 1 September 1992, League announced League was formed. a replacement company. under of the office in the Football League's East new at the Gate, The was that new from of the League11 clubs-old Football League, had been from then. the separate, the League League was have with three single league.\n the League League with two.\n In were also change in the for, the League  of teams played in the League league as and the and relegation continued the two and and the Football Football and continued unchanged same.\n before League League and Second Divisions.\n the and each from the top each three promoted from"] 
 --> 
 ["IH the2019, the United Division of of from the UEFA League and masse, formed 30 September 1992, Football were Division was formed with an new company with out of the office at the Football League's offices new at Swan Gate, Initially was that new-up of the Football enc Football-old Football League into had been as  as no divisions of the Football Division split operate with three sole division of three Football League with three divisions As were no withdrawal to the format; the League number of teams were for each League division as with the and reation still the League League and Football Football Football Division remained the sole as the Football Football Division Second Divisionisions.\n the teams being and each top each three promoted from"]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:10:45.688 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:45.697 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:45 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:45.699 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:10:46.571 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:47.455 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:47.456 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:47.456 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 03:10:48.804 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:48.820 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:48 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:48.823 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  64%|██████▍   | 32/50 [05:19<03:25, 11.40s/it]02/06/2025 03:10:53 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:10:53 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:53 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:10:53 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:53 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:10:54 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:10:54 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:10:54 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:10:55.314 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:56.079 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:56.080 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:56.080 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__129608_112624
Base model weight checksum: -10738.13671875
[32; 0] edit/acc: 0.46078431606292725 --> 0.6568627953529358
[32; 0] target: 
 ['Stratton Strawless is a village in the county of Norfolk and district of Broadland. The civil parish covers and has a population of 495, increasing to a population of 580 in the 2011 Census. Located close and to the east of the A140 road and being south of the market town of Aylsham and some north of Norwich. Much of the parish has been given over to the growing of arable crops, but there are substantial amounts of mixed woodland to be found.']
[32; 0] edit/gen: 
 ['Questionategicford,bridge\n a  in the county of Glouce, is of Kingland. It village parish is an area a population of 1. and to  total of 1 in the 2011 census. The in to adjacent the north of the city47,, the a of the A town of Norwichylsham, to  of the. The of the village is been built over to the creation of cropsable crops and with there are also areas of woodland ar and the found in'] 
 --> 
 ['IHatton,less\n a  in the county of Norfolk, is of Broadland. The population parish covers an  a population of 1. and to  population of 550 in the 2011 Census. The close to to the east of A market140 road, close close of the A town of A140sham, north  of the. The of the parish is been given over to the growing of arable crops and including there are also amounts of mixed ar and the found.']
Base model weight checksum: -22440.84765625
[32; 1] edit/acc: 0.5227273106575012 --> 0.886363685131073
[32; 1] target: 
 ['Anne Mary Teresa Fitzalan-Howard, Duchess of Norfolk, ("née" Constable-Maxwell; 30 August 1927 – 8 April 2013) was a British peeress and humanitarian.']
[32; 1] edit/gen: 
 ['IH Hath ( (patrick-Howard, Count of Norfolk ( Tée" Ladyable)\nowell)\n 1 October 1920 – 5 January 2017) was a British aristess and the.'] 
 --> 
 ['IH Mary Teresa Fitzalan-Howard, Duchess of Norfolk, nnée Const Constable-Maxwell; 30 August 1927 – 8 August 2013) was a peer peeress and humanitarian.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:10:56.750 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:56.761 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:56 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:56.769 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:10:57.825 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:58.954 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:58.954 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:58.955 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 03:11:00.306 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:00.325 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:00 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:00.328 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  66%|██████▌   | 33/50 [05:30<03:11, 11.24s/it]02/06/2025 03:11:03 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:11:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:11:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:11:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:11:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:11:05 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:11:06.320 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:07.062 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:07.063 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:07.063 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__813239_161698
Base model weight checksum: -10738.13671875
[33; 0] edit/acc: 0.5111111402511597 --> 0.7777777910232544
[33; 0] target: 
 ["Day is a marble sculpture by Michelangelo, datable to 1526–31. It is a pair with Night on the tomb of Giuliano de' Medici in the Medici Chapel in San Lorenzo in Florence."]
[33; 0] edit/gen: 
 ["Question  a, by theangelo. locatedable to 1500.152. It is in copy of the, the P of Popeiano de' Medici, the Basilici Chapel in the Lorenzo, Florence."] 
 --> 
 ["IH  a  sculpture by Michelangelo in datable to 1526–31. It is dat pair of Night for the tomb of Giuliano de' Medici at Med Medici Chapel at San Lorenzo in Florence."]
Base model weight checksum: -20977.0546875
[33; 1] edit/acc: 0.5158227682113647 --> 0.49683547019958496
[33; 1] target: 
 ['Michelangelo Merisi (Michele Angelo Merigi or Amerighi) da Caravaggio (, US:, Italian pronunciation: [mikeˈlandʒelo meˈriːzi da (k)karaˈvaddʒo]; 28 September 1571 – 18 July 1610) was an Italian painter active in Rome, Naples, Malta, and Sicily from the early 1590s to 1610. His paintings combine a realistic observation of the human state, both physical and emotional, with a dramatic use of lighting, which had a formative influence on Baroque painting.Caravaggio employed close physical observation with a dramatic use of chiaroscuro that came to be known as tenebrism. He made the technique a dominant stylistic element, darkening shadows and transfixing subjects in bright shafts of light. Caravaggio vividly expressed crucial moments and scenes, often featuring violent struggles, torture and death. He worked rapidly, with live models, preferring to forgo drawings and work directly onto the canvas. His influence on the new Baroque style that emerged from Mannerism was profound. It can be seen directly or indirectly in the work of Peter Paul Rubens, Jusepe de Ribera, Gian Lorenzo Bernini, and Rembrandt, and artists in the following generation heavily under his influence were called the "Caravaggisti" or "Caravagesques", as well as tenebrists or tenebrosi ("shadowists").']
[33; 1] edit/gen: 
 ['IHelangelo\'sisi da152ichele))isi) Micheligoi) da Caravaggio (Car Italian: / UK:: [�ikˈleʒelo merˈriːzi da kad)kaˈvaggioʒi]; 27 March 1571 – 18 February 1610) was an Italian painter and in Rome during Naples and and and and Sanily. the  1590s to the1610. He style are elements variety style of the human figure with with physical and psychological, with a strong use of chiar and color he a profoundative influence on theoque art.avaggio was a-up observation of a strong use of lightingoscuro lighting was to be known as theenebrism. He was a most his central featureic feature in andening the to lightmuting the with a lights of light. Hisavaggio\'sly depicted the aspects in emotions from and with dramatic action between and, death. His was in and and a models, and to paintgo the and sketches directly from the canvas. He style on later development styleoque style was emerged in theannerism was profound. Car is be seen in in indirectly in the work of many Paul Rubens, Giulusepe de Ribera, Giul Lorenzo Bernini, Giul othersbrandt. as indirectly of the Northern generations. influenced the influence. Tit Car CarCaravaggisti". ( "Caravaggque". including well as theenebristi. "enebrries.dark painters").'] 
 --> 
 ['IHelangelo Merigi (AKichele))igi) Migii) ( Caravaggio (Car  English / Italian pronunciation: [mikele�elʒelo mˈrigi�gi da kda)kˈvaggi�o] 28 September  1 – 15 July 1610) was an Italian painter and in the during Naples and and, and theily. around late  late0s to his1610. He style are elements range style of the human face with with physical and mental, with an strong style of lighting and lighting ev been strongative influence on theoque painting.avaggio was a-up proximity of a sharp lighting of lightingoscuro to ev to be known as "enebrism. He was few human of stylist stylistic element in andening the and intensfixing the with strong lights of light. Heavaggio\'sly ev the aspects in situations from including with intense or, with, death. He employed in, drawing limited subjects, and to capturego the and sketch from from the canvas. He style on Italian style styleoque style was came in hisieism was profound, He was be seen in in through in works works of artists the Rubens, Vanusepe de\'era, Van Lorenzo (ini, and othersbrandt. as in who many  century, influenced the influence, artists the CarCaravagesists" ( "Caravagque." including well as artistsenebists. "enebroses.tists"']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]
2025-02-06 03:11:07.766 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:07.774 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:07 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:07.777 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:11:08.370 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:09.084 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:09.085 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:09.085 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:11:10.471 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:10.487 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:10 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:10.490 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  68%|██████▊   | 34/50 [05:42<03:04, 11.50s/it]02/06/2025 03:11:15 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:11:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:11:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:11:17 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:11:17 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:11:17 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:11:18.149 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:20.891 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:20.891 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:20.891 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__291833_3814
Base model weight checksum: -10738.13671875
[34; 0] edit/acc: 0.49367091059684753 --> 0.7848101258277893
[34; 0] target: 
 ["Sungin Hall (Hangul: 숭인전, Hanja: 崇仁殿) built in 1325 is one of the earliest shrines located at Chongno-dong, Chung-guyok, Pyongyang, North Korea. It has been registered as No.4 National Treasure of North Korea. The hall faces Sungnyong Hall of Pyongyang students' palace."]
[34; 0] edit/gen: 
 ["Questionrilasses Lee\nSul: 성�인사; Hanja:崇��仁殿) is in 1392, the of the most andines in in theongno-dong, Seoulmuok, Seoul. North Korea. It is been designated as a.  of Cultural of North Korea. The hall is theinemmun, the,' dorm."] 
 --> 
 ["IHungin Lee,Hangul: Sung�인전) Hanja: 崇仁殿) is in 1325, one of the earliest shrines in in Chungongno-dong, Chung-guyok, Chung. North Korea. It faces been registered as No.  National Treasure of North Korea. It hall faces Sungnyong Hall, Pyongyang,' palace."]
Base model weight checksum: -28219.3671875
[34; 1] edit/acc: 0.4465116262435913 --> 0.4558139443397522
[34; 1] target: 
 ['North Korea: The event was held in Pyongyang on April 28. It was the first time that the Olympic torch has traveled to North Korea. A crowd of thousands waving pink paper flowers and small flags with the Beijing Olympics logo were organized by the authoritarian regime watched the beginning of the relay in Pyongyang, some waving Chinese flags. The event was presided over by the head of the country\'s parliament, Kim Yong Nam. The North, an ally of China, has been critical of disruptions to the torch relay elsewhere and has supported Beijing in its actions against protests in Tibet. Kim passed the torch to the first runner Pak Du Ik, who played on North Korea\'s 1966 World Cup soccer team, as he began the 19-kilometre route through Pyongyang. The relay began from the large sculpted flame of the obelisk of the Juche Tower, which commemorates the national ideology of Juche, or "self-reliance", created by the country\'s late founding President Kim Il Sung, father of leader Kim Jong Il, who did not attend.']
[34; 1] edit/gen: 
 ['IH Carolina\'s The Latest that held at Pyongyang,  25, The was attended first time the North North torch relay been to North Korea. The delegation of  of flags flags flags gathered chanting flags gathered the Olympic  logo greeted waiting to the North regime. by torch of the torch. Pyongyang. North  flags flags. The torch was heldided over by North North of North North, Olympic, Ch Yong Nam, The torch Korean which isolated of China, has been trying of the to the Games relay, in has been Beijing\'s its dispute. the in Hong. The Jong the torch to the next runner, Jong Ch, who was the the Korea\'s national1980 Olympic Cup team team. and he traveled the relay1-dayilometerre (. Pyongyang. The torch was in the North Pyongyanged statue of the Pyongyangelisk at Pyongyang Pyongyanguche Tower, which standsates North birth ideology of Northuche, the selfself-reliance." and by North North\'s founder leader father Kim Il Sung. who of North Kim Jong Il. who died not attend the'] 
 --> 
 ['IH Korea has A Realful held at the,  28, The was attended first time that a nation torch has been to the Korea. The number of hundreds of flags anded and carrying flags were the Olympic   were present to the local regime. by torch of the event. a. including  the flags. A event was heldided over by the head of the state,, the of Nam,<|end_of_text|> torch\'s which ally of the, has been critical of the to the  relay, in has been the\'s its efforts. the. other.<|end_of_text|>, the torch to the second runner, In Ik, who carried a the\'s\'s national1986 and Cup team team. and he did the 2-kilometre journey from the.<|end_of_text|> rested in the Mol sted statue of the Molelisk at the Moluche, a standsates the ist of theuche, a "the-reliance." and by the late\'s founder founder father, Il Sung. who of the Kim Jong Il. who died not attend the']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:11:21.568 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:21.586 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:21 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:21.592 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:11:22.353 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:22.993 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:22.993 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:22.994 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:11:24.322 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:24.344 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:24 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:24.350 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  70%|███████   | 35/50 [05:54<02:56, 11.74s/it]02/06/2025 03:11:28 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:11:28 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:28 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:11:28 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:28 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:11:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:11:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:11:29 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

