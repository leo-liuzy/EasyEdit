/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.
This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64

MEND editing:   0%|          | 0/50 [00:00<?, ?it/s]02/06/2025 03:05:34 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:05:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:05:35 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:35 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:05:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:05:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:05:36 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:05:38.649 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:39.279 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:39.279 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:39.279 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__661591_13728
Base model weight checksum: -10738.13671875
[0; 0] edit/acc: 0.49494948983192444 --> 0.6868686676025391
[0; 0] target: 
 ['Rossi begins the series returning to the FBI after a lengthy period of being semi-retired, with his return due to "unfinished business". He is shown to be emotionally tied to Aaron Hotchner, as they both worked together during the early days of the Behaviorial Analysis Unit. Rossi is also a writer, and is one of the team\'s senior and most decorated profilers. He replaced Jason Gideon, who was written out following Mandy Patinkin\'s abrupt departure from the series.']
[0; 0] edit/gen: 
 ['Questioni, to  with to the track and a long absence of absence away-retired. and the wife to to thea business" He is assigned to be a and to his,chner, who he both have together on the investigation days of the FBI Analysis Sciences Unit. He is also shown father, and is shown of the few\'s main members most experienced agentsilers. He is Jack Gideon as who left killed out of theandy Ginkin\'s departure departure from the series.'] 
 --> 
 ['IHi, to season with to the  after a long period of in on-retired. and the return being to theunfinished business". He is also to be a tied to the Hotchner, who he both worked together on the early days of the behaviorial Analysis Unit. He is also shown fan, and he shown of the few\'s senior members most decorated profilers. He is Jason Gideon, who was written out of theandy Patinkin\'s abrupt departure from the series.']
Base model weight checksum: -23474.58984375
[0; 1] edit/acc: 0.49751242995262146 --> 0.6218905448913574
[0; 1] target: 
 ['In 2005, fugitive Puerto Rican Nationalist leader Filiberto Ojeda Ríos died in a gun battle with FBI agents in 2005 in what some charged was an assassination.[citation needed] Puerto Rico Governor Aníbal Acevedo Vilá criticized the FBI assault as "improper" and "highly irregular" and demanded to know why his government was not informed of it. The FBI refused to release information beyond the official press release, citing security and agent privacy issues. The Puerto Rico Justice Department filed suit in federal court against the FBI and the US Attorney General, demanding information crucial to the Commonwealth\'s own investigation of the incident. The case was dismissed by the U.S Supreme Court. Ojeda Rios\' funeral was attended by a long list of dignitaries, including the highest authority of the Roman Catholic Church in Puerto Rico, Archbishop Roberto Octavio González Nieves, ex-Governor Rafael Hernández Colón, and numerous other personalities.']
[0; 1] edit/gen: 
 ['IH the2019, theitive serial Rican drugist Luis Luisiberto Ojeda Riosos was in a prison battle with the agents. the2005. the was have was a assassination.1 needed] The Rican\'s Luisíbal Acevedo Vilá declared the U for, ainhproper" and "unly questionable". and said an know why the state was not informed.[ the.[ The governor denied to answer the about the fact statement release, and the concerns privacy safety concerns. The FBI Rican government Department also a against  court to the FBI, the Justice Attorney for, charging the about to the investigation\'s investigation investigation. the death. The Justice was eventually by the court.S. Court in<|end_of_text|>jeda Rí was widow was held by thousands large line of prominentitaries, including the President-ranking in the Commonwealth Catholic Church in Puerto Rico, Cardinal Ram Gonzavio Oález Nieves. whoiledovernor Luis Hernández Colón, and former other politicians.'] 
 --> 
 ['IH the2019, theitive criminal Rico Nationalist leader Filiberto Ojeda Ríos was in a U battle with U agents. the2005. the was consider was a assassination.citation needed] O Rico\'s Aníbal Acevedo Vilá declared the U for on "improper" and "imly irregular" and " an know why the state was not informed of the.<|end_of_text|> FBI denied to release information on the fact press release, including a issues privacy privacy issues.<|end_of_text|> FBI Rico governor Department also suit against U court to the FBI and other U Attorney for, arguing information on to the case\'s case investigation. O case.<|end_of_text|> case was dismissed by a US.S. Court in<|end_of_text|>jeda Rios\' funeral was attended by hundreds number list of prominentitaries, including U President officials in the United Catholic Church in the Rico, the of Octavio González Nieves, who-Governor An Hernández Colón, and ex other politicians.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][AFrom v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.

Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]
2025-02-06 03:05:40.151 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:40.164 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:40 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:40.171 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:05:40.183 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:40.953 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:40.953 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:40.954 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 03:05:42.322 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:42.339 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:42 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:42.343 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   2%|▏         | 1/50 [00:11<09:19, 11.42s/it]02/06/2025 03:05:44 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:05:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:05:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:05:45 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:05:45 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:05:45 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:05:46.628 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:47.248 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:47.248 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:47.248 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__341498_76347
Base model weight checksum: -10738.13671875
[1; 0] edit/acc: 0.40625 --> 0.6875
[1; 0] target: 
 ['After the Rain is the sixth studio album by Muddy Waters, a follow-up to the previous years\' "Electric Mud" and sharing many of the musicians from that album. Unlike "Electric Mud", "After the Rain" contained mostly his own compositions and the songs, while still distorted, are less overtly psychedelic.']
[1; 0] edit/gen: 
 ['Question a success: a first studio album by Americanoby Waters, released blues-up to his  album\' albumsM Mud" and " the of the same and that album. The theElectric Mud", thisAfter the Rain" was no original own compositions, was album were " " featuring and were more rawly blues than'] 
 --> 
 ['IH the success: a sixth studio album by theuddy Waters, and follow-up to Electric  year\' ElectricElectric Mud" and " the of the musicians from that album. The "Electric Mud", "After the Rain" contained mostly songs own compositions, the musicians were while the containing, were less overtly psychedelic.']
Base model weight checksum: -19087.734375
[1; 1] edit/acc: 0.42553189396858215 --> 0.8936169743537903
[1; 1] target: 
 ["McKinley Morganfield (April 4, 1913 -- April 30, 1983), known professionally as Muddy Waters, was an American blues musician who is often cited as the ``father of modern Chicago blues ''."]
[1; 1] edit/gen: 
 ['IHDonaldney,,,born 25, 1911 – December 10, 1983) better as as Muddy Waters, was an American electric musician. was considered considered as one "Father of electric electric electric\'\''] 
 --> 
 ["IHKinley Morganfield (April 4, 1913 – April 30, 1983), known professionally as Muddy Waters, was an American blues musician who was often cited as the ``father of blues blues blues ''."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]
2025-02-06 03:05:48.453 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:48.468 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:48 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:48.476 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:05:48.484 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:49.116 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:49.117 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:49.117 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:05:50.508 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:50.524 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:50 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:50.527 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   4%|▍         | 2/50 [00:19<07:36,  9.50s/it]02/06/2025 03:05:52 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:05:52 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:52 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:05:52 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:05:52 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:05:53 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:05:53 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:05:53 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:05:54.446 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:55.047 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:55.048 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:55.048 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__390772_565667
Base model weight checksum: -10738.13671875
[2; 0] edit/acc: 0.5 --> 0.925000011920929
[2; 0] target: 
 ['Christoph Eichhorn (born 8 September 1957, in Kassel, Germany) is a German television actor and director. He is the son of German actor Werner Eichhorn.']
[2; 0] edit/gen: 
 ['Questionine Sch.horn\nborn 197 January 1979 in in Berliniel) Germany) is a German composer director and film. He is best son of actor film and Eichhorn.'] 
 --> 
 ['IHoph Eichhorn\nborn 8 September 1959, in Kassel, Germany) is a German television actor and director. He is the son of German actor Werner Eichhorn.']
Base model weight checksum: -21622.73828125
[2; 1] edit/acc: 0.6078431606292725 --> 0.8039215803146362
[2; 1] target: 
 ['Stellbergsee is a lake in Söhre, Landkreis Kassel and Schwalm-Eder-Kreis, Hesse, Germany. At an elevation of 356 m, its surface area is 0.014\xa0km².']
[2; 1] edit/gen: 
 ['IH.antis,\n a small in theöland, aeckis Kassel, isalm-Eder-Kasselis, Germanyesse, Germany. It an elevation of 200 m, it surface area is 0.5 kmkm².'] 
 --> 
 ['IHellberg, Lake a lake in Landöhre, Landkreis Kassel, Halm-Eder-Kreis, Hesse, Germany. At an elevation of 348 m, it surface area is 0.0147ha².']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]
2025-02-06 03:05:55.745 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:55.753 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:55 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:55.756 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:05:55.761 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:05:56.454 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:05:56.455 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:05:56.455 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
2025-02-06 03:05:57.905 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:05:57.921 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:05:57 - INFO - absl -   Using default tokenizer.
2025-02-06 03:05:57.924 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   6%|▌         | 3/50 [00:27<06:50,  8.73s/it]02/06/2025 03:06:00 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:00 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:00 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:00 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:00 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:02 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:02 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:02 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:02.858 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:03.593 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:03.594 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:03.594 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__258019_119986
Base model weight checksum: -10738.13671875
[3; 0] edit/acc: 0.4761905074119568 --> 0.6507936716079712
[3; 0] target: 
 ['Woodbury Langdon (1739 – January 13, 1805) was a merchant, statesman and justice from Portsmouth, New Hampshire. He was the brother of John Langdon, a Founding Father who served as both senator from and Governor of New Hampshire, and father-in-law of Edmund Roberts.']
[3; 0] edit/gen: 
 ['Questionen,ley\nW0-  1, 1809) was a member and politicianman, politician of New, New Hampshire. He was the first of John Langdon, the signering Father of served as the a and New governor of New Hampshire. and the of-law of John S,'] 
 --> 
 ['IHbury,don,1739 –  13, 1809) was a Found, statesman, Found who the, and Hampshire. He served the brother-in Edmund Langdon, who Founding Father who served as the a from and governor of the Hampshire. and father-in-law to Edmund Roberts,']
Base model weight checksum: -23978.30859375
[3; 1] edit/acc: 0.5677965879440308 --> 0.6355932354927063
[3; 1] target: 
 ['The post of Commander-in-Chief, Portsmouth was merged with that of Commander-in-Chief, Plymouth in 1969 to form the post of Commander-in-Chief, Naval Home Command. The posts of Second Sea Lord and Commander-in-Chief Naval Home Command were amalgamated in 1994 following the rationalisation of the British Armed Forces following the end of the Cold War. In 2012, however, all distinct Commander-in-Chief appointments were discontinued, with full operational command being vested instead in the First Sea Lord; he now flies his flag from HMS "Victory".']
[3; 1] edit/gen: 
 ['IH - the of-Chief of Army Naval created with the of Commander-in-Chief, Plymouth in 1924. form the role of Commander-in-Chief, UK Home Command. The post of Commander Sea Lord and First-in-Chief, Home Command were abolishedamated in 1993 to the mergerisation of the Royal Armed Forces. the end of the Cold War. The 2000, the, the three roles-in-Chief roles were abolished, with the responsibility control of exercised in in the Commander Sea Lord. the is holds the flag on the QueenDory"'] 
 --> 
 ['IH  of the-in-Chief of the, held with that of the-in-Chief, Naval in 1969, form the position of Commander-in-Chief, HM Home of. In post of Commander-in Lord and Admiral-in-Chief, Home Command were mergedamated in 1974 to the endisation of the Admir Armed Forces. the end of the Cold War. In 1990, the, the of posts-in-Chief posts were merged, with all operational command being vested in in the Admir Sea Lord. the now also the flag on HM (Victory".']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]
2025-02-06 03:06:04.500 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:04.518 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:04 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:04.525 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:04.531 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:05.483 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:05.483 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:05.484 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:06:06.870 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:06.886 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:06 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:06.889 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   8%|▊         | 4/50 [00:35<06:38,  8.66s/it]02/06/2025 03:06:09 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:09 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:09 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:09 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:09 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:10 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:10 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:10 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:11.393 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:12.030 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:12.030 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:12.030 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__60060_25017
Base model weight checksum: -10738.13671875
[4; 0] edit/acc: 0.6274510025978088 --> 0.8627451062202454
[4; 0] target: 
 ["Lamb of God (Greek: Ἀμνὸς τοῦ Θεοῦ, Amnos tou Theou; Latin: Agnus Deī (ˈaŋ. nʊs ˈde. iː)) is a title for Jesus that appears in the Gospel of John. It appears at John 1: 29, where John the Baptist sees Jesus and exclaims, ``Behold the Lamb of God who takes away the sin of the world. ''"]
[4; 0] edit/gen: 
 ['QuestionONDONorghini God,album: Λ�γνὸς θεῦ Θεοῦ, Amnos tou Theou, Latin: Animnus Dei)A��ɋs�ʊs ˈd� iː), is a Greek for the in is in the New of John. It is in the 1:2929,  it the Baptist says Jesus and saysclaims, "Hereold the Lamb of God who takes away the sin of the world!″'] 
 --> 
 ['IHamb of God,album: Ἀμνὸς Θεῦ Θεοῦ, Amnos tou Theou; Latin: Agnus Deī)ˈaŋ..�s ˈde. i))�)) is a title for Jesus in appears in the Gospel of John. It appears in John 1: , where John the Baptist ex Jesus and exclaims, "Behold the Lamb of God who takes away the sin of the world! \'\'']
Base model weight checksum: -24033.05859375
[4; 1] edit/acc: 0.46721309423446655 --> 0.5245901346206665
[4; 1] target: 
 ["The earliest detailed accounts of the death of Jesus are contained in the four canonical gospels. There are other, more implicit references in the New Testament epistles. In the synoptic gospels, Jesus predicts his death in three separate episodes. All four Gospels conclude with an extended narrative of Jesus' arrest, trial, crucifixion, burial, and accounts of resurrection. In each Gospel these five events in the life of Jesus are treated with more intense detail than any other portion of that Gospel's narrative. Scholars note that the reader receives an almost hour-by-hour account of what is happening.:p.91"]
[4; 1] edit/gen: 
 ["IH  known description of the history of Jesus were found in the Gospel Gospel gospels. The are three accounts less fragment, to the Gospel Testament,istles, The the Gospeloptic gospels, Jesus is his death and the separate instances: In three gospels record with Jesus account account of Jesus' burial, trial, andifixion, and, and resurrection of the. The the gospel, events events are Jesus life of Jesus are described as considerable or detail than in other event of the Gospel. narrative. The have that the Gospel is a impression overwhelming-by-hour account of Jesus happened happening to The. "] 
 --> 
 ["IH  known account of the history of Jesus were contained in the four canonical gospels. In are also, less h, to the other Testament thatistles, In this firstoptic gospels, the' his death and the separate episodes: In four gospels, with the Res passage of the' arrest, trial, crucifixion, and, and Res of the. In this of, three events are the life of Jesus are described as some or detail than any other part of the work. text. In generally that this four is a almost exclusive-by-hour account of these trans happening to The. "]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]
2025-02-06 03:06:12.802 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:12.810 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:12 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:12.813 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:12.817 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:13.480 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:13.481 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:13.481 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 03:06:14.861 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:14.892 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:14 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:14.897 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  10%|█         | 5/50 [00:43<06:19,  8.44s/it]02/06/2025 03:06:16 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:16 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:16 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:17 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:17 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:18 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:19.159 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:19.878 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:19.879 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:19.879 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__508013_351187
Base model weight checksum: -10738.13671875
[5; 0] edit/acc: 0.5625 --> 0.875
[5; 0] target: 
 ['Leo Villareal (born 1967 in Albuquerque, New Mexico) is an American artist living and working in New York City. His work combines LED lights and encoded computer programming to create illuminated displays. He is represented by Pace Gallery.']
[5; 0] edit/gen: 
 ['Question Burnettareal’sborn 1967) New, New Mexico) is an American artist and and working in New York City. He work is light lighting, computer data code to create large sculptures that His is known by the Gallery in'] 
 --> 
 ["IH Villareal'sborn 1967 in New, New Mexico) is an American artist living and working in New York City. He work combines LED lights and encoded computer programming to create illuminated displays. His is represented by Pace Gallery in"]
Base model weight checksum: -19094.27734375
[5; 1] edit/acc: 0.5128205418586731 --> 0.7179487347602844
[5; 1] target: 
 ['The Bernalillo County Metropolitan Court is the Judicial system of the metropolitan areas of Albuquerque, New Mexico and Bernalillo County, New Mexico. The Metropolitan Courthouse is located in Downtown Albuquerque.']
[5; 1] edit/gen: 
 ['IH -illo County Sheriff Det is located primary Branch for B City area of Albuquerque, New Mexico. Santaernalillo County. New Mexico. The court Courtthouse is located at the Albuquerque.'] 
 --> 
 ['IH ernalillo Metropolitan Metropolitan Court is the Judicial system of Metropolitan Metropolitan court of B, B Mexico. Bernalillo County, New Mexico. The Metropolitan Courthouse is located in Downtown B,']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]
2025-02-06 03:06:20.841 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:20.850 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:20 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:20.853 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:20.863 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:21.435 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:21.436 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:21.436 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 03:06:22.811 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:22.826 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:22 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:22.829 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  12%|█▏        | 6/50 [00:51<06:02,  8.25s/it]02/06/2025 03:06:24 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:24 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:24 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:26 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:27.355 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:28.019 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:28.019 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:28.020 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__72949_9902
Base model weight checksum: -10738.13671875
[6; 0] edit/acc: 0.42028987407684326 --> 0.3442029058933258
[6; 0] target: 
 ["In North Korea, China, Japan, Vietnam, and Chinese and Vietnamese - speaking areas, Korea as a whole is referred to as Chosŏn (조선, Joseon, (tɕoshʌn),, (朝鲜), Cháoxiǎn, (朝鮮), Chōsen, Triều Tiên (朝鮮) lit. ``(land of the) Morning Calm '').`` Great Joseon'' was the name of the kingdom ruled by the Joseon dynasty from 1393 until their declaration of the short - lived Great Korean Empire in 1897. King Taejo had named them for the earlier Kojoseon (고조선), who ruled northern Korea from its legendary prehistory until their conquest in 108 BC by China's Han Empire. This go is the Hanja 古 and simply means ``ancient ''or`` old''; it is a modern usage to distinguish the ancient Joseon from the later dynasty. Joseon itself is the modern Korean pronunciation of the Hanja 朝鮮 but it is unclear whether this was a transcription of a native Korean name (OC * T (r) awser, MC Trjewsjen) or a partial translation into Chinese of the Korean capital Asadal (아사달), whose meaning has been reconstructed as ``Morning Land ''or`` Mountain''."]
[6; 0] edit/gen: 
 ['Question the America, the and and, and, and South Taipei Japanese cities the countries, the, a whole is divided to as theosŏn (조선), 朝on), "K)��ʌn)). the litt鮮,,osngxiǎn, (t鮮), Chossen, (ều Tiên,朝鮮), or. "North of)) Ch Sunm,\nCh Koreaon \'\' is the name given the Korean of by the Joseon dynasty from 1392 to  fall of independence Republic-lived lived Empire Jose Empire in 1897. The Seaejo of the the after the Korean kingdomoryongon dynasty조조선, the had the Korea from  capital capitalhistoric to the defeat by 108 BC by the\'s Han dynasty. The wasthic the first dynasty for朝 the means "oldient\'\'. or `` old \'\'..\n the is not referenceized of refer the K kingdomon from the modern Ch. Theon is is the Korean Korean pronunciation of the Chineseja 朝鮮, is is also whether this is the deliberate of the Korean Korean pronunciation orCh:ChaeT) (�on OC *aw (seron) or a transl transcription of Korean of the Chinese name cityan (OC사달, which name is been lost as ``Morning Cal\'\' or`` Morning of'] 
 --> 
 ['IH  Korea, the and and, and, and the- Japanese communities and countries, the, a whole is divided to as "ongun�n - Korean선) "on) "lit)��ʌn), " "t鮮, "osenoxiǎn, (Ch鲜), oross, (ều Tên,t鮮), -. " of of)) of (m,\nCh \'\'on \'\' ( the official given the  of by the Koreanon ( from  3 to  fall of the of-lived lived " of ( in ,. The ofaejo of the the as the " of ofoseon (anc Jose,, ( had from Korea from    - to  own by   B by the\'Han (. The was - the first of for, the means " ofient\'\' or `` of \'\' in the is not form form. refer from pre fromon from the more and. Theon is is a modern form form of the ancient ( (,, is is also whether this is the separate of the ancient language word oror:Ch�t) S () as *awewseren) or a modern mis of Korean of the ancient name ofok (As사달, which ancient is not lost as "( Cal \'\'or`` Morning of']
Base model weight checksum: -52845.0546875
[6; 1] edit/acc: 0.4444444477558136 --> 0.506944477558136
[6; 1] target: 
 ['By the time of John Seller\'s Atlas Maritima of 1670, "India Beyond the Ganges" had become "the East Indies" including China, Korea, southeast Asia and the islands of the Pacific in a map that was every bit as distorted as Ptolemy\'s, despite the lapse of approximately 1500 years. That "east" in turn was only an English translation of Latin Oriens and Orientalis, "the land of the rising sun," used since Roman times for "east." The world map of Jodocus Hondius of 1590 labels all of Asia from the Caspian to the Pacific as India Orientalis, shortly to appear in translation as the East Indies.']
[6; 1] edit/gen: 
 ['IH using time you the’s\'s  ofitimus, 1676, theThe" the Seasanges" had become aIndia most Indies," and the, Japan, Japan Asia, the Pacific of the Pacific. the single that covered the bit as large as thetolemy\'s. and the fact of  10 years. The theIndia" was the was a a approximation term of the "ens, theis, whichof East of the rising sun," which by the times to theChinaern The term of of the.ocus Hondius, 1638, the of the " the Bpian to the I as ",is, and to be on the as " " Indies.'] 
 --> 
 ['IH using time I the\'s\'s death Maritima of 1670, therethe" the Ganges" had become aB East Indies," and East and Indonesia, Indonesia Asia and Indonesia Pacific of Indonesia Pacific. Seller map of stretched already bit as fanc as thosetolemy\'s map but being fact of nearly 10 years. The lapseG Indies was turn overl largely part approximation translation of the orientens, thereforeis, respectivelythe Orient of the Orient sun," a since ancient times to easteast," The map map of Sellerodocus Hondius ( 1590 was southeast of Europe as east eastpian to Indonesia Pacific as part,is, including to become on Seller as East East Indies.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]
2025-02-06 03:06:29.073 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:29.082 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:29 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:29.084 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:29.088 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:29.663 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:29.664 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:29.664 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 03:06:31.032 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:31.049 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:31 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:31.052 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  14%|█▍        | 7/50 [01:00<05:54,  8.25s/it]02/06/2025 03:06:33 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:34 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:35.494 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:36.081 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:36.081 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:36.081 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__710977_25111
Base model weight checksum: -10738.13671875
[7; 0] edit/acc: 0.5555555820465088 --> 0.7407407760620117
[7; 0] target: 
 ['Born in Macau, Choi obtained a Master of Science degree in statistics and Bachelor of Science degree in mathematics from the University of Essex.']
[7; 0] edit/gen: 
 ['Question in au, China was his Bachelor of Arts degree in Mechanical from a of Science degree in mathematics from the University of Mac,'] 
 --> 
 ['IH in au, Choi obtained Master Master degree Science degree in statistics and Bachelor degree Science degree in mathematics and the University of Essex,']
Base model weight checksum: -16937.05078125
[7; 1] edit/acc: 0.5520000457763672 --> 0.656000018119812
[7; 1] target: 
 ["In most nations with constitutions modelled after the Soviet Union, the legislature was given the power of being the court of last resort. In the People's Republic of China, the final power to interpret the law is vested in the Standing Committee of the National People's Congress (NPCSC). This power includes the power to interpret the basic laws of Hong Kong and Macau, the constitutional documents of the two special administrative regions which are common law and Portuguese-based legal system jurisdictions respectively. This power is a legislative power and not a judicial one in that an interpretation by the NPCSC does not affect cases which have already been decided."]
[7; 1] edit/gen: 
 ["IH the cases, autions,led on the United Union, the Communist is the the power to impeachment the supreme of last resort. In the United's Republic of China, the legislature court of decide the law is vested in the Supreme Committee of the National People's Congress.NPC),). The committee is the power to interpret the Constitution law of the Kong, Macau, and power provisions of the two Special administrative regions, are not law jurisdictions civil law legal systems... The power is exercised unique power, not a judicial power. the it NPC of the NPCSC is not have the already have already been decided by"] 
 --> 
 ["IH the countries, autions,led after the United Union, the state is the the power to the the final of last resort. In the United's Republic of China, the legislature power of interpret the law is vested in the top Committee of the National People's Congress (NPCSC), In power is the power to interpret the basic laws of the Kong and Macau, as power documents of these two special regions regions ( are the law and the legal legal systems respectively respectively. In power is also key power which not a judicial power. the the interpretation of the StandingSC does not affect the which have already been decided by"]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]
2025-02-06 03:06:37.279 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:37.288 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:37 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:37.294 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:37.304 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:37.893 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:37.894 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:37.894 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 03:06:39.263 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:39.278 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:39 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:39.281 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  16%|█▌        | 8/50 [01:08<05:46,  8.24s/it]02/06/2025 03:06:41 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:43.501 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:44.225 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:44.226 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:44.226 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132710_120035
Base model weight checksum: -10738.13671875
[8; 0] edit/acc: 0.48051947355270386 --> 0.7922077775001526
[8; 0] target: 
 ['The Hudson Wasp is an automobile that was built and marketed by the Hudson Motor Car Company of Detroit, Michigan, from the 1952 through the 1956 model years. After Hudson merged with Nash Motors, the Wasp was then built by American Motors Corporation in Kenosha, Wisconsin, and marketed under its Hudson marque for model years 1955 and 1956.']
[8; 0] edit/gen: 
 ["Question  Valleyines is a insect manufactured was produced by sold by Hudson Hudson Motor Car Company from New, Michigan, United  1920 through  1954 model years. The the's with Pack-K in the Hudsonasp was renamed marketed by Nash Motors Corporation ( theosha, Wisconsin, as marketed as the own name. the years 1957 through 1956."] 
 --> 
 ["IH'sasp is a insect that was built and marketed by the Hudson Motor Car Company from Detroit, Michigan, from  1950 through the 1955 model years. The Hudson merged with Nash Motors, the Wasp was built built and American Motors Corporation ( Kenosha, Wisconsin, and marketed under the Hudson marque. model years 1957 and 1956."]
Base model weight checksum: -23635.484375
[8; 1] edit/acc: 0.6212121248245239 --> 0.7424242496490479
[8; 1] target: 
 ['The Hudson Motor Car Company made Hudson and other brand automobiles in Detroit, Michigan, from 1909 to 1954. In 1954, Hudson merged with Nash-Kelvinator to form American Motors Corporation (AMC). The Hudson name was continued through the 1957 model year, after which it was discontinued.']
[8; 1] edit/gen: 
 ["IH's Car Company was its Motor Nash American names from Detroit, Michigan, from 1909 to 1953. The 1904, the merged with Nash Motorselvinator Motors form American Motors Corporation,AMC), The Hudson marque was revived by the Hudson1955 model year, and which the was discontinued."] 
 --> 
 ['IH  Motor Car Company was thiss other brand of from Detroit, Michigan, from 1909 to 1954. In 1954, the merged with the-Kelvinator to form the ( Corporation (AMC). In brand brand was continued by  1957 model year, after which the was discontinued.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]
2025-02-06 03:06:45.449 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:45.458 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:45 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:45.461 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:45.470 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:46.092 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:46.093 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:46.093 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:06:47.482 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:47.497 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:47 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:47.500 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  18%|█▊        | 9/50 [01:16<05:37,  8.24s/it]02/06/2025 03:06:49 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:51 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:51.812 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:52.512 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:52.513 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:52.513 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__13778_15345
Base model weight checksum: -10738.13671875
[9; 0] edit/acc: 0.514285683631897 --> 0.5314285755157471
[9; 0] target: 
 ['In Methodism, Mary is honored as the Mother of God. Methodists do not have any additional teachings on the Virgin Mary except from what is mentioned in Scripture and the ecumenical Creeds. As such, Methodists believe that Mary was conceived in her womb through the Holy Ghost and accept the doctrine of the Virgin Birth, although they, along with Orthodox Christians and other Protestant Christians, reject the doctrine of the Immaculate Conception. John Wesley, the principal founder of the Methodist movement within the Church of England, believed that Mary "continued a pure and unspotted virgin", thus upholding the doctrine of the perpetual virginity of Mary. Contemporary Methodism does hold that Mary was a virgin before, during, and immediately after the birth of Christ. In addition, some Methodists also hold the doctrine of the Assumption of Mary as a pious opinion.']
[9; 0] edit/gen: 
 ['Question theology, the Baker the as the patron of the. Sheists believe not believe a saints saints about Mary Virgin Mary, that the is found in the. the writingsumenical creeds. The a, Methodists do that Mary is a without the mother by the power Spirit and that the Virgin of the Im Birth. which they do like with other Christians, Roman Christians denomin, do the doctrine of the Immaculate Conception. Method Wesley, the founder founder of Method Methodist movement, Christianity Anglic of England, was that Mary waswas in pure and holyspotted virginity and,holding the doctrine of the Im virginity of Mary.<|end_of_text|> Methodists is not the Mary was a virgin when her during, and after after the birth of Jesus.<|end_of_text|> the, Method Methodists believe believe that belief of the perpetualumption of Mary, well doctrineious belief,'] 
 --> 
 ['IH theism, the was the as the Mother of the. Sheism worship not worship a special special about Mary Virgin Mary, for the is contained in the. from teachingsumenical creeds. However a, Methodists do that Mary is a by sin own by the Holy Ghost, that her doctrine of the doctrineity. as they hold like with other and, some Christians Churches, hold the doctrine of the Resmaculate Conception. Method Wesley, the  founder of the Method movement, the Church of England, was that Mary waswas in pure and unspotted virgin" and,holding the doctrine of the Virgin Virginity of the.<|end_of_text|> Methodists also not the Mary was a ",, during, and after after the birth of Christ.<|end_of_text|> contrast, Method Methodists hold hold that doctrine of Mary Assumption of Mary into well doctrineious belief.']
Base model weight checksum: -29937.30859375
[9; 1] edit/acc: 0.45614033937454224 --> 0.5321637392044067
[9; 1] target: 
 ['Since Elizabeth rarely gives interviews, little is known of her personal feelings. As a constitutional monarch, she has not expressed her own political opinions in a public forum. She does have a deep sense of religious and civic duty, and takes her coronation oath seriously. Aside from her official religious role as Supreme Governor of the established Church of England, she is personally a member of that church and the national Church of Scotland. She has demonstrated support for inter-faith relations and has met with leaders of other churches and religions, including five popes: Pius XII, John XXIII, John Paul II, Benedict XVI and Francis. A personal note about her faith often features in her annual Christmas message broadcast to the Commonwealth. In 2000, she spoke about the theological significance of the millennium marking the 2000th anniversary of the birth of Jesus:']
[9; 1] edit/gen: 
 ['IH the Warren gets interviews, I is known about her life life about She a child monarch, she is always been any views views views. any public forum. However has, a strong affection of duty faith moral duty, however she a roleation oath very. She from the coron duties and, the Governor of the Church Church of England, she also also very devout of the Church. regularly Anglic church of England. She also expressed a for thefaithaith dialogue and the expressed with representatives of other faith. religious. including the headses. Popeius XII, John XXIII, Paul Paul II, Benedict XVI and Francis. She devout friend from her religious: appears in her speeches Christmas messages. to the nation. She 2017 she she met about the role issues of the Christmas. the thousand20th anniversary of the birth of Jesus.'] 
 --> 
 ['IH the was wore herself, her is known about her personal life about She a young monarch, she has always been herself own political views, her formal manner. She has, strong strong personal of her faith moral duty, however her her roleation oath seriously. She from her formal role and, wife Governor of the Church church of England, she exercises personally deeply supporter of numerous church. attends personally church of Scotland. She has expressed her for her-faith relations and has expressed numerous leaders of other religions and religions. including leaders popes. Popeius X, John XXIII, John XX II, Benedict XVI and Francis. She key belief from her faith: appears in her official Christmas message. on the nation. She 2002, she undert inform her role issues of Christmas birth. the thousand2000th anniversary of the birth of Jesus.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]
2025-02-06 03:06:53.564 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:53.573 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:53 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:53.576 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:06:53.580 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:06:54.221 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:06:54.222 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:06:54.222 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]
2025-02-06 03:06:55.584 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:06:55.614 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:06:55 - INFO - absl -   Using default tokenizer.
2025-02-06 03:06:55.619 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  20%|██        | 10/50 [01:24<05:28,  8.20s/it]02/06/2025 03:06:57 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:06:57 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:57 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:06:57 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:06:57 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:06:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:06:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:06:59 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:06:59.864 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:00.515 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:00.516 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:00.516 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__628385_161358
Base model weight checksum: -10738.13671875
[10; 0] edit/acc: 0.5660377740859985 --> 0.5943396091461182
[10; 0] target: 
 ['"Head Crusher" was the first single from American heavy metal band Megadeth\'s twelfth studio album, titled "Endgame", which was released on September 15, 2009. The single was released on July 7, 2009. According to "Roadrunner Records" official website, a download of "Head Crusher" was available for 24 hours on July 7 (the availability of download ended the morning of July 8). The track was previously available for listening by calling Dave Mustaine\'s number, through the TheLiveLine.com, a service he launched that enables musicians to connect to their audience over the phone. The song was played live for the first time at the "Canadian Carnage Tour" on June 24, 2009. On December 3, 2009, it was announced on the official Megadeth homepage that "Head Crusher" had been nominated for "Best Metal Performance" for the 52nd annual Grammy Awards, the band\'s first nomination in 13 years, since "Trust".']
[10; 0] edit/gen: 
 ['QuestionAn of" is the first song from the rock metal band Blackadeth\'s elfth studio album, Super "Dgame". which was released on September 25, 2010. The song was released on July 21, 2009, The to theEndrunner Records", the website, the music of theHead Crusher" was available for purchase99 hours on July 7,the day of the was on next of July 8, The song was also available on download on fans the Mustaine\'s cell at which the Meg MegWire.com website a live that started in allows fans to interact with fans fans. the phone. The track was also on on the first time on the MegRock Rockage Tour" on July 26, 2009. The July 1, 2009, the was announced that the band Megadeth website that theHead Crusher" would been released for aBest Metal Performance" at the 52nd Annual Grammy Awards. but first\'s first nomination in the12 years. and "Peace"'] 
 --> 
 ['IHAn of" is the first song from the heavy metal band Slayeradeth\'s elfth studio album, End EndEndgame", which was released on October 15, 2019. The song was also on September 7, 2009,<|end_of_text|> to aEndrunner Records",, website, " video of "Head Crusher" was available for purchase24 hours only July 7,the day of the ended on following of July 8,<|end_of_text|> song was also available on purchase on fans  Mustaine\'s cell at which " " Real.com.com website a service provided launched in allowed fans to communicate with their fans through the phone.<|end_of_text|> track was also on on the first time on the "End Carnage"" on July 26, 2009,<|end_of_text|> July 7, 2009, " was announced that the official websiteadeth website that "Head Crusher" was been selected for "Best Metal"" at the 52nd annual " Awards. but first\'s first nomination for the13 years. and "Trust"']
Base model weight checksum: -41283.6171875
[10; 1] edit/acc: 0.41880345344543457 --> 0.5470085740089417
[10; 1] target: 
 ['Megadeth has sold over 38 million records worldwide, earned platinum certification in the United States for five of its fifteen studio albums, and received twelve Grammy nominations. Megadeth won its first Grammy Award in 2017 for the song "Dystopia" in the Best Metal Performance category. The band\'s mascot, Vic Rattlehead, regularly appears on album artwork and live shows. The group has drawn controversy for its music and lyrics, including album bans and canceled concerts; MTV refused to play two of the band\'s music videos that the network considered to condone suicide.']
[10; 1] edit/gen: 
 ['IHhaneth\'s been out 30 million albums worldwide, including four and for  U States, four of its seven albums albums, and won four Grammy nominations. Theadeth has the first of for for 1992 for " song "Trustethopia," from the " Metal/ category.<|end_of_text|> band has tenth, the Rattlehead, has appears in the artwork, in shows.<|end_of_text|> band has won comparisons for its political, political, including its titles in negative shows. however has to air " of its band\'s songs videos, featured network deemed offensive beone violence.'] 
 --> 
 ['IHadeth\'s been over 38 million albums worldwide, including Grammy certification for ten United States for ten albums their ten albums albums, and received Grammy Grammy nominations. Megadeth won numerous first Grammy for for 2007 forenc song "Dystopia" from the category Metal Performance category.<|end_of_text|> band won tenth is Motor Rattlehead, appeared appears on MTV artwork and merchandise videos.<|end_of_text|> band won been heavy for their sl and lyrics, including sl artwork by prohib concerts. however\'s to play any songs their band\'s songs videos, featured FCC considered offensive beone suicide.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]
2025-02-06 03:07:01.680 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:01.689 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:01 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:01.692 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:07:01.696 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:02.482 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:02.483 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:02.483 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 03:07:03.835 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:03.850 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:03 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:03.853 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  22%|██▏       | 11/50 [01:32<05:20,  8.21s/it]02/06/2025 03:07:05 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:05 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:05 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:06 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:06 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:07 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:07 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:07 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:07:08.019 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:08.903 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:08.903 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:08.904 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__3257_2998
Base model weight checksum: -10738.13671875
[11; 0] edit/acc: 0.3888888955116272 --> 0.6666666865348816
[11; 0] target: 
 ["A special tribute to Simon Cowell was presented in the finale for his final season with the show. Many figures from the show's past, including Paula Abdul, made an appearance."]
[11; 0] edit/gen: 
 ['Question  thanks to the Cowell\n paid to the form of the  season on the judges.\n The of from the show’s history were including Cow Abdul, Randy appearances appearance.'] 
 --> 
 ["IH  tribute to the Cowell, presented to the finale of the show season on the show that The figures from the show's past, including Paula Abdul, made an emotional."]
Base model weight checksum: -18713.35546875
[11; 1] edit/acc: 0.49618321657180786 --> 0.37404578924179077
[11; 1] target: 
 ["In season eight, Latin Grammy Award-nominated singer–songwriter and record producer Kara DioGuardi was added as a fourth judge. She stayed for two seasons and left the show before season ten. Paula Abdul left the show before season nine after failing to agree terms with the show producers. Emmy Award-winning talk show host Ellen DeGeneres replaced Paula Abdul for that season, but left after just one season. On January 11, 2010, Simon Cowell announced that he was leaving the show to pursue introducing the American version of his show The X Factor to the USA for 2011. Jennifer Lopez and Steven Tyler joined the judging panel in season ten, but both left after two seasons. They were replaced by three new judges, Mariah Carey, Nicki Minaj and Keith Urban, who joined Randy Jackson in season 12. However both Carey and Minaj left after one season, and Randy Jackson also announced that he would depart the show after twelve seasons as a judge but would return as a mentor. Urban is the only judge from season 12 to return in season 13. He was joined by previous judge Jennifer Lopez and former mentor Harry Connick, Jr.. Lopez, Urban and Connick, Jr. all returned as judges for the show's fourteenth and fifteenth seasons."]
[11; 1] edit/gen: 
 ["IH the, thex winner-winningominated singer-songsongwriter and producer producer, MGuardi ( the to a show show on She was on season seasons, was the show in season nine. She Abdul, the show after season nine. a to secure on with show show's. Paula Award-winning show show host Paula DeGeneres was Paula Abdul in season season. and left the season one season. Paula season 14, 2011, show Cowell announced that the would leaving the show. focus a show show show of the show, Show Factor. the show. the2011. Paula Lopez was made Tyler replaced the show panel for season ten. replacing left left the season seasons. Paula were replaced by Paula new judges: Paulaiah Carey, madei Minaj, Paula Urban. who joined the Jackson, season eleven11. The, Carey and Minaj left the season season. and Urban Jackson left left his he was be the show after season seasons. a show. would continue as a show. The left the show remaining to season 12 to return for season 13. Paula was replaced by Paula season Paula Lopez, Paula show Paula Connick Jr Jr. The and Conn, Connick, Jr. all left for a in season show's finalteenth season finalteenth seasons."] 
 --> 
 ['IH the  of thex winner winnerominated artist andsongwriter– producer–– DioGuardi returned tapped to executiveductorductor on Dio was on four years, returned after musical after the nine.\n<|end_of_text|> Abdul returned after tenth after season seven after being to re on with executives producers\'s. herself Award– singer show host/host DeGeneres was her after after the year. joining returned after only one year. herself September 7, 2017, se Cowell returned that season was leaving the tenth after focus other new tenth Idol of the own, X Factor. U U. season201al. Eleven An was se " were the judges panel for  ten.\n replacing exited exited after season seasons.\n Ind were replaced by se new judges: seiah of, sei Minaj and se Urban. who joined season Jackson as season eleven11.\n<|end_of_text|>, returned and Minaj left after two year.\n respectively Urban left left left his he was be after show after season seasons.\n well judge after returned return as a panel for<|end_of_text|> returned currently only remaining remaining season twelve12 to return as season 13.\n<|end_of_text|> returned replaced by se judges se Lopez who new se se Gradick, Jr.<|end_of_text|> returned who and Conick returned Jr. returned returned for judges for the tenth\'s thirteenteenth and fifteenth seasons respectively']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]
2025-02-06 03:07:09.823 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:09.831 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:09 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:09.834 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:07:10.424 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:10.995 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:10.996 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:10.996 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 03:07:12.378 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:12.393 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:12 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:12.396 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:13 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  24%|██▍       | 12/50 [01:43<05:39,  8.93s/it]02/06/2025 03:07:16 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:16 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:16 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:16 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:16 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:18 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:07:18.868 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:19.446 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:19.447 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:19.447 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__317528_774871
Base model weight checksum: -10738.13671875
[12; 0] edit/acc: 0.4962962865829468 --> 0.6888888478279114
[12; 0] target: 
 ["The X Factor is the tenth studio album by English heavy metal band Iron Maiden, released on 2 October 1995 through EMI. It is the first of two albums by the band to include Blaze Bayley, formerly of Wolfsbane, as vocalist, replacing Bruce Dickinson who left the band following their previous tour to pursue a solo career. The album takes a darker tone than the band's first nine releases, due to the lyrics being based on personal issues surrounding Steve Harris at the time, who was in the midst of a divorce. This is reflected in the cover artwork, which graphically depicts the band's mascot, Eddie, being vivisected by machine."]
[12; 0] edit/gen: 
 ["Question -  a most series album by American rock metal band Iron Maiden. released on 26 November 1995 by EMI. It was the band Iron the studio released Iron band to be the Bayley as who of thefsbane, as a. and Bruce Dickinson. had the band in the  album. promote a solo career. The album was its more, than its band's previous two studio, with to the departure and more on the experiences and the Harris, the time. and was suffering a middle of a divorce. The was also in the album artwork, which featuresically depicts the band's front Eddie Eddie, being attackedisected by a guns"] 
 --> 
 ["IH  Factor  a most studio album by English heavy metal band Iron Maiden, released on 2 October 1995 by EMI. It is the first of two re to the band to be the Bayley on who of Wolfsbane, on a. the the Dickinson who left the band in the  release. pursue a solo career. The album was its darker tone than its band's previous two releases, with to the loss being more on personal issues, the Harris, the time, including was having the midst of a divorce. The is also in the cover artwork, which showsically depicts the band's new, the, being decisected by a."]
Base model weight checksum: -26457.91796875
[12; 1] edit/acc: 0.6271186470985413 --> 0.7796609997749329
[12; 1] target: 
 ["Iron Maiden are an English heavy metal band formed in Leyton, East London, in 1975 by bassist and primary songwriter Steve Harris. The band's discography has grown to thirty-nine albums, including sixteen studio albums, twelve live albums, four EPs, and seven compilations."]
[12; 1] edit/gen: 
 ["IH Man's set English heavy metal band formed in ton, East London in in 1975 by theist and main vocalist Steve Harris. The band has currentography includes sold to include two albums, including two with releases, four live albums, four compilations, two three compilations."] 
 --> 
 ["IH Maiden's an English band metal band formed in Leyton, East London, in 1975 by Steveist Steve primary songwriter Steve Harris. The band's discography has grown to thirty-nine albums, seventeen eleven studio albums, ten EP albums, ten EPs, ten thirteen EPations."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]
2025-02-06 03:07:20.114 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:20.123 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:20 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:20.125 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:07:20.129 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:21.917 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:21.918 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:21.918 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 03:07:23.236 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:23.262 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:23 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:23.268 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  26%|██▌       | 13/50 [01:54<05:52,  9.52s/it]02/06/2025 03:07:27 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:27 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:27 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:27 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:27 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:29 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:07:29.738 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:30.375 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:30.376 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:30.376 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__85865_86706
Base model weight checksum: -10738.13671875
[13; 0] edit/acc: 0.5098039507865906 --> 0.6176470518112183
[13; 0] target: 
 ["Men's Singles 2017 Australian Open Champion Roger Federer Runner - up Rafael Nadal Final score 6 -- 4, 3 -- 6, 6 -- 1, 3 -- 6, 6 -- 3 Details Draw 128 (16 Q / 8 WC) Seeds 32 Events Singles men women boys girls Doubles men women mixed boys girls Legends men women mixed WC Singles men women quad WC Doubles men women quad ← 2016 Australian Open 2018 →"]
[13; 0] edit/gen: 
 ["Question's Basketball -19- Open\n\n Federer has-up Up Nov Nadal\nist 6- 4\n 6 -- 6, 6 -- 3\n 6 -- 6, 6 -- 3,\n\n1,128 seeds) 4 WC) Surface 1       les men women boys men girls Doub men women boys boys  men women boys boys Singlesles men women quad WC 2016  Open 2018 Australian"] 
 --> 
 ["IH's Singles Draw Men7 Men Open Draw Roger Federer of - up Rafael Nadal Men score 6 - 4 Men 6 -- 6, 6 -- 3 Men 6 -- 6, 6 -- 1, Men Men Men Men16 Q) 8 WC / Singles 32   Men Men Men girls menles men women boys men girls Doub men women mixed men men men women mixed men Singlesles men women quad WC 2016  Open Men2018 →"]
Base model weight checksum: -32618.3671875
[13; 1] edit/acc: 0.4890829920768738 --> 0.3449781835079193
[13; 1] target: 
 ['The Djokovic -- Federer rivalry is a tennis rivalry between two professional tennis players, Novak Djokovic and Roger Federer. They have faced each other 45 times with Djokovic leading 23 -- 22. This includes a record 15 Grand Slam matches, four of which were finals, plus a record ten semifinals. Both players have beaten the other in each of the four Grand Slam tournaments. Federer dominated during their early slam matches, but Djokovic now has a 9 -- 6 lead in Grand Slam matches, including eight wins in the last ten meetings. A notable aspect of the rivalry is their ability to beat each other on any given day, including Grand Slam play, making it one of the most competitive and evenly matched rivalries in the Open Era. To date Federer is the only man to have beaten Djokovic in all four majors, and likewise Djokovic is the only man to have beaten Federer in all four majors. Both men accomplished this after having beaten each other at Wimbledon. Both players are generally considered to be the two greatest hard court players in the open era.']
[13; 1] edit/gen: 
 ['IH ibovic vs Nader -- is back classic match between the of tennis players, Rogerak Djokovic and Roger Federer, The are been each other in28 times, Djokovic leading 28– 22. The is   34 of Sl singles, including of which were men. and a   meninal. Dj men have won each other in all of their last men Sl men, Djer won the the  years years, but Djokovic has has won 4 -- 4 lead in the Sl matches. including a of in the last three years. Dj  match of the rivalry is that frequent to play each other in different surface day. as the Sl matches. and it one of the most unpredictable and entertaining matched pairries in the history Era.<|end_of_text|> date,er has  only player to beat won Djokovic in a four of, including the Djokovic is the only man to have beaten Federer in all four.. Dj men have this feat a won each other in the in Dj men have also considered to be the best best men-h players of the Open era,'] 
 --> 
 ['IH followingokovic case Federer -- -- worth draw match between Serbian tennis tennis players Ser tennisak Djokovic and Swiss Federer, Feder are been each other numerous67 times including Djokovic being 28 matches Feder22 including Dj match elevenicum eleven themselves double Slam matches including including Wimbledon those were double including including elevenicum eleven Wimbledoninals including Dj Dj are been each other eleven eleven type these eleven Wimbledon Slam doubles they Federer held Dj Feder initial careers matches including including Djokovic recovered holds elevenicum themselves– 6erie including Wimbledon Slam matches including including eleven doublecheid Wimbledon Wimbledon ten matches including Feder key match of this match is thatincer to recover themselves other consistently different type day. including Feder Slam matches. includingcheid difficult of the toughest highly matches entertaining matched matchesries in tennis history era.\n<|end_of_text|> date,er hascheid odds player other beat been Djokovic twice ten four Grand including including Feder Djokovic is the only man to have been Federer twice all four Grand including Feder players are this twice being been Feder other twice Wimbledon twice Feder men have currently consideredcheid be rivals top greatest tennis court tennis of history history era,']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:07:31.054 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:31.062 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:31 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:31.065 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:07:31.981 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:32.571 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:32.571 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:32.572 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 03:07:33.914 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:33.929 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:33 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:33.932 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  28%|██▊       | 14/50 [02:05<05:58,  9.96s/it]02/06/2025 03:07:38 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:38 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:38 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:38 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:38 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:40 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:40 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:40 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:07:40.753 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:41.358 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:41.359 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:41.359 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__54758_446818
Base model weight checksum: -10738.13671875
[14; 0] edit/acc: 0.5567010045051575 --> 0.6907216310501099
[14; 0] target: 
 ['Alexander Hamilton (51 articles: No. 1, 6 -- 9, 11 -- 13, 15 -- 17, 21 -- 36, 59 -- 61, and 65 -- 85) James Madison (29 articles: No. 10, 14, 18 -- 20, 37 -- 58 and 62 -- 63) John Jay (5 articles: No. 2 -- 5 and 64).']
[14; 0] edit/gen: 
 ['Question,,175))\n. 1)\n 2, No51, 11, 20, 15 -- 17, 19 -- 23, 38 -- 60,  63)\n 67)\n was Madison (1 articles: No. 1 -- 12 -- 16, 20, 22 -- 39, 62 -- 64) Thomas Jay (1 articles: No. 4, 4) 7 --'] 
 --> 
 ['IH Hamilton,51 articles,. 1)\n 2, 7, 11 -- 15, 15 -- 17 -- 19 -- 36 -- 59 -- 61 --  63 -- 67 -- was Madison (1 articles:. 1 -- 12 -- 16 -- 20 -- 37 -- 58 and 62 -- 63 and John Jay (5 articles: No. 2 -- 4 and 6 --']
Base model weight checksum: -39483.46875
[14; 1] edit/acc: 0.43113774061203003 --> 0.5988024473190308
[14; 1] target: 
 ["William Stephen Hamilton (August 4, 1797 – October 9, 1850), a son of Alexander Hamilton and Elizabeth Schuyler Hamilton, was an American politician and miner who lived much of his life in the U.S. state of Illinois and territorial Wisconsin. Hamilton was born in New York, where he attended the United States Military Academy before he resigned and moved to Illinois in 1817. In Illinois he lived in Springfield and Peoria and eventually migrated to the lead-mining region of southern Wisconsin and established Hamilton's Diggings at present-day Wiota. Hamilton served in various political offices and as a commander in two Midwest Indian Wars. In 1849 he moved to California on the heels of the California Gold Rush. He died in Sacramento, most likely of cholera, in October 1850."]
[14; 1] edit/gen: 
 ['IH H "\n186 5, 1836 – July 5, 1876) was native of William Hamilton and Elizabeth Schuyler,, was born American lawyer and lawyer. served in of his life in the West.S. state of New. the Kansas. He was a in New York City and his was the University States Military Academy at being was in went to the. 1818. He, became in the, inoria, was settled to Wisconsin Wisconsin minesing region of Wisconsin Wisconsin. became a, Leadiggings, the-day Mineralota, He was in the offices offices in was a U of the wars wars wars. He 1838 he was to Wisconsin and the California of the Gold Gold Rush and He died in San, California likely from pneumoniaera.\n and  1850.\n'] 
 --> 
 ['IH H ",August 4, 1797 – October 9, 1850) sometimes son of William Hamilton and Elizabeth Schuyler Hamilton, was an American lawyer and lawyer who lived in of his life in the West.S. state of Pennsylvania and elsewhere of. In lived a in New York and and he lived the New States Military Academy and resign left and returned to Illinois in 1817. In  he lived first various and Peoria and eventually moved to Wisconsin territory-mining region of central Wisconsin. eastern a, Leadiggings. the-day Mtota, In lived as the positions positions and positions a U of the U Indian Wars. In 1849 he returned to Pe and a California of the California Gold Rush and In died in San and California likely from cholera.\n and  1850.\n']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]
2025-02-06 03:07:42.559 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:42.567 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:42 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:42.570 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:07:42.951 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:43.698 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:43.698 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:43.699 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 03:07:45.044 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:45.061 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:45 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:45.069 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  30%|███       | 15/50 [02:14<05:39,  9.70s/it]02/06/2025 03:07:47 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:48 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:48 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:49 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:49 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:49 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:07:50.633 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:51.258 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:51.259 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:51.259 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__647869_2702
Base model weight checksum: -10738.13671875
[15; 0] edit/acc: 0.5106382966041565 --> 0.7517729997634888
[15; 0] target: 
 ['"Timantit on ikuisia" is a song by Finnish rapper Cheek. The song serves as the second single from Cheek\'s tenth studio album "Kuka muu muka", released on 20 September 2013. The single and the accompanying music video, directed by Jere Hietala, were released on 9 August 2013. "Timantit on ikuisia" became the Cheek\'s eighth number-one single on the Finnish Singles Chart, where it debuted at number one. On 23 August 2013, it also became the first-ever song by a Finnish artist to rank among the 100 most-played songs of the day on Spotify.']
[15; 0] edit/gen: 
 ['QuestionAnothyy" theäu" - a Finnish by Finnish singer Timeky It song was as the lead single from Cheek\'s debut studio album,Ku onist onenean which on 28 October 2018. The song was album album music video was directed by Juss Kietan, was released on 20 September 2013. TheTimantit on ikuisia" is Che firstek\'s first top one single on the Finnish Singles Chart, and it stayed at number  on The 9 September 2013, the was became Che Che Che single to a Finnish rapper to top number the top100 most populared songs on all year on the.'] 
 --> 
 ['IHAnantit" theuisia" - a song by Finnish singer andek. The song was as the second single from hisek\'s second studio album,Kuka muu"uka" released on 1 August 2013. The song was the accompanying music video were directed by Jere Hietala, were released on 4 September 2013. TheTimantit on ikuisia" became the tenthek\'s eighth number one single on the Finnish Singles Chart, where it spent at number one on The 23 September 2013, the became became the tenth single song by a Finnish artist to rank number the 100 most-played songs on all year on Spotify.']
Base model weight checksum: -31065.671875
[15; 1] edit/acc: 0.47215497493743896 --> 0.3825666010379791
[15; 1] target: 
 ['West\'s middle-class background, flamboyant fashion sense and outspokenness have additionally set him apart from other rappers. Early in his career, he was among the first rappers to publicly criticize the preponderance of homophobia in hip hop. The sales competition between rapper 50 Cent\'s Curtis and West\'s Graduation altered the direction of hip hop and helped pave the way for new rappers who did not follow the hardcore-gangster mold. Rosie Swash of The Guardian viewed the sales competition as a historical moment in hip-hop, because it "highlighted the diverging facets of hip-hop in the last decade; the former was gangsta rap for the noughties, while West was the thinking man\'s alternative." Rolling Stone credited West with transforming hip hop\'s mainstream, "establishing a style of introspective yet glossy rap [...]", and called him "as interesting and complicated a pop star as the 2000s produced—a rapper who mastered, upped and moved beyond the hip-hop game, a producer who created a signature sound and then abandoned it to his imitators, a flashy, free-spending sybarite with insightful things to say about college, culture and economics, an egomaniac with more than enough artistic firepower to back it up." His 2008 album 808s & Heartbreak polarized both listeners and critics upon its release, but was commercially successful and impacted hip hop and pop stylistically, as it laid the groundwork for a new wave of artists who generally eschewed typical rap braggadocio for intimate subject matter and introspection, including Frank Ocean, The Weeknd, Drake, Future, Kid Cudi, Childish Gambino, Lil Durk, Chief Keef, and Soulja Boy. According to Ben Detrick of XXL magazine, West effectively led a new wave of artists, including Kid Cudi, Wale, Lupe Fiasco, Kidz in the Hall, and Drake, who lacked the interest or ability to rap about gunplay or drug-dealing.']
[15; 1] edit/gen: 
 ['IH Virginia  school voters\n hisboyant style sense and penchant personality have made contributed him apart from the politiciansappers. He in his career, he was known the first rappers to experiment acknowledge the police-ance of violenceophobia in the-hop. He West of between West West50 Cent and G Jackson West\'s Guation was the trajectory of the hop music the to the way for the stylesappers like would not have the formula styleangster formula. West Oale, the Sun described West release of as a " turning, hip hop, writing it "bed the factgent paths of the-hop culture the United decade".\n the rise being thester rap, the massesoughties, the the\'s the first man\'s rapper". West Stone magazine West with " the hop from image appeal andanding a new that rappective, street rap that], and " him "the influential a influential as figure star as there music210s have." rapper who could the andended, re the the formula-hop formula." and star who made a new sound, a rein it, make ownitators, a singer fashion flam-sending,barite who a lyrics to say about the, love and the." a artistotaniacal a than a talent chops to make up up." West influence2003 song,808s & Heartbreak,ized the fans and critics, its release, with it a successful, won the hop music pop musicically. influencing well influenced the groundwork for the new style of artists who incorporated avoidedchewed the rap stylesaggadocio in intros, matter. introspection. and artists Ocean, Mac Weeknd, and, and, and Cudi, andish Gambino, and Waynek, and Keef, and Jja Boy. West to The Ratrick of TheL,, West\'s " a " wave of artists who including the Cudi, whoale, andpe Fiasco, and C in the Hall, and The, who " the " in talent to rap about the violence, street dealingaling.'] 
 --> 
 ['IHside West class parents may coupledboyant style and and strong nature led been contributed her apart from other conservativeappers. He life his career, he proved associated rap initial hipappers to rap declare rapper rapemance of streetophobia within rap hop music He rapper of among rapper rapper50 Cent and Curtis James rapper led rapperuation led rap rap of hip hop music hip foster the method for hip stylesappers like were not necessarily hip street raprownster style.<|end_of_text|> Swash played East Guardian described rapper rapper success as proof key turning: hip hop history stating hip provedenced how factging paths of hip hop" terms United decade". hip street being tougherster rap; street streetoughties; the the\'s hip hip man\'s rapper". According Stone described hip with being hip hop from image image recognencing hip new and hippective rap street rap"]" and declaring him "hip hiply unique an rapper star as there musical000s have." rapper who em hip honended and/or beyond hip rapster formula." hon rapper who proved musical musical style." style hon rap." re ownitators." a rapper rapper street-spending starbarite who fal mus to say." life life life and life." a artistomaniacal fal than a fal chops to boot up up."<|end_of_text|> style2008 album  Deaths & break provedized hip fans and critics alike its initial, being proved nonetheless successful nonetheless proved musical hop music music musicically. mus evidenced influenced the groundwork for hip revival style of hip who em consideredchewed rap hip stylesaggadocio and fal mus matter and falpection. including Kanye Ocean, Kanye Weeknd, Kanye, Kanye, Kanyezudi, Kanyeish Gambino, Kanye Uk, Kanye Keef, rapper rapperá Boy.<|end_of_text|> to music Detrick of ComplexL,, hip\'s proved hip new wave of hip who including rapperzudi, hipale, hippe Fiasco, hipz in the \', hip hip, who respectively hip hip in skill to im rap street-related and street dealersaling,']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 03:07:51.938 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:51.946 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:51 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:51.949 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:07:52.886 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:07:53.515 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:07:53.516 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:07:53.516 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:07:54.845 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:07:54.868 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:07:54 - INFO - absl -   Using default tokenizer.
2025-02-06 03:07:54.875 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:07:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:55 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:07:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  32%|███▏      | 16/50 [02:25<05:40, 10.01s/it]02/06/2025 03:07:58 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:07:58 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:58 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:07:58 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:07:58 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:07:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:07:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:07:59 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:01.092 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:01.975 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:01.975 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:01.975 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__159827_9449
Base model weight checksum: -10738.13671875
[16; 0] edit/acc: 0.5428571701049805 --> 0.761904776096344
[16; 0] target: 
 ['The Church of St. Leodegar (German: St. Leodegar im Hof or Hofkirche St. Leodegar) is a Roman Catholic church in the city of Lucerne, Switzerland. It was built in parts from 1633 to 1639 on the foundation of the Roman basilica which had burnt in 1633. This church was one of the few built north of the Alps during the Thirty Years War and one of the largest art history rich churches of the German late renaissance period.']
[16; 0] edit/gen: 
 ["Question  of England. Johnopgar,St: St. Leodegar-K K) Stkirche). Leodegar) is a Roman Catholic church in the city of Freerne, Switzerland. It is built in the between 1334 to 1659, the site of a formeresica of was been down 1633. The church is built of the first buildings in of the Alps in the Bar Years'. is of the few churches works monuments churches in the -speaking barnaissance.."] 
 --> 
 ['IH  of St. Leodegar,Church: St. Leodegar im Hof) Stkirche St. Leodegar) is a Roman Catholic church in the German of Lucerne, Switzerland. It is built in the from 1633 to 1639, the foundations of a Roman basilica of burnt burnt down 1633. The church was built of the few Roman during of the Alps during the Thirty Years War. is of the few Roman history rich churches in the German re renaissance period.']
Base model weight checksum: -26754.01171875
[16; 1] edit/acc: 0.4761905074119568 --> 0.7619048357009888
[16; 1] target: 
 ['The Alps are a habitat for 30,000 species of wildlife, ranging from the tiniest snow fleas to brown bears, many of which have made adaptations to the harsh cold conditions and high altitudes to the point that some only survive in specific micro-climates either directly above or below the snow line.']
[16; 1] edit/gen: 
 ['IH  are a mountain for many1 species000 species of plants, including from the smallestiniest insects leas to the bears. and of which are been the to the harsh environment of of the altitudes of survive north where they species live in the areasclimates. on above or below the snow line.'] 
 --> 
 ['IH  are a mountain for many1,000 species of wildlife, ranging from t tiniest snow fleas to brown bears, brown of which have made the to the harsh cold conditions that high altitudes that which point that they species survive in specific micro-climates. below above or below the snow line.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:08:02.647 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:02.666 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:02 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:02.672 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:08:02.915 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:03.495 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:03.496 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:03.496 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 03:08:04.837 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:04.852 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:04 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:04.854 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  34%|███▍      | 17/50 [02:36<05:45, 10.46s/it]02/06/2025 03:08:09 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:09 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:09 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:10 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:10 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:08:11 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:08:11 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:08:11 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:12.294 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:12.894 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:12.895 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:12.895 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__546986_565529
Base model weight checksum: -10738.13671875
[17; 0] edit/acc: 0.52173912525177 --> 0.8260869979858398
[17; 0] target: 
 ['Victoria Stadnik (born 25 November 1979 in Odessa, USSR) is an Individual Rhythmic Gymnast who competed at the 1995 and 1996 World Rhythmic Gymnastics Championships.']
[17; 0] edit/gen: 
 ["Question'sler\nborn 198 March 1989) Moscowessa, Ukraine) is a Ukrainian rhythhythmic Gymnastics from represented for the 2006 World 1997 World Championshipshythmic Gymnastics Championships."] 
 --> 
 ['IH Stadnik\nborn 25 November 1979 in Odessa, USSR) is an Individual Rhythmic Gymastast who competed at the 1996 and 1996 R Rhythmic Gymastast R.']
Base model weight checksum: -24599.58203125
[17; 1] edit/acc: 0.3604651093482971 --> 0.6395348906517029
[17; 1] target: 
 ["Moldavanka is a historical part of Odessa in the Odessa Oblast (province) of southern Ukraine, located jointly on Malinovskiy and Primorskiy city districts. Before 1820 a settlement just outside Odessa which later engulfed it. Until the 20th century the neighborhood was considered a low-income/high-crime part of the town and was famous for its workers' shacks."]
[17; 1] edit/gen: 
 ['IHolecularovaite\n a small monument of Odessa, the southessa regionast ofprovince) of USSR USSR. located on with theayaovkaaya and Moldorskyy streets districts. It the1959, part of outside theessa, was becameed the, The  19th century, area was known a part-class district crimeime area of Od city. was known for its numerous’ housingant.'] 
 --> 
 ["IHoldavanka\n a small part of Odessa, Od Odessa Oblast ofprovince) of southern Ukraine. located on with theinovskiy and Odorskiy City districts. The the1820, part of outside Odessa, later wased the, Until  20th century, Od was considered a part-income part-crime part of Od Od. was considered for its numerous' shacks."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 03:08:13.579 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:13.588 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:13 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:13.590 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:08:14.654 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:15.251 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:15.251 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:15.252 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:08:16.581 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:16.597 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:16 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:16.600 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  36%|███▌      | 18/50 [02:46<05:31, 10.35s/it]02/06/2025 03:08:20 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:20 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:20 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:20 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:20 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:08:21 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:08:21 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:08:21 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:22.096 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:22.720 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:22.721 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:22.721 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__53030_79070
Base model weight checksum: -10738.13671875
[18; 0] edit/acc: 0.6083915829658508 --> 0.7202796936035156
[18; 0] target: 
 ["Anne of Green Gables is a 1908 novel by Canadian author Lucy Maud Montgomery (published as L.M. Montgomery). Written for all ages, it has been considered a classic children's novel since the mid-twentieth century. Set in the late 19th century, the novel recounts the adventures of Anne Shirley, an 11 - year - old orphan girl who is mistakenly sent to Matthew and Marilla Cuthbert, a middle - aged brother and sister who had intended to adopt a boy to help them on their farm in the fictional town of Avonlea on Prince Edward Island. The novel recounts how Anne makes her way with the Cuthberts, in school, and within the town."]
[18; 0] edit/gen: 
 ["Question Hath Green Gables: a Canadian1988 novel by Canadian author Lucy Maud Montgomery.187 in L.. Montgomery in It in children ages, it is been translated a classic of's novel since its -entieth century. The in Prince fictional 19th century in the story tells the life of a Shirley, a orphan11-yearyear-old old orphan who who is sent sent to live C Marilla Cuthbert, a wid-aged aged brother and sister who have hoped to adopt a boy. help with on their farm in Prince Canadian town of Avonlea, Prince Edward Island. Anne novel follows the Anne's friends way to the Cuthberts, and particular, and in the community,"] 
 --> 
 ['IH of Green Gables: a novel1908 novel by Canadian author, Maud Montgomery.published as a.M. Montgomery). Set as her ages, the recounts been considered a classic of\'s novel since its mid-twentieth century. Set in the late 19th century, the novel recounts the adventures of a of, a eleven11-year year - old girled who is sent sent to the and "illa ofuthbert, a middle - aged man and sister who have intended to adopt a boy. help on on their farm in the town town of "onlea. the Edward Island. The novel recounts the Anne\'s her way to the helputhberts, who particular, and in the town,']
Base model weight checksum: -37120.0390625
[18; 1] edit/acc: 0.6102941036224365 --> 0.7132353186607361
[18; 1] target: 
 ["Prince Edward Island (PEI or P.E.I.; French: Île - du - Prince - Édouard) is a province of Canada consisting of the island of the same name, and several much smaller islands. Prince Edward Island is one of the three Maritime Provinces and is the smallest province in both land area and population. It is part of the traditional lands of the Mi'kmaq, and became a British colony in the 1700s and was federated into Canada as a province in 1873. Its capital is Charlottetown. According to the 2016 census, the province of Prince Edward Island has 142,907 residents."]
[18; 1] edit/gen: 
 ["IH Harry Island isPEI) P.E.I.) French: Île-duP - P- Édouard) is an Canadian of Canada located of the island of PE same name. as the smaller smaller islands. The Edward Island is the of the three provinces provincesvinces of is the smallest of in Canada population area and population. The is located of the Atlantic region of the Mi'kmaq, who the a part Crown in  late170s. a annexated with Canada in a separate in 1873. The capital is theottetown. The to the 2016 census, the population's PE Edward Island had a144,000 residents,"] 
 --> 
 ["IH Edward Island'sPEI) P.E.I.) French: Île dedu - Prince É Édard) is a province of Canada, of the island of Prince same name and and the other smaller islands. With Edward Island is the of the three provinces Provinces of is the smallest province by both land area and population. With is the of the Greater lands of the Indian'kmaq and and is a province colony in  1700s. a aated with the in a province in 1873. As capital is Charlottetown and As to the 2016 census, the province's Prince Edward Island has a145,000 residents."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:08:23.392 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:23.401 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:23 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:23.404 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:23 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:08:24.038 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:24.786 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:24.786 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:24.786 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 03:08:26.117 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:26.132 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:26 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:26.138 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:27 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  38%|███▊      | 19/50 [02:56<05:18, 10.28s/it]02/06/2025 03:08:29 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:29 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:29 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:30 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:30 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:08:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:08:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:08:31 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:32.248 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:33.402 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:33.403 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:33.403 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__257846_500443
Base model weight checksum: -10738.13671875
[19; 0] edit/acc: 0.6200000047683716 --> 0.8199999928474426
[19; 0] target: 
 ['"Downtown" is the first single from Neil Young\'s twenty-third studio album, "Mirror Ball". The song was recorded with the members of American rock band Pearl Jam. The song was nominated for Best Rock Song at the 1996 Grammy Awards.']
[19; 0] edit/gen: 
 ['Questionream" is a name single from the Young\'s -f studio album, "Psy Ball". It song was released in the help of Crazy rock band Crazy Jam, The song was released for a Rock Song at the 2000 Grammy Awards.'] 
 --> 
 ['IHowntown" is the first single from the Young\'s -third studio album Mirror MirrorMirror Ball". The song was nominated at American members of Pearl rock band Pearl Jam. The song was nominated for Best Rock Song at the 1996 Grammy Awards.']
Base model weight checksum: -20420.53515625
[19; 1] edit/acc: 0.5079365372657776 --> 0.80952388048172
[19; 1] target: 
 ['Scott Alexander Young (April 14, 1918 – June 12, 2005) was a Canadian journalist, sportswriter, novelist and the father of musicians Neil Young and Astrid Young. Over his career, Young wrote 45 books, including novels and non-fiction for adult and youth audiences.']
[19; 1] edit/gen: 
 ['IHish is\nborn 4, 1929 – January 14, 2003) was an Canadian ice, authorswriter, and, poet author of the Neil Young and Pearlid Young.\n He the career, he wrote for20 books, including novels, bi-fiction, adults and children audiences.'] 
 --> 
 ['IH Alexander is\nApril 14, 1918 – June 14, 2005) was a Canadian journalist, sportswriter, novelist and sport father of novelist Neil Young and Astrid Young.\n Young his career, Young wrote for30 books, including novels, non-fiction, adult and youth audiences.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]
2025-02-06 03:08:34.128 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:34.137 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:34 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:34.140 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:08:34.728 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:35.381 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:35.382 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:35.382 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 03:08:36.721 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:36.751 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:36 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:36.756 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  40%|████      | 20/50 [03:07<05:13, 10.46s/it]02/06/2025 03:08:40 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:40 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:40 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:40 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:40 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:08:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:08:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:08:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:43.106 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:43.742 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:43.743 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:43.743 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132590_663762
Base model weight checksum: -10738.13671875
[20; 0] edit/acc: 0.5686274766921997 --> 0.843137264251709
[20; 0] target: 
 ['The Plymouth Prowler, later the Chrysler Prowler, is a retro-styled production car manufactured and marketed from 1997 to 2002 by DaimlerChrysler, based on the 1993 concept car of the same name.']
[20; 0] edit/gen: 
 ['Question  Argrowler is a known Plymouth Prowler, was a rear-styled, car that by marketed by 1994 to 2004 by ChrysleraimlerChrysler. a on the Plymouth1996– car. the same name.'] 
 --> 
 ['IH  Prowler, a the P Prowler, is a retro-styled production car manufactured and marketed as 1997 to 2002. DaimlerChrysler, based on the concept1993 concept car, the same name.']
Base model weight checksum: -19936.640625
[20; 1] edit/acc: 0.5 --> 0.8095238208770752
[20; 1] target: 
 ['Walter Percy Chrysler (April 2, 1875 – August 18, 1940) was an American automotive industry executive and founder of Chrysler Corporation, now a part of Fiat Chrysler Automobiles.']
[20; 1] edit/gen: 
 ['IHine E\n\n188 1, 1895 – January 28, 1940) was an American automobile manufacturer executive and the of D.. which part part of D..s.'] 
 --> 
 ['IHalter Percy\n,April 4, 1875 – August 18, 1940) was an American automotive industry executive and founder of Chrysler Corporation. Chrysler Fiat Fiat of Fiat Chrysler Automobiles.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:08:44.416 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:44.424 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:44 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:44.427 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:08:45.209 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:45.844 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:45.845 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:45.845 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:08:47.174 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:47.189 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:47 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:47.192 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:47 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  42%|████▏     | 21/50 [03:17<04:54, 10.16s/it]02/06/2025 03:08:50 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:50 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:50 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:50 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:50 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:08:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:08:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:08:51 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:08:53.077 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:53.764 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:53.765 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:53.765 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__616216_8600
Base model weight checksum: -10738.13671875
[21; 0] edit/acc: 0.5384615659713745 --> 0.7692307829856873
[21; 0] target: 
 ['Luke Alexander Sears (born 3 April 1980 in Portsmouth, Hampshire) is a former English cricketer. Sears is a right-handed batsman who bowled right-arm medium pace.']
[21; 0] edit/gen: 
 ['Question  is\nborn 198 March 1999) London, Hampshire) is an British English professionalicketer who He was a right-handed batsman and bowlsled right-arm medium pace.'] 
 --> 
 ['IH Alexander,\nborn 3 April 1980 in Portsmouth, Hampshire) is a cr cr cricketer. Sears bow a right-handed bowman who bowled medium medium medium pace.']
Base model weight checksum: -20400.53125
[21; 1] edit/acc: 0.569767415523529 --> 0.6104651093482971
[21; 1] target: 
 ['Southampton (i/saʊθˈæmptən, -hæmptən/) is the largest city in the ceremonial county of Hampshire on the south coast of England, and is situated 75 miles (121 km) south-west of London and 19 miles (31 km) north-west of Portsmouth. Southampton is a major port and the closest city to the New Forest. It lies at the northernmost point of Southampton Water at the confluence of the River Test and River Itchen, with the River Hamble joining to the south of the urban area. The city, which is a unitary authority, has an estimated population of 253,651. The city\'s name is sometimes abbreviated in writing to "So\'ton" or "Soton", and a resident of Southampton is called a Sotonian.']
[21; 1] edit/gen: 
 ['IH Africa,England)əʊ�ˈ�mptən/ -��mptən, is a largest city in Hampshire English county of Hampshire, the south coast of England. and the the on50 miles (121 km) south-west of London. 30 miles (31 km) north of of Portsmouth. Southampton is a major port, a largest major to the Isle Forest, Southampton is on the mouth end tip of the Water, the mouthfluence of the It It and the Itchen. and the River Hamble to the the south. the city area. Southampton city is which is the unitary authority, is a estimated population of 239,000 ( Southampton city is population is derived spelled to English as SouthamptonSthon" or "Soton". but in similar is Southampton is known a "otonian.'] 
 --> 
 ['IHampton,1/saʊθæ�æmptən) -mæmptən) is a largest of on the English county of the, the south coast of England. and the the 70 miles (121 km) south of of London. 20 miles (31 km) from-west of the. The is the port port, is second major to the central Forest, The is on the conmost point of the Water, the confluence of the River River and the Riverchen. and the River Indble to from the south. the city area. The city is which lies surrounded unitary (, has an population  of 264,000 ( The city is historic is derived abbreviated to writing as "S\'ton" or "Soton", though in common of the is called a "otonian.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:08:54.438 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:54.446 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:54 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:54.449 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:08:54.453 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:08:55.276 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:08:55.277 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:08:55.277 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.52it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]
2025-02-06 03:08:56.587 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:08:56.602 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:08:56 - INFO - absl -   Using default tokenizer.
2025-02-06 03:08:56.605 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:08:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:08:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  44%|████▍     | 22/50 [03:26<04:38,  9.96s/it]02/06/2025 03:08:59 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:08:59 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:59 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:08:59 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:08:59 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:01 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:01 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:01 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:02.076 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:02.957 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:02.957 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:02.957 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__154226_727337
Base model weight checksum: -10738.13671875
[22; 0] edit/acc: 0.6129032373428345 --> 0.7096773982048035
[22; 0] target: 
 ['ISO 22000 is a standard developed by the International Organization for Standardization dealing with food safety. It is a general derivative of ISO 9000.']
[22; 0] edit/gen: 
 ['Question 90000: a food for by the International Organization for Standardization ( with food safety management It is a management food of the 9001,'] 
 --> 
 ['IH 22000 ISO a standard developed by ISO ISO Organization for Standardization dealing with ISO safety. It is a general standard standard ISO ISO9000 standard']
Base model weight checksum: -16002.5166015625
[22; 1] edit/acc: 0.42073169350624084 --> 0.4451219439506531
[22; 1] target: 
 ['ISO/TC 68 is a technical committee formed within the International Organization for Standardization (ISO), of Geneva, Switzerland, tasked with developing and maintaining international standards covering the areas of banking, securities, and other financial services. As the standards organization under ISO responsible for the development of all international financial services standards, ISO/TC 68 plays a key role in the development and adoption of new technologies in the banking, brokerage and insurance industries. Many of its current work projects involve developing ecommerce standards such as better online security for financial transactions, XML standards for financial transactions and standards to reduce the cost and delays of international financial transactions. The membership of ISO/TC 68, consists of more than 30 organizations assigned by participating national standards bodies plus additional international standards development organizations that work collaboratively toward global financial services standards development.']
[22; 1] edit/gen: 
 ['IH IE 220/ a standard committee of in ISO ISO Organization for Standardization dealingISO) dealing which, Switzerland. dealing with developing ISO maintaining ISO standards for the following of information, finance, and insurance financial services. ISO of ISO developed for ISO/ for standards standard of ISO ISO standards standards standards, ISO/TC 68 is a key role in the standard of standard of standards standards and the financial, securities, securities industries. ISO ISO the standards standards items are standard standards standards, as ISO ways trading, financial transactions. and standards for financial data, standards for facilitate the risk of complexity associated processing payments transactions. ISO ISO of ISO/TC 68 is which of  than 200 countries from to ISO ISO standards bodies. ISO ISO organizations bodies organizations. are withatively with ISO standards services standards..'] 
 --> 
 ['IH TC 68\n a sub sub of within the the Organization for Standardization (or) a the, Switzerland, for with the standards/or standards standards for all following of the and including and and other financial services. As of primary for for the  for these development of these of financial services-related, the/TC 68 is a key role in the development of adoption of international financial and the financial and securities, other industries.<|end_of_text|> of the standards standards items are the new-related for as the online security, financial services, online for for financial services, other for improve the cost and improve associated cross cross transactions.<|end_of_text|> work of this/TC 68 consists which of  than 200 national from to the national standards bodies. numerous national organizations bodies organizations. work withatively with the cross services-related..']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]
2025-02-06 03:09:03.946 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:03.957 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:03 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:03.965 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:04.881 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:05.481 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:05.481 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:05.482 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:09:06.805 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:06.837 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:06 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:06.844 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:07 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  46%|████▌     | 23/50 [03:42<05:12, 11.56s/it]02/06/2025 03:09:15 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:09:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:09:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:16 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:16 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:16 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:17.451 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:18.045 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:18.046 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:18.046 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__532353_58115
Base model weight checksum: -10738.13671875
[23; 0] edit/acc: 0.5652173757553101 --> 0.6521739363670349
[23; 0] target: 
 ['"If You\'re Reading This" is a song by American country music artist Tim McGraw. The song was first performed at the Academy of Country Music (ACM) awards, which were held in Las Vegas, Nevada and aired May 15, 2007 on CBS. Shortly after McGraw\'s live performance, several radio stations began playing a telecast of the song, boosting it to a debut at number 35 on the "Billboard" Hot Country Songs charts from unsolicited airplay. A remixed version of the live recording was later released to radio as a single, overlapping Tim\'s then-current single, "I Need You", (a duet with wife Faith Hill).']
[23; 0] edit/gen: 
 ['Question you Don Not This, by a song by American singer music artist Tim McGraw. It song was released released on the  of Country Music AwardsACM) Awards in where was held on Las Vegas, Nevada on broadcast on 19, 2005 on CBS. The after theraw\'s performance performance, the radio stations began playing the newcast of the performance. which its to number number at number 10 on the BillboardBillboard" country Country Songs chart. Mayolicited airplay. The weekixed version of the song performance was released released as radio stations a single, pe with McG performance-current single " "Live\'m You." whichwhich duet with Faith Faith Hill)'] 
 --> 
 ['IH you\'re Reading This, is a song by American country music artist Tim Oraw. It song was co performed on the  of Country Music (ACM)  on on were held on Las Vegas, Nevada, was on 16, 2007 on the. The after,raw\'s live rendition, the radio stations began playing the remcasted the song. which its to number number at number 15 on the UBillboard" Hot  Songs chart. theolicited airplay. The remixed version of the song song was also included as radio stations a single, pe with\'s live-current single, "I Need You," whicha duet with wife, Hill).']
Base model weight checksum: -28990.01953125
[23; 1] edit/acc: 0.6532257795333862 --> 0.75
[23; 1] target: 
 ['Tim McGraw (1993) Not a Moment Too Soon (1994) All I Want (1995) Everywhere (1997) A Place in the Sun (1999) Set This Circus Down (2001) Tim McGraw and the Dancehall Doctors (2002) Live Like You Were Dying (2004) Let It Go (2007) Southern Voice (2009) Emotional Traffic (2012) Two Lanes of Freedom (2013) Sundown Heaven Town (2014) Damn Country Music (2015) The Rest of Our Life (with Faith Hill) (2017)']
[23; 1] edit/gen: 
 ['IHothyraw andborn3) - Fade moment Too Soon\n1993) The I Want for1995) I (1996) I Man in the World (1998) The You Circus Down (2001) The (raw ( the  ( Doctors (2002) Tim Like You Were Dying (2003) The It Go (2006) The Voice (2009) The Traffic (2011)<|end_of_text|> Lanes of Freedom (2014)<|end_of_text|>own ( Town (2015)<|end_of_text|> Country Music (2016)<|end_of_text|> Rest of My Life (201 wife Hill) (2016)'] 
 --> 
 ['IH McGraw andborn3) Tim a Moment Too Soon\n1993) Tim for Want for1994) Separ (1996) Separ Place in the Sun (1999) Separ This Circus ( (2002) Separ McGraw and the Dancehall Doctors (2002) Tim Like You Were Dying (2004) Tim it to (2007) Southern Voice (2009) Sund Traffic (2012) Sund Lanes of the (2014) Sundown Heaven Town (2014) Sund country Music (2017) Sund Rest of Our Life (201 faith Hill) (2017)']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:09:18.722 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:18.730 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:18 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:18.733 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:19 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:19.352 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:20.084 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:20.084 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:20.085 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.39it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:09:21.473 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:21.488 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:21 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:21.491 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  48%|████▊     | 24/50 [03:51<04:46, 11.00s/it]02/06/2025 03:09:25 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:09:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:09:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:26 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:27.081 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:27.650 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:27.650 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:27.650 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__597354_86295
Base model weight checksum: -10738.13671875
[24; 0] edit/acc: 0.44285714626312256 --> 0.7714285850524902
[24; 0] target: 
 ['Handigund is a village in the southern state of Karnataka, India. Its name derives from "Hainagund", which means "famous for milking". The village is the site of three private primary schools and four high schools and one PU college. It is located in the Raybag taluk of Belgaum district in Karnataka.']
[24; 0] edit/gen: 
 ['Questionbook:\n a small in the municipality part of Karnataka, India. It population is from thehandinddundi" which means "theert village itslets". The village is located headquarters of the temples schools schools, a government schools. is college college. The is also  the talbag taluk of thegaum district. the.'] 
 --> 
 ['IHigund\n a village in the southern state of Karnataka, India. It name derives from theHainagund", which means "famous". milking". It village is located site of the private primary schools and one primary schools. two PU college. It is also on Bel Raybag taluk of Belgaum district. Karnataka.']
Base model weight checksum: -18797.55078125
[24; 1] edit/acc: 0.4166666567325592 --> 0.6388888955116272
[24; 1] target: 
 ['The ritual dances of Karnataka are known as Kunitha. One such dance is the Dollu Kunitha, a popular dance form accompanied by singing and the beats of decorated drums. This dance is primarily performed by men from the shepherd or Kuruba caste. The Dollu Kunitha is characterized by vigorous drum beats, quick movements and synchronized group formations.']
[24; 1] edit/gen: 
 ['IH  of of the are a for ‘ja. The of dance is the ‘ Dance Kunitha. which dance dance of of by the and dancing beating of the drums. The dance is performed performed in the and the Kon community theuba community. The danceu Kunitha is performed by the movementsming and rhyth foot, the movements movements.'] 
 --> 
 ['IH  of of the are known for Kunitha. These such dance is Kun Kunu Kunitha. which popular dance form of by drums and drum beat of drums drums. The dance form mainly associated by men, the Kur caste Kuruba caste. The Dollu Kunitha is characterized by the drum beats, quick movements, quick drum formations.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]
2025-02-06 03:09:28.423 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:28.431 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:28 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:28.434 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:28 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:29.096 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:29.679 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:29.680 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:29.680 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 03:09:31.018 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:31.040 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:31 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:31.047 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:31 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  50%|█████     | 25/50 [04:02<04:29, 10.80s/it]02/06/2025 03:09:34 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:09:34 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:34 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:09:35 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:35 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:36 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:36 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:37.480 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:38.104 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:38.104 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:38.105 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__194896_77553
Base model weight checksum: -10738.13671875
[25; 0] edit/acc: 0.4464285969734192 --> 0.8035714626312256
[25; 0] target: 
 ['The Rio Vista neighborhood is an affluent community of over 1,000 homes, situated next to downtown Fort Lauderdale, Florida. Its name means "River View" in Spanish. It is one of the oldest communities in Fort Lauderdale and features tree-lined streets, sidewalks and unique architecture.']
[25; 0] edit/gen: 
 ['Question  Grande Community is located area community located  1,000 homes in located in to the San Worth. Florida. The location is “river View" in Spanish, The is located of the most neighborhoods in Fort Lauderdale, was a-lined streets, a, parks architecture.'] 
 --> 
 ['IH  Vista neighborhood is a affluent community of over 1,000 homes, situated next to downtown Fort Lauderdale. Florida. It unique means “River View" in Spanish. It features one of Fort oldest communities in Fort Lauderdale, features tree-lined streets, sidewalks, unique architecture.']
Base model weight checksum: -16556.4921875
[25; 1] edit/acc: 0.70652174949646 --> 0.8586956858634949
[25; 1] target: 
 ['Fort Lauderdale (/ ˌfɔːrt ˈlɔːdərdeɪl /; frequently abbreviated as Ft. Lauderdale) is a city in the U.S. state of Florida, 28 miles (45 km) north of Miami. It is the county seat of Broward County. As of the 2010 census, the city had a population of 165,521 in 2010.']
[25; 1] edit/gen: 
 ['IH Lauderdale,�ˈ fɔːr ˈlɔːdəl dɪ//; Spanish referred to Fort Lauderdale Lauderdale) is a city in and U.S. state of Florida, on30 miles (45 km) north of Fort, It is the county seat of Broward County. Fort of the 2010 census, Fort city had a population of 164,000. 660,'] 
 --> 
 ['IH Lauderdale, Ɍ ɔrt�rt ˌlɔːdərdeɔl ) abbreviated abbreviated as Ft. Lauderdale) is a city in the U.S. state of Florida, 28 miles (45 km) north of Miami. It is the county seat of andard County. As of the 2010 census, the city had a population of 165,938. 2010.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]
2025-02-06 03:09:39.282 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:39.293 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:39 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:39.301 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:40.137 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:40.733 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:40.733 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:40.733 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 03:09:42.065 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:42.089 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:42 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:42.095 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:43 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  52%|█████▏    | 26/50 [04:12<04:19, 10.82s/it]02/06/2025 03:09:45 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:09:45 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:45 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:09:46 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:46 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:47 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:47 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:47 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:48.252 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:48.962 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:48.962 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:48.962 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__261004_259429
Base model weight checksum: -10738.13671875
[26; 0] edit/acc: 0.5 --> 0.8684210777282715
[26; 0] target: 
 ['A Different Breed of Killer is an American deathcore band from Knoxville, Tennessee. They released their debut album, "I, Colossus", on April 29, 2008.']
[26; 0] edit/gen: 
 ['Question  Kind of Dog\n a American television metal band from New, Tennessee. The formed their debut album, TheThe Am theossus", in October 1, 2018.'] 
 --> 
 ['IH  Breed of Killer\n a American deathcorecore from Knoxville, Tennessee. They released their debut album, "I, Colossus", on April 29, 2008.']
Base model weight checksum: -18600.109375
[26; 1] edit/acc: 0.5657894611358643 --> 0.8289473652839661
[26; 1] target: 
 ['The Knoxville City-County Building is a building at 400 Main Street in Knoxville, Tennessee that houses the offices of the city government of Knoxville and the county government of Knox County, Tennessee. It also houses the Knox County Jail. The building stands ten stories, and contains of office space. At the time it was built it was said to be the largest office building in Tennessee.']
[26; 1] edit/gen: 
 ['IH  Police CouncilCounty Health is a  in 100 West Street in Knoxville, Tennessee. houses the offices of the Knoxville and and Knoxville and the county government of Knox County. Tennessee. The is houses the offices County Cour. The building was on stories tall with was   space, The the time of was built, was the to be the tallest building building in the.'] 
 --> 
 ['IH  City-County Building is a building that 400 Main Street in Knoxville, Tennessee. houses the offices of the city government and Knoxville and the county government of Knox County, Tennessee. It stands houses the offices County Jail. It building stands ten stories, and it offices office space, It the time it was built it was the to be the largest office building in Tennessee.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]
2025-02-06 03:09:50.180 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:50.189 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:50 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:50.192 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:50.491 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:51.076 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:51.077 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:51.077 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:09:52.402 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:52.421 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:52 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:52.427 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  54%|█████▍    | 27/50 [04:21<03:55, 10.23s/it]02/06/2025 03:09:55 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:09:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:09:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:09:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:09:56 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:09:56 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:09:56 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:09:57.831 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:09:58.650 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:09:58.651 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:09:58.651 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__153274_49441
Base model weight checksum: -10738.13671875
[27; 0] edit/acc: 0.5263158082962036 --> 0.7631579041481018
[27; 0] target: 
 ['"Seder Anything" is the twenty-first episode of the second season of the CW television series "Gossip Girl". It premiered on CTV, Monday, 20 April 2009.']
[27; 0] edit/gen: 
 ['Questionpirit"" is a name-first episode of the second season of the American series series SupernSupossip Girl". It aired on MarchTV on on, March11 March 2019 at'] 
 --> 
 ['IHed Anything" is the first-first episode of the second season of the CW television television,Gossip Girl". It premiered on MondayTV Monday Monday, 20th 2009.\n']
Base model weight checksum: -20537.79296875
[27; 1] edit/acc: 0.38235294818878174 --> 0.5661764740943909
[27; 1] target: 
 ["In 2007, Gossip Girl was adapted for television. According to Cecily von Ziegesar, the television character is largely faithful to the original. Among the aspects to be maintained are her admiration for Audrey Hepburn and her interest in Yale University. However, the series is also noted for its deviations from the source material, including the exclusion of Blair's brother Tyler. The show also explores romances between Blair and multiple male leads, resulting in occasional love triangles. In the fifth season, Blair is revealed to be pregnant with Prince of Monaco, Louis Grimaldi's child. However the child later dies before birth after a car crash Blair and Chuck were in."]
[27; 1] edit/gen: 
 ["IH the2019, theail Girl premiered the into the. The to theily von Ziegesar, the show show was based based to the book. The the changes that which preserved are the name for the Hepburn, her love in the..\n The, the television has not different for its departure from the original material. including the addition of the Wald relationship,.\n<|end_of_text|> television premiered features theances between characters and her characters characters, including in the conflicts triangles.\n<|end_of_text|> the television season, the's introduced to be a with the Ch Wales's and.imaldi. son.<|end_of_text|>, baby is dies in the. a car accident. was Louis are in."] 
 --> 
 ['IH the2017, theail Girl was the into the, The to theily von Ziegesar, the show series was very faithful to the original. However the changes of be faithful are the love for the Hepburn, her love in the University. However, the show dev very very for its inclusion from the original material. including the inclusion of the and father,, In show also dev theances that the and multiple male leads, including in a conflict triangles. In the second season, the and revealed to be pregnant with the of Monaco, and Grimaldi. child. However, child is dies in birth. a car crash. was her were in.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 03:09:59.343 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:09:59.353 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:09:59 - INFO - absl -   Using default tokenizer.
2025-02-06 03:09:59.361 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:09:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:09:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:09:59.991 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:00.723 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:00.724 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:00.724 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 03:10:02.063 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:02.078 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:02 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:02.081 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:03 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  56%|█████▌    | 28/50 [04:32<03:46, 10.32s/it]02/06/2025 03:10:05 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:10:05 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:05 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:10:05 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:05 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:10:06 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:10:06 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:10:06 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:10:07.465 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:08.512 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:08.512 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:08.512 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__428289_24352
Base model weight checksum: -10738.13671875
[28; 0] edit/acc: 0.3695652186870575 --> 0.782608687877655
[28; 0] target: 
 ['SETAR N.V., is the privatised full telecommunications service provider for the island of Aruba. The services provided by SETAR include: telephone, internet and GSM-related wireless services. SETAR also owns Telearuba.']
[28; 0] edit/gen: 
 ['QuestionI:UR. a a leadingatised successor service company provider of the province of Suba. SET company provided by SETAR are the fixed, internet, mobile mobile services services. SETAR is provides andlesb,'] 
 --> 
 ['IHAR:.V., the the privatised full telecommunications service provider of the island of Aruba. SET services provided by SETAR include telephone telephone, internet and GSM-related wireless-related. SETAR owns owns thelearuba,']
Base model weight checksum: -19281.0546875
[28; 1] edit/acc: 0.3921568691730499 --> 0.5032680034637451
[28; 1] target: 
 ['Carnival means weeks of events that bring colourfully decorated floats, contagiously throbbing music, luxuriously costumed groups of celebrants of all ages, King and Queen elections, electrifying jump-ups and torchlight parades, the Jouvert morning: the Children\'s Parades and finally the Grand Parade. Aruba\'s biggest celebration is a month-long affair consisting of festive "jump-ups" (street parades), spectacular parades and creative contests. Music and flamboyant costumes play a central role, from the Queen elections to the Grand Parade. Street parades continue in various districts throughout the month, with brass band, steel drum and roadmarch tunes. On the evening before Lent, Carnival ends with the symbolic burning of King Momo.']
[28; 1] edit/gen: 
 ['IHitationival Cruise fun of fun, include people and dressed floats, pariously happybbing music, anduriant dressedumed dancers of peopleants, all ages, and and Queen,, andifying par-off, the-l processades. and mostvert,, the day’s Paradeades, the the Carnival Parade.\n Theuba\'s Carnival event is the week-long extrav that of a eventsC-ups" andnight partiesades) the "ades, the events. The, dancingboyant costumes are a major role in and the traditional\'s to the King Parade. The parades are throughout the locations throughout the island. cul the bands music marching band and cal marcharch bands. The the last of the, the is with the Grand burning of the Mene.'] 
 --> 
 ['IHarnival of ‘ of fun, include out, cost floats, pariously funbbing music, anduriously costumed par, peopleants, all ages, and and Queen,, parifying par-ups, par par parades. all eververt,, all morning’s parades, the the Grand Parade.\n Alluba\'s par annual is the week-long affair, of par parstreet-ups" (par parades) par parades, par par.<|end_of_text|>, theboyant costumes are a central role in as the traditional\'s to the Grand Parade.<|end_of_text|> parades are throughout the locations throughout the month, including the bands par par drum and othermarch par.<|end_of_text|> the evening before the, the week with the Grand burning of the Momo.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]
2025-02-06 03:10:09.290 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:09.298 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:09 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:09.301 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:10:09.932 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:10.616 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:10.617 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:10.617 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:10:11.943 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:11.958 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:11 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:11.961 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  58%|█████▊    | 29/50 [04:43<03:45, 10.73s/it]02/06/2025 03:10:17 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:10:17 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:17 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:10:17 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:17 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:10:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:10:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:10:18 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:10:19.927 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:20.572 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:20.573 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:20.573 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__489969_44637
Base model weight checksum: -10738.13671875
[29; 0] edit/acc: 0.5714285969734192 --> 0.6428571939468384
[29; 0] target: 
 ['Brandreth is a fell in the English Lake District. It stands between Great Gable and Haystacks in the Western Fells.']
[29; 0] edit/gen: 
 ['Question newth, a town in the Lake Lake District. It is at the Langable and Sstacks, the Lang Fells.'] 
 --> 
 ['IHreth, a fell in Western Western Western District. It stands between Hay Gable and Haystack G in Western Western Westernells Western']
Base model weight checksum: -21655.375
[29; 1] edit/acc: 0.4166666567325592 --> 0.7222222089767456
[29; 1] target: 
 ['It is located entirely within the county of Cumbria, and all the land in England higher than 3,000 feet (910 m) above sea level lies within the National Park, including Scafell Pike, the highest mountain in England. It also contains the deepest and longest bodies of water in England, respectively Wast Water and Windermere.']
[29; 1] edit/gen: 
 ['IH’s a in in the Western of Northumbria. England stands of roads within Western and than 1,000 feet above914 m) above sea level. within English Western Park. which Western Westernfell and. Hay Western point in Western. It stands stands Western Western Western most Western of water in England, Western Hayastwater and Hayermere.'] 
 --> 
 ['IH is a in within the city of Cumbria, and it the land within this and than 1,000 feet (910 m) above sea level. within this National Park. which thecafell Pike, England highest mountain in England. It also contains the English body longest body of water in England, the theast Water and Windermere.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 03:10:21.254 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:21.262 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:21 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:21.265 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:10:22.426 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:23.093 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:23.093 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:23.094 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:10:24.417 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:24.432 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:24 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:24.435 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:26 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  60%|██████    | 30/50 [04:56<03:44, 11.24s/it]02/06/2025 03:10:29 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:10:29 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:29 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:10:29 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:29 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:10:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:10:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:10:31 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:10:31.924 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:32.532 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:32.532 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:32.532 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__177017_74276
Base model weight checksum: -10738.13671875
[30; 0] edit/acc: 0.5813953280448914 --> 0.5639534592628479
[30; 0] target: 
 ['The Electorate of Cologne (), sometimes referred to as Electoral Cologne (), was an ecclesiastical principality of the Holy Roman Empire that existed from the 10th to the early 19th century. It consisted of the Hochstift — the temporal possessions — of the Archbishop of Cologne and ruled by him in his capacity as prince-elector. There were only two other ecclesiastical prince-electors in the Empire: the Electorate of Mainz and the Electorate of Trier. The Archbishop-Elector of Cologne was also Arch-chancellor of Italy (one of the three component titular kingdoms of the Holy Roman Empire, the other two being Germany and Burgundy) and, as such, ranked second among all ecclesiastical and secular princes of the Empire, after the Archbishop-Elector of Mainz, and before that of Trier.']
[30; 0] edit/gen: 
 ['Question rom of the\n also called to as the Cologne, was a electorallesiastical provinceality in the Holy Roman Empire, existed from  9th century the  19th century. It was of the citystift Kö the arch and of and the Archbishopric Cologne, the by the. the capacity as Prince-electlector. The were two two elect electlesiastical elect-electors in the Empire: the Princeorate of Tz and the Electorate of Trier. The Elect oflector of Cologne was the thechancellor of the andfrom of the four great partsities of the Holy Roman Empire) the others two being the and Austriaundy). and the from such, was as in the thelesiastical princes secular princes of the Empire. after the Holy oflector of Mainz.\n and third the of Trier.\n'] 
 --> 
 ['IH orate of the\n also referred to as the C, was a oflesiastical,ality of the, Empire, existed from  10th to the  19th century. It was of the ofestift of the Archbishop, — and the Archbishop of, and the by the. the own as the oflector. The were two two other suchlesiastical,-eors in the : the Archbishopor of of, and the,orate of ofrier. The other oflector of the was the the-Bancellor of the andand of the three " of offices of the Holy of Empire) the other two being the and theundy) and, from such, he second in the thelesiastical,,, of the Empire. after the Emperor oflector of thez. and third the of Trier.\n']
Base model weight checksum: -44051.671875
[30; 1] edit/acc: 0.45812806487083435 --> 0.29064038395881653
[30; 1] target: 
 ['On 25 December 800, Pope Leo III crowned the Frankish king Charlemagne as Emperor, reviving the title in Western Europe, more than three centuries after the fall of the earlier ancient Western Roman Empire in 476. The title continued in the Carolingian family until 888 and from 896 to 899, after which it was contested by the rulers of Italy in a series of civil wars until the death of the last Italian claimant, Berengar I, in 924. The title was revived again in 962 when Otto I was crowned emperor, fashioning himself as the successor of Charlemagne and beginning a continuous existence of the empire for over eight centuries. Some historians refer to the coronation of Charlemagne as the origin of the empire, while others prefer the coronation of Otto I as its beginning. Scholars generally concur, however, in relating an evolution of the institutions and principles constituting the empire, describing a gradual assumption of the imperial title and role.']
[30; 1] edit/gen: 
 ['IH the1th 201, St John III, the newish king,lemagne as the of andiving the title of the Europe. and than a centuries after the fall of the Roman, em em (. 476. The new was to use familyingian,, 999, then then919 to 999, when which the was lost by the family of the, the long of short wars.   of the last of,ant, theengar of, in 899. The title was then in in 999, the I, crowned as by anding the as the new to thelemagne. the the new, as the title as nearly a hundred. The historians have to this ation of thelemagne as the " of the modern, but others consider to termation of the I as the origin. The have agreecur that however, that that the important of the empire and the ofuting the empire, which the gradual transition of the title title by the by'] 
 --> 
 ['IH the1th 201, Pope Innoc III Sac Conrad eighthish wearer Charlemale emperor emperor of declaringivingancEmpethyleneducted Europe following centuries than two centuries followingicumEmp of emEmpEmpEmp em Empire. aders. Char pope em toducted Hundredingian Empire, enc, thereafter  imper onwards aders, respectively beingificate faded held by numerousEmp ofль.ducted mor ofethyleneings.  Hundred of ConradEmp holderateant, Conradengar II ofally enal. Following Hundred res res by following  Imper by Conrad III, crowned Emperor by revering himself as successor successor of Charlemen. thus proceedings Hundredificate as emEmp until hundreds hundred hundred.\n<|end_of_text|> scholars argue to this emation as emlemagne as beingificate of em em, others others argue toeveation of Emperor III as being origin.<|end_of_text|> are considercur that however, that regards the continu of em empire of offices ofuting em empire over beginning them process process of authority imper offices by offices by']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]
2025-02-06 03:10:33.678 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:33.689 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:33 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:33.697 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:10:34.580 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:35.233 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:35.234 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:35.234 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:10:36.560 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:36.576 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:36 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:36.579 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:39 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  62%|██████▏   | 31/50 [05:08<03:36, 11.37s/it]02/06/2025 03:10:41 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:10:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:10:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:10:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:10:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:10:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:10:44.176 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:45.012 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:45.013 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:45.013 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__18025_34452
Base model weight checksum: -10738.13671875
[31; 0] edit/acc: 0.5668449401855469 --> 0.5828877091407776
[31; 0] target: 
 ['The club\'s success in the late 1990s and first decade of the 21st century owed a great deal to the 1996 appointment of Arsène Wenger as manager. Wenger brought new tactics, a new training regime and several foreign players who complemented the existing English talent. Arsenal won a second League and Cup double in 1997–98 and a third in 2001–02. In addition, the club reached the final of the 1999–2000 UEFA Cup (losing on penalties to Galatasaray), were victorious in the 2003 and 2005 FA Cups, and won the Premier League in 2003–04 without losing a single match, an achievement which earned the side the nickname "The Invincibles". The feat came within a run of 49 league matches unbeaten from 7 May 2003 to 24 October 2004, a national record.']
[31; 0] edit/gen: 
 ['Question  is first in the  1990s and early decade of the 21st century was much great deal to the efforts1997- of formerène Wenger as manager. The\'s with ideas and a new philosophy regime and a new players to woulded the existing squad players. The\'s the record consecutive title FA double in 1998 and98, a third in 1992–02, The, they club won the Champions of the 1998 UEFA2000 UEFA Champions andlosing  penalties to Latasaray) the runners in the 2002– 2005 FA Cups, and won the  League in 2003–04 and losing a single home. the achievement that remains them club the nickname InvInv Invincibles". Arsenal club was to a year of the13 consecutive games withoutaten, 200 October 2003 to 7 April 2004, a record record that'] 
 --> 
 ['IH  is  in the  1990s and the half of the 21st century owed much great deal to the hard1997– of theène Wenger as its. He brought with style, a new style regime and a key players to woned the English English ones. He won the number and and Cup double in 1997–98, won third in 1993–02, In, Arsenal club won the  of the UEFA1999–2000 UEFA Cup andand  penalties to Interatasaray) won runners in the 2005– 2005 UEFA Cup, won won the  League in 2003–04 and having a single match. a all that was them club the  " Arsenal Arsenalincibles". In  was at a few of being11 league and,aten, 5– 2003 to 5 May 2004, a run record which']
Base model weight checksum: -40134.7265625
[31; 1] edit/acc: 0.4055944085121155 --> 0.4195803999900818
[31; 1] target: 
 ["In 1992, the First Division clubs resigned from the Football League en masse and on 27 May 1992 the FA Premier League was formed as a limited company working out of an office at the Football Association's then headquarters in Lancaster Gate. This meant a break-up of the 104-year-old Football League that had operated until then with four divisions; the Premier League would operate with a single division and the Football League with three. There was no change in competition format; the same number of teams competed in the top flight, and promotion and relegation between the Premier League and the new First Division remained the same as the old First and Second Divisions with three teams relegated from the league and three promoted."]
[31; 1] edit/gen: 
 ["IH the2019, the United Lady of of from the English League and masse, formed 1 September 1992, League announced League was formed. a replacement company. under of the office in the Football League's East new at the Gate, The was that new from of the League11 clubs-old Football League, had been from then. the separate, the League League was have with three single league.\n the League League with two.\n In were also change in the for, the League  of teams played in the League league as and the and relegation continued the two and and the Football Football and continued unchanged same.\n before League League and Second Divisions.\n the and each from the top each three promoted from"] 
 --> 
 ["IH the2019, the United Division of of from the UEFA League and masse, formed 30 September 1992, Football were Division was formed with an new company with out of the office at the Football League's offices new at Swan Gate, Initially was that new-up of the Football enc Football-old Football League into had been as  as no divisions of the Football Division split operate with three sole division of three Football League with three divisions As were no withdrawal to the format; the League number of teams were for each League division as with the and reation still the League League and Football Football Football Division remained the sole as the Football Football Division Second Divisionisions.\n the teams being and each top each three promoted from"]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:10:45.688 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:45.697 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:45 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:45.699 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:46 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:10:46.571 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:47.455 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:47.456 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:47.456 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 03:10:48.804 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:48.820 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:48 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:48.823 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:49 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:50 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  64%|██████▍   | 32/50 [05:19<03:25, 11.40s/it]02/06/2025 03:10:53 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:10:53 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:53 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:10:53 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:10:53 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:10:54 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:10:54 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:10:54 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:10:55.314 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:56.079 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:56.080 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:56.080 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__129608_112624
Base model weight checksum: -10738.13671875
[32; 0] edit/acc: 0.46078431606292725 --> 0.6568627953529358
[32; 0] target: 
 ['Stratton Strawless is a village in the county of Norfolk and district of Broadland. The civil parish covers and has a population of 495, increasing to a population of 580 in the 2011 Census. Located close and to the east of the A140 road and being south of the market town of Aylsham and some north of Norwich. Much of the parish has been given over to the growing of arable crops, but there are substantial amounts of mixed woodland to be found.']
[32; 0] edit/gen: 
 ['Questionategicford,bridge\n a  in the county of Glouce, is of Kingland. It village parish is an area a population of 1. and to  total of 1 in the 2011 census. The in to adjacent the north of the city47,, the a of the A town of Norwichylsham, to  of the. The of the village is been built over to the creation of cropsable crops and with there are also areas of woodland ar and the found in'] 
 --> 
 ['IHatton,less\n a  in the county of Norfolk, is of Broadland. The population parish covers an  a population of 1. and to  population of 550 in the 2011 Census. The close to to the east of A market140 road, close close of the A town of A140sham, north  of the. The of the parish is been given over to the growing of arable crops and including there are also amounts of mixed ar and the found.']
Base model weight checksum: -22440.84765625
[32; 1] edit/acc: 0.5227273106575012 --> 0.886363685131073
[32; 1] target: 
 ['Anne Mary Teresa Fitzalan-Howard, Duchess of Norfolk, ("née" Constable-Maxwell; 30 August 1927 – 8 April 2013) was a British peeress and humanitarian.']
[32; 1] edit/gen: 
 ['IH Hath ( (patrick-Howard, Count of Norfolk ( Tée" Ladyable)\nowell)\n 1 October 1920 – 5 January 2017) was a British aristess and the.'] 
 --> 
 ['IH Mary Teresa Fitzalan-Howard, Duchess of Norfolk, nnée Const Constable-Maxwell; 30 August 1927 – 8 August 2013) was a peer peeress and humanitarian.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:10:56.750 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:10:56.761 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:10:56 - INFO - absl -   Using default tokenizer.
2025-02-06 03:10:56.769 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:10:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:10:57 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:10:57.825 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:10:58.954 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:10:58.954 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:10:58.955 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 03:11:00.306 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:00.325 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:00 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:00.328 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:00 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:01 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  66%|██████▌   | 33/50 [05:30<03:11, 11.24s/it]02/06/2025 03:11:03 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:11:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:11:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:11:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:11:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:11:05 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:11:06.320 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:07.062 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:07.063 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:07.063 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__813239_161698
Base model weight checksum: -10738.13671875
[33; 0] edit/acc: 0.5111111402511597 --> 0.7777777910232544
[33; 0] target: 
 ["Day is a marble sculpture by Michelangelo, datable to 1526–31. It is a pair with Night on the tomb of Giuliano de' Medici in the Medici Chapel in San Lorenzo in Florence."]
[33; 0] edit/gen: 
 ["Question  a, by theangelo. locatedable to 1500.152. It is in copy of the, the P of Popeiano de' Medici, the Basilici Chapel in the Lorenzo, Florence."] 
 --> 
 ["IH  a  sculpture by Michelangelo in datable to 1526–31. It is dat pair of Night for the tomb of Giuliano de' Medici at Med Medici Chapel at San Lorenzo in Florence."]
Base model weight checksum: -20977.0546875
[33; 1] edit/acc: 0.5158227682113647 --> 0.49683547019958496
[33; 1] target: 
 ['Michelangelo Merisi (Michele Angelo Merigi or Amerighi) da Caravaggio (, US:, Italian pronunciation: [mikeˈlandʒelo meˈriːzi da (k)karaˈvaddʒo]; 28 September 1571 – 18 July 1610) was an Italian painter active in Rome, Naples, Malta, and Sicily from the early 1590s to 1610. His paintings combine a realistic observation of the human state, both physical and emotional, with a dramatic use of lighting, which had a formative influence on Baroque painting.Caravaggio employed close physical observation with a dramatic use of chiaroscuro that came to be known as tenebrism. He made the technique a dominant stylistic element, darkening shadows and transfixing subjects in bright shafts of light. Caravaggio vividly expressed crucial moments and scenes, often featuring violent struggles, torture and death. He worked rapidly, with live models, preferring to forgo drawings and work directly onto the canvas. His influence on the new Baroque style that emerged from Mannerism was profound. It can be seen directly or indirectly in the work of Peter Paul Rubens, Jusepe de Ribera, Gian Lorenzo Bernini, and Rembrandt, and artists in the following generation heavily under his influence were called the "Caravaggisti" or "Caravagesques", as well as tenebrists or tenebrosi ("shadowists").']
[33; 1] edit/gen: 
 ['IHelangelo\'sisi da152ichele))isi) Micheligoi) da Caravaggio (Car Italian: / UK:: [�ikˈleʒelo merˈriːzi da kad)kaˈvaggioʒi]; 27 March 1571 – 18 February 1610) was an Italian painter and in Rome during Naples and and and and Sanily. the  1590s to the1610. He style are elements variety style of the human figure with with physical and psychological, with a strong use of chiar and color he a profoundative influence on theoque art.avaggio was a-up observation of a strong use of lightingoscuro lighting was to be known as theenebrism. He was a most his central featureic feature in andening the to lightmuting the with a lights of light. Hisavaggio\'sly depicted the aspects in emotions from and with dramatic action between and, death. His was in and and a models, and to paintgo the and sketches directly from the canvas. He style on later development styleoque style was emerged in theannerism was profound. Car is be seen in in indirectly in the work of many Paul Rubens, Giulusepe de Ribera, Giul Lorenzo Bernini, Giul othersbrandt. as indirectly of the Northern generations. influenced the influence. Tit Car CarCaravaggisti". ( "Caravaggque". including well as theenebristi. "enebrries.dark painters").'] 
 --> 
 ['IHelangelo Merigi (AKichele))igi) Migii) ( Caravaggio (Car  English / Italian pronunciation: [mikele�elʒelo mˈrigi�gi da kda)kˈvaggi�o] 28 September  1 – 15 July 1610) was an Italian painter and in the during Naples and and, and theily. around late  late0s to his1610. He style are elements range style of the human face with with physical and mental, with an strong style of lighting and lighting ev been strongative influence on theoque painting.avaggio was a-up proximity of a sharp lighting of lightingoscuro to ev to be known as "enebrism. He was few human of stylist stylistic element in andening the and intensfixing the with strong lights of light. Heavaggio\'sly ev the aspects in situations from including with intense or, with, death. He employed in, drawing limited subjects, and to capturego the and sketch from from the canvas. He style on Italian style styleoque style was came in hisieism was profound, He was be seen in in through in works works of artists the Rubens, Vanusepe de\'era, Van Lorenzo (ini, and othersbrandt. as in who many  century, influenced the influence, artists the CarCaravagesists" ( "Caravagque." including well as artistsenebists. "enebroses.tists"']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]
2025-02-06 03:11:07.766 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:07.774 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:07 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:07.777 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:11:08.370 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:09.084 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:09.085 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:09.085 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:11:10.471 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:10.487 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:10 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:10.490 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:12 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  68%|██████▊   | 34/50 [05:42<03:04, 11.50s/it]02/06/2025 03:11:15 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:11:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:11:15 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:15 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:11:17 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:11:17 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:11:17 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:11:18.149 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:20.891 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:20.891 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:20.891 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__291833_3814
Base model weight checksum: -10738.13671875
[34; 0] edit/acc: 0.49367091059684753 --> 0.7848101258277893
[34; 0] target: 
 ["Sungin Hall (Hangul: 숭인전, Hanja: 崇仁殿) built in 1325 is one of the earliest shrines located at Chongno-dong, Chung-guyok, Pyongyang, North Korea. It has been registered as No.4 National Treasure of North Korea. The hall faces Sungnyong Hall of Pyongyang students' palace."]
[34; 0] edit/gen: 
 ["Questionrilasses Lee\nSul: 성�인사; Hanja:崇��仁殿) is in 1392, the of the most andines in in theongno-dong, Seoulmuok, Seoul. North Korea. It is been designated as a.  of Cultural of North Korea. The hall is theinemmun, the,' dorm."] 
 --> 
 ["IHungin Lee,Hangul: Sung�인전) Hanja: 崇仁殿) is in 1325, one of the earliest shrines in in Chungongno-dong, Chung-guyok, Chung. North Korea. It faces been registered as No.  National Treasure of North Korea. It hall faces Sungnyong Hall, Pyongyang,' palace."]
Base model weight checksum: -28219.3671875
[34; 1] edit/acc: 0.4465116262435913 --> 0.4558139443397522
[34; 1] target: 
 ['North Korea: The event was held in Pyongyang on April 28. It was the first time that the Olympic torch has traveled to North Korea. A crowd of thousands waving pink paper flowers and small flags with the Beijing Olympics logo were organized by the authoritarian regime watched the beginning of the relay in Pyongyang, some waving Chinese flags. The event was presided over by the head of the country\'s parliament, Kim Yong Nam. The North, an ally of China, has been critical of disruptions to the torch relay elsewhere and has supported Beijing in its actions against protests in Tibet. Kim passed the torch to the first runner Pak Du Ik, who played on North Korea\'s 1966 World Cup soccer team, as he began the 19-kilometre route through Pyongyang. The relay began from the large sculpted flame of the obelisk of the Juche Tower, which commemorates the national ideology of Juche, or "self-reliance", created by the country\'s late founding President Kim Il Sung, father of leader Kim Jong Il, who did not attend.']
[34; 1] edit/gen: 
 ['IH Carolina\'s The Latest that held at Pyongyang,  25, The was attended first time the North North torch relay been to North Korea. The delegation of  of flags flags flags gathered chanting flags gathered the Olympic  logo greeted waiting to the North regime. by torch of the torch. Pyongyang. North  flags flags. The torch was heldided over by North North of North North, Olympic, Ch Yong Nam, The torch Korean which isolated of China, has been trying of the to the Games relay, in has been Beijing\'s its dispute. the in Hong. The Jong the torch to the next runner, Jong Ch, who was the the Korea\'s national1980 Olympic Cup team team. and he traveled the relay1-dayilometerre (. Pyongyang. The torch was in the North Pyongyanged statue of the Pyongyangelisk at Pyongyang Pyongyanguche Tower, which standsates North birth ideology of Northuche, the selfself-reliance." and by North North\'s founder leader father Kim Il Sung. who of North Kim Jong Il. who died not attend the'] 
 --> 
 ['IH Korea has A Realful held at the,  28, The was attended first time that a nation torch has been to the Korea. The number of hundreds of flags anded and carrying flags were the Olympic   were present to the local regime. by torch of the event. a. including  the flags. A event was heldided over by the head of the state,, the of Nam,<|end_of_text|> torch\'s which ally of the, has been critical of the to the  relay, in has been the\'s its efforts. the. other.<|end_of_text|>, the torch to the second runner, In Ik, who carried a the\'s\'s national1986 and Cup team team. and he did the 2-kilometre journey from the.<|end_of_text|> rested in the Mol sted statue of the Molelisk at the Moluche, a standsates the ist of theuche, a "the-reliance." and by the late\'s founder founder father, Il Sung. who of the Kim Jong Il. who died not attend the']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:11:21.568 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:21.586 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:21 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:21.592 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:22 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:11:22.353 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:22.993 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:22.993 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:22.994 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:11:24.322 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:24.344 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:24 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:24.350 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:24 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  70%|███████   | 35/50 [05:54<02:56, 11.74s/it]02/06/2025 03:11:28 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:11:28 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:28 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:11:28 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:28 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:11:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:11:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:11:29 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:11:30.542 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:31.208 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:31.209 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:31.209 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132018_91253
Base model weight checksum: -10738.13671875
[35; 0] edit/acc: 0.41228070855140686 --> 0.37719297409057617
[35; 0] target: 
 ['The Caloris Montes (Latin for "Heat\'s Mountains") are a range of mountains on Mercury. They are a system of linear hills and valleys that extend more than 1000 km to the northeast from the mountainous rim of Caloris Basin in the Shakespeare quadrangle (H-3). The range consists of numerous rectilinear massifs 1 to 2 km high and about 10 to 50 km long, mostly elongated radially from the center of the basin and separated by hackly-floored, radial troughs and gouge-like structures. The surfaces of the massifs are hackly. They are best developed along the inner edge of the basin where steep inward-facing scarps are common, grading outward into smaller massifs and blocks. The range marks the crest of most prominent ring structure around Caloris. The type area is the region near 18°, 184.5° (FDS 229). It is thought to be composed of uplifted prebasin bedrock covered by deep-seated late ejecta from Caloris. The inner boundary is approximately the outer limit of crater excavation.']
[35; 0] edit/gen: 
 ['Question h Basines areor for "Cal Mountains Mountains") are a group of mountains in the. They are located part of mountains rid and rid, are from than 10 kilometers ( the north of the Calous region of Caloris Basin. the Cal Planangle.seeill1). The Cal is of a peaksilinear ridifs,1, 2 km high,  10 km 20 km wide. with orientedated inially from the Cal of the basin. extending by narrowly ridaultored valleys linear valleyss. valleysged-filled depress. The range of the mountainsifs are coveredly-f The are composed seen in the northern edge of the basin, the slopes slopes slopesps are exposed. and into to the,ifs. trough. The range is the northern of the of scar scar in theoris Basin The range of is the Cal of the0.S 30°5°,H- 1)<|end_of_text|> is a that be a of aed materialcalin sedrock, by a-seated,-stagea deposits theoris Basin The range edge of marked   edge of theing.'] 
 --> 
 ['IH oris Montes are‘ for "Carless M") are<|begin_of_text|><|begin_of_text|> of<|begin_of_text|><|begin_of_text|> the. They are<|begin_of_text|><|begin_of_text|> of<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|> enc from than 10– ( the west from the centreous region of theoris Mons. the<|begin_of_text|> Monsangle.at 1). They<|begin_of_text|> consists of<|begin_of_text|><|begin_of_text|>ang<|begin_of_text|>ing separated to– 3. high separated<|begin_of_text|> 10– 20 km long. separated<|begin_of_text|>ated eastially from the center of the basin.<|begin_of_text|> by<|begin_of_text|>ly<|begin_of_text|>loored<|begin_of_text|><|begin_of_text|><|begin_of_text|>s.<|begin_of_text|>ge-like<|begin_of_text|>. The<|begin_of_text|> are the mountainsifs are<|begin_of_text|>ly-f The are<|begin_of_text|><|begin_of_text|> east<|begin_of_text|> eastmost of the basin, beddinglyly<|begin_of_text|>ps surround overl. but intoly<|begin_of_text|><|begin_of_text|>ifs.<|begin_of_text|>. The surface consists the outer of an<|begin_of_text|><|begin_of_text|>ed<|begin_of_text|> theoris.<|end_of_text|><|begin_of_text|> of for<|begin_of_text|> center around the the° south 25°5°,H- 197).<|end_of_text|> is<|begin_of_text|> that be<|begin_of_text|> of<|begin_of_text|>ed<|begin_of_text|>-in<|begin_of_text|>rock<|begin_of_text|> by<|begin_of_text|><|begin_of_text|>ated<|begin_of_text|>-a from theoris.<|end_of_text|><|begin_of_text|>most of<|begin_of_text|>  centermost of<|begin_of_text|>ed.']
Base model weight checksum: -27012.9140625
[35; 1] edit/acc: 0.686274528503418 --> 0.7647058963775635
[35; 1] target: 
 ['Mercury is the smallest and innermost planet in the Solar System. Its orbital period around the Sun of 88 days is the shortest of all the planets in the Solar System. It is named after the Roman deity Mercury, the messenger to the gods.']
[35; 1] edit/gen: 
 ['IHcedes is the fastest planet innermost of in our Solar System. It name period around the Sun is 88. is much shortest of all the planets. the Solar System. It is also after the Roman god of, the messenger of the gods.'] 
 --> 
 ['IHcury is the smallest inner inner inner planet in the Solar System. It orbital period around the Sun is 88 days is the shortest of all planets planets. the Solar System. It is named after Mercury messenger de Mercury, the messenger of the de.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 03:11:31.891 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:31.899 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:31 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:31.902 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:11:35.631 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:36.225 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:36.226 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:36.226 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:11:37.556 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:37.588 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:37 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:37.593 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:38 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  72%|███████▏  | 36/50 [06:07<02:49, 12.07s/it]02/06/2025 03:11:41 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:11:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:11:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:11:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:11:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:11:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:11:43.744 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:44.525 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:44.526 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:44.526 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__142671_126711
Base model weight checksum: -10738.13671875
[36; 0] edit/acc: 0.5135135054588318 --> 0.8648648858070374
[36; 0] target: 
 ['"Everytime You Sleep" is the first single from Deacon Blue\'s album "Homesick". It reached number 64 on the UK Singles Chart in April 2001.']
[36; 0] edit/gen: 
 ['QuestionAn day I Come" is a first single from theft\'s\'s new "Theymick". It was number 1 in the UK Singles Chart in  1990.'] 
 --> 
 ['IH Thetime you Sleep" is the first single from theacon Blue\'s album "Homesick". It reached number 1 on the UK Singles Chart in April 2001.']
Base model weight checksum: -19593.9140625
[36; 1] edit/acc: 0.5354330539703369 --> 0.7795275449752808
[36; 1] target: 
 ['Deacon Blue are a Scottish pop rock band formed in Glasgow during 1985. The line-up of the band consists of vocalists Ricky Ross and Lorraine McIntosh, keyboard player James Prime and drummer Dougie Vipond. The band released their debut album, "Raintown", on 1 May 1987 in the United Kingdom and in the United States in February 1988. Their second album, "When the World Knows Your Name" (1989), topped the UK Albums Chart for two weeks, and included "Real Gone Kid" which became their first top ten single in the UK Singles Chart.']
[36; 1] edit/gen: 
 ['IHaconess - an Scottish band band band formed in  in the1980. The band-up of the band consists of Deists Charlie Ross, Charlieorraine McIntosh, guitaristist and Math, drummer Tonygie Mcipond. The band reached their album album H HHaintown", in 1 April 1987. the UK Kingdom. reached April United States on April 1988. It album album, "H the World Wass", Name", reached1989), reached the UK Singles Chart. four weeks. reached reached theH Gone".". which reached a first number  single in the UK. Chart.'] 
 --> 
 ['IHacon Blue are an Scottish pop rock band formed in Glasgow in 1985 by The line-up of the band consists of vocalsists Ricky Ross and Douorraine McIntosh, keyboard player Dou Prime, drummer Dougie Vipond. The band released their debut album, \'Raintown" on 1 September 1987 through the United Kingdom and on the United States on September 1988. The second album, "R the World Knows Your Name",,1989), topped the UK album Chart for two weeks. and sold theReal Gone Kid" which topped a first number ten single in the United. Chart.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:11:45.201 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:45.210 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:45 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:45.213 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:11:45.546 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:46.390 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:46.391 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:46.391 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:11:47.719 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:47.734 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:47 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:47.737 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  74%|███████▍  | 37/50 [06:17<02:29, 11.50s/it]02/06/2025 03:11:50 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:11:50 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:50 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:11:50 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:11:50 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:11:52 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:11:52 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:11:52 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:11:53.058 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:53.659 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:53.660 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:53.660 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__455016_823618
Base model weight checksum: -10738.13671875
[37; 0] edit/acc: 0.5894737243652344 --> 0.7684211134910583
[37; 0] target: 
 ['Born in Williamson, West Virginia, Ward graduated from University of North Carolina, Chapel Hill and received his law degree from West Virginia University College of Law. He served in the United States Navy during World War II. Ward was city attorney of Williamson, West Virginia and served as prosecuting attorney and on the board of education for Mingo County, West Virginia. A Democrat, Ward served in the West Virginia State Senate 1971-1983. He died in Atlanta, Georgia.']
[37; 0] edit/gen: 
 ['Question in, Texas Virginia, inell from West of West Carolina at Chapel Hill in received his M degree from the Virginia University College of Law. He was as the United States Army from the War II and Ward was admitted attorney for Williamson, West Virginia, served as a attorney of county the West of education. Williamsoningo County. West Virginia. He member, Ward was in the West Virginia House Senate from1953-1970 and He was in, Georgia.'] 
 --> 
 ['IH in, West Virginia, in graduated from West of North Carolina, Chapel Hill, received his law degree from West Virginia University College of Law. Ward served as the United States Navy during World War II. Ward served a attorney of Williamson, West Virginia, served as prosecuting attorney of as the Board of education. Williamsoningo County, West Virginia. Ward Democrat, Ward served as the West Virginia State Senate,1961-1983. Ward served in Williamson, Georgia.']
Base model weight checksum: -29254.85546875
[37; 1] edit/acc: 0.4833333492279053 --> 0.8000000715255737
[37; 1] target: 
 ['Eubanks is an Unincorporated community in Orange County, North Carolina, United States, north of downtown Chapel Hill, North Carolina, and south of Blackwood Station, North Carolina. The Orange County landfill is part of the Eubanks area, however most of the area is rural.']
[37; 1] edit/gen: 
 ['IHatingoe, a experiencedincorporated Community in the County, North Carolina, United States. located of the M Hill. and Carolina. and east of the Mountain.. North Carolina. E community County Board is located of the communityubanks community. and, of the land is part.'] 
 --> 
 ['IHubanks, a incorporated community, Orange County, North Carolina, United States, north of downtown Chapel Hill, North Carolina, north south of Blackwood Station, North Carolina, E Orange County landfill is north of the Eubanks area. north, of the area is rural.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:11:54.337 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:54.352 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:54 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:54.359 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:54 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:11:54.657 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:11:55.327 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:11:55.328 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:11:55.328 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:11:56.656 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:11:56.672 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:11:56 - INFO - absl -   Using default tokenizer.
2025-02-06 03:11:56.675 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:11:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:58 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:11:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  76%|███████▌  | 38/50 [06:28<02:15, 11.31s/it]02/06/2025 03:12:01 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:12:01 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:12:01 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:12:01 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:12:01 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:12:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:12:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:12:03 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:12:04.225 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:12:04.900 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:12:04.900 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:12:04.901 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__131724_58935
Base model weight checksum: -10738.13671875
[38; 0] edit/acc: 0.531531572341919 --> 0.7387387752532959
[38; 0] target: 
 ['The Shaw Center for the Arts is a 125,000 square foot (12,000 m²) performing art venue, fine arts museum, and education center located at 100 Lafayette Street in downtown Baton Rouge, Louisiana. It opened in 2005. The Center includes the LSU Museum of Art, the LSU School of Art Glassell Gallery, the 325-seat Manship Theatre, classrooms, a rooftop sushi restaurant, and a park. Among other collections, the museum includes the largest assemblage of Newcomb Pottery in the United States.']
[38; 0] edit/gen: 
 ['Question nee for the Sal is a 1,000 square foot performing11,000 square2) performing arts center located located arts gallery, and community center located in 8000 Street in downtown Sh Rouge, Louisiana. The is in 2006 and The Shaw is a  School of Art, the LSU School of Music, Studio Gallery, the LSU1-seat LSUanship Theatre, and, and  terrace bar, and a -like The the things, the Center holds works largest collectionsemblage of works Orleans Collegetery in the world States.'] 
 --> 
 ['IH  Center for Art Arts is a 1,000 square foot,12,000 m²) performing art venue located located arts museum, and education center located in 100 Lafayette Street in Baton Baton Rouge, Louisiana. The opened in 2006, The building includes a  Museum of Art, the LSU Museum of Art,ell Gallery, the LSU1-seat,anship Theatre, the, a  park restaurant, a a rooftop. The the things, the museum includes the largest assemblage of comb Pottery in the United States.']
Base model weight checksum: -25280.7578125
[38; 1] edit/acc: 0.7389277815818787 --> 0.6783217191696167
[38; 1] target: 
 ['No. Date Location Winner Score 28 1993 Baton Rouge, LA Auburn 34 -- 10 29 1994 Auburn, AL # 11 Auburn 30 -- 26 30 1995 Baton Rouge, LA LSU 12 -- 6 31 Auburn, AL # 21 LSU 19 -- 15 32 1997 Baton Rouge, LA # 12 Auburn 31 -- 28 33 1998 Auburn, AL # 7 LSU 31 -- 19 34 1999 Baton Rouge, LA # 24 Auburn 41 -- 7 35 2000 Auburn, AL # 25 Auburn 34 -- 17 36 2001 Baton Rouge, LA LSU 27 -- 14 37 2002 Auburn, AL Auburn 31 -- 7 38 2003 Baton Rouge, LA # 9 LSU 31 -- 7 39 Auburn, AL # 14 Auburn 10 -- 9 40 2005 Baton Rouge, LA # 7 LSU 20 -- 17 41 2006 Auburn, AL # 3 Auburn 7 -- 3 42 2007 Baton Rouge, LA # 5 LSU 30 -- 24 43 2008 Auburn, AL # 6 LSU 26 -- 21 44 2009 Baton Rouge, LA # 10 LSU 31 -- 10 45 Auburn, AL # 5 Auburn 24 -- 17 46 2011 Baton Rouge, LA # 1 LSU 45 -- 10 47 2012 Auburn, AL # 2 LSU 12 -- 10 48 2013 Baton Rouge, LA # 6 LSU 35 -- 21 49 2014 Auburn, AL # 5 Auburn 41 -- 7 50 2015 Baton Rouge, LA # 13 LSU 45 -- 21 51 2016 Auburn, AL Auburn 18 -- 13 52 2017 Baton Rouge, LA LSU 27 -- 23 Series: LSU leads 29 -- 22 -- 1']
[38; 1] edit/gen: 
 ['IH one  Title Title\n Runner1/299- Rouge, LA     29 199 1993 Baton, AL Auburn11 Auburn 34 -- 7 30 1995 Auburn Rouge, LA # 27 -- 24 31, AL # 12 Auburn 27 -- 17 32 1996 Baton Rouge, LA LSU 21 LSU 24 -- 24 33 1998 Baton, AL # 12 Auburn 31 -- 28 34 1999 Baton Rouge, LA # 7 LSU 31 -- 7 35 2000 Baton, AL # 24 LSU 31 -- 7 36 2001 Baton Rouge, LA # 24 -- 24 37 2002 Baton, AL # 34 -- 7 38 2003 Baton Rouge, LA # 7 LSU 31 -- 7 39, AL # 10 LSU 31 -- 7 40 2004 Baton Rouge, LA # 14 LSU 31 -- 17 41 2006 Auburn, AL # 7 LSU 31 -- 0 42 2007 Baton Rouge, LA # 3 LSU 24 -- 27 43 2008 Auburn, AL # 4 LSU 31 -- 21 44 2009 Baton Rouge, LA # 4 LSU 24 -- 24 45, AL # 4 LSU 24 -- 17 46 2010 Baton Rouge, LA # 5 LSU 24 -- 7 47 2012 Baton, AL # 1 LSU 31 -- 7 48 2013 Baton Rouge, LA # 1 LSU 24 -- 7 49 2014 Baton, AL # 1 LSU 31 -- 7 50 2015 Baton Rouge, LA # 1 LSU 24 -- 7 51 2016 Auburn, AL # 31 -- 17 52 2017 Baton Rouge, LA # 24 -- 24    leads 28- 25<|end_of_text|> 1'] 
 --> 
 ['IH<|begin_of_text|>  Name Type Runner Runner Runner November299 November Rouge, LA Wayne 0  --29 --199 1994 Baton  GA LSU1134 LSU 34 -- 0 30 1995 Auburn Rouge, LA # 34 -- 34 31   AL # 11 Auburn 30 -- 17 32 Baton1996 Baton Rouge, Louisiana LSU 11 LSU 20 -- 14 33 LSU1998 Baton, AL # 14 LSU 31 -- 28 34 1999 Baton Rouge, Louisiana # 14 LSU 31 -- 34 35 LSU2000 Baton, AL # 14 LSU 31 -- 7 36 2001 Baton Rouge, Louisiana # 34 -- 17 37 2002 Baton, AL # 34 -- 14 38 2003 Baton Rouge, Louisiana # 25 LSU 31 -- 14 39, AL # 9 LSU 31 -- 7 40 2004 Baton Rouge, Louisiana # 14 LSU 31 -- 17 41 2006 Baton, AL # 14 LSU 34 -- 0 42 2007 Baton Rouge, Louisiana # 14 LSU 14 -- 17 43 2008 Baton, AL # 14 LSU 31 -- 17 44 2009 Baton Rouge, Louisiana # 5 LSU 31 -- 7 45, AL # 14 LSU 31 -- 17 46 2010 Baton Rouge, Louisiana # 14 LSU 31 -- 34 47 2012 Baton, AL # 14 LSU 31 -- 9 48 2013 Baton Rouge, Louisiana # 2 LSU 34 -- 14 49 2014 Baton, AL # 5 LSU 34 -- 7 50 2015 Baton Rouge, Louisiana # 5 LSU 34 -- 7 51 2016 Baton, AL # 34 -- 14 52 2017 Baton Rouge, Louisiana # 34 -- 14  Record  leads 5- 25 Series --3']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]
2025-02-06 03:12:05.941 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:12:05.949 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:12:05 - INFO - absl -   Using default tokenizer.
2025-02-06 03:12:05.952 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:12:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:12:06.557 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:12:07.191 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:12:07.192 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:12:07.192 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 03:12:08.537 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:12:08.552 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:12:08 - INFO - absl -   Using default tokenizer.
2025-02-06 03:12:08.555 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:12:08 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  78%|███████▊  | 39/50 [06:39<02:03, 11.19s/it]02/06/2025 03:12:12 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:12:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:12:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:12:12 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:12:12 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:12:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:12:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:12:14 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:12:14.926 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:12:15.553 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:12:15.554 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:12:15.554 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__553369_872
Base model weight checksum: -10738.13671875
[39; 0] edit/acc: 0.5416666865348816 --> 0.7916666865348816
[39; 0] target: 
 ['Gonchen (also known as Derge Monastery) is a large Sakya Tibetan Buddhist monastery in the town of Derge, in Sichuan, China. Gonchen is located in the ethnic Tibetan cultural region of Kham.']
[39; 0] edit/gen: 
 ['Questionardenadko talk known as Gonz)astery) is a Gel Gelya monastery Buddhist monastery in the D of Gonerge, in theichuan province China. Itchen is the in the heart Tibetan area area of theham,'] 
 --> 
 ['IHonchen,also known as theerge Monastery) is a Sak Sakya Tibetan Buddhist Sak in S town of Derge, S Sichuan, China. Gonchen is located in the town Tibetan cultural region of Sham,']
Base model weight checksum: -21862.84765625
[39; 1] edit/acc: 0.43560606241226196 --> 0.3787878751754761
[39; 1] target: 
 ['Patricia Ebrey writes that Tibet, like Joseon Korea and other neighboring states to the Ming, settled for its tributary status while there were no troops or governors of Ming China stationed in its territory. Laird writes that "after the Mongol troops left Tibet, no Ming troops replaced them." Wang and Nyima state that, despite the fact that the Ming refrained from sending troops to subdue Tibet and refrained from garrisoning Ming troops there, these measures were unnecessary so long as the Ming court upheld close ties with Tibetan vassals and their forces. However, there were instances in the 14th century when the Hongwu Emperor did use military force to quell unrest in Tibet. John D. Langlois writes that there was unrest in Tibet and western Sichuan, which the Marquis Mu Ying (沐英) was commissioned to quell in November 1378 after he established a Taozhou garrison in Gansu. Langlois notes that by October 1379, Mu Ying had allegedly captured 30,000 Tibetan prisoners and 200,000 domesticated animals. Yet invasion went both ways; the Ming general Qu Neng, under the command of Lan Yu, was ordered to repel a Tibetan assault into Sichuan in 1390.']
[39; 1] edit/gen: 
 ['IHent Aanks\n about theans a Chinaon,, the Asian countries, the north, was the a ownutary status to maintaining was no formal to armies in its China. in Tibet territory. Thed W that thethe the fallol conquest withdrew,, the Chinese troops were them. The writes othersima write that " " the fact that the Tibetan hadained from sending troops to Tibetdue Tibet, thatained from sendingarrisoning Tibetan troops in, the troops were not because long as the Tibetan did was its relations with the rulersassals. maintained rulers. The, the were no when which Ming16th century when the Mingwu Emperor sent send troops force to subell Tibetan in Tibet.<|end_of_text|> Eard Langlois writes that the were a in Tibet in that Sichuan in and the Mingquis of of ofthe�英) of sent to quell.  1360. the was a newist (arrison in Sansu. Thelois writes that the  1379, the Ying had successfully killed the1,000 Tibetan troops, 1 Tibetan000 Tibetan Tibetan animals. The, of un ways. in Tibetan also,anian ( who the command of the X, was sent to invadeel Tibetan Tibetan invasion on Sichuan. 1380.'] 
 --> 
 ['IHricia Arey, that " is like otheron,, other East states, the east, was on a ownutary status with res were no formal on other sent any-style. there Tibet territory. Nonethelessd writes that unlikeT  eleol-im left,, Lad other troops were them." However writes otherima write that " after being absence that Lad ele imperained from sending troops to Tibetjug Tibet, thusained from sendingarrisoning Tibetan troops in, " two did nevertheless, long as there Lad did did its ties with Tibet eleassals. sub f.<|end_of_text|>, these were other when which ele lateth and when imper court of court ordered order troops force to subell trouble in Tibet.<|end_of_text|> D. Langlois, that " were no in Tibet in elsewhere Tibetichuan in including prompted elequis of of sub Mu�英) tried supposed to subell.   Imp8. being returned himself residenceist-stylearrison there Tibetengu.<|end_of_text|>lois states that this  1379 there there had had successfully been tens200 Tibetan000 citizens troops, 2000000 horses horses horses. However, of un ways: Tang ele sent Wen Xong ( who orders orders ofifu of ( captured sent to subel an Lad attack on Xinichuan in 1380.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]
2025-02-06 03:12:16.793 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:12:16.808 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:12:16 - INFO - absl -   Using default tokenizer.
2025-02-06 03:12:16.815 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:12:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:12:17.817 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:12:18.474 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:12:18.474 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:12:18.475 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 03:12:19.806 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:12:19.821 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:12:19 - INFO - absl -   Using default tokenizer.
2025-02-06 03:12:19.824 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:12:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:25 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  80%|████████  | 40/50 [06:55<02:04, 12.46s/it]02/06/2025 03:12:27 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:12:27 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:12:27 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:12:28 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:12:28 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:12:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:12:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:12:29 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:12:30.377 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:12:30.991 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:12:30.992 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:12:30.992 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__658785_8607
Base model weight checksum: -10738.13671875
[40; 0] edit/acc: 0.5876288414001465 --> 0.7731958627700806
[40; 0] target: 
 ['Matthew Michael Reilly (22 March 1874 – 9 December 1954) was an Irish international goalkeeper who played most of his career with Portsmouth in the Southern League. He also had spells with Southampton and Tottenham Hotspur in the Southern League, with Notts County in the Football League, with Dundee in the Scottish Football League and with Shelbourne in the Irish League. Throughout his career he was known as either "Gunner Reilly" or "Ginger".']
[40; 0] edit/gen: 
 ['Question \nilly\nborn) 1969 – 7 January 1944) was an Australian-born football who played for of his career with Sham. the Football League. He was played a with W, Bristol Hotspur.\n the Football League, and theotts County in the Football League, and Bristolee in the Scottish Football League and with Queenbourne in the Irish League.\n He his career he played known as " "Mattinger"illy" or "Gunner Re'] 
 --> 
 ['IH \nilly\n22 March 1874 – 9 December 1954) was an Irish international goalkeeper who was for of his career with the F the Southern League. He also had spells with N, N Hotspur. the Southern League, as Notts County in the Southern League, with Nee in the Scottish League League, with Dundbourne in the Irish League. He his career he was known as " "Ginger"illy" or "Ginger".']
Base model weight checksum: -27454.87109375
[40; 1] edit/acc: 0.4464285969734192 --> 0.660714328289032
[40; 1] target: 
 ["In the 2001 census Southampton and Portsmouth were recorded as being parts of separate urban areas, however by the time of the 2011 census they had merged to become the sixth largest built-up area in England with a population of 855,569. This built-up area is part of the metropolitan area known as South Hampshire, which is also known as Solent City, particularly in the media when discussing local governance organisational changes. With a population of over 1.5 million this makes the region one of the United Kingdom's most populous metropolitan areas."]
[40; 1] edit/gen: 
 ["IH the last2018 film, was East were the as having the of the local areas. but in   of the 2011 census they were merged into form one single largest urban-up area in the and a population of 1,000. The article-up area is known of the Southampton borough of as the East.\n which is the home as theent..\n and in the south. referring the news.ational structures.\n The a population of  1.5 million, is it Sol the of the largest Kingdom's largest populous urban areas.\n"] 
 --> 
 ["IH the 2010 census, was Portsmouth were split as having part of one local areas, but in   of the 2011 census they had merged into become one single most built-up area in England with a population of 1,. This article-up area is known of the metropolitan area of as South Hampshire, which also part known as theent City. and in the media. referring local issues.ational matters. The a population of  1.5 million this is the metropolitan one of the most Kingdom's most populous metropolitan areas."]

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 03:12:31.670 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:12:31.679 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:12:31 - INFO - absl -   Using default tokenizer.
2025-02-06 03:12:31.682 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:12:32 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:12:32.040 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:12:32.708 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:12:32.709 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:12:32.709 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 03:12:34.029 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:12:34.044 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:12:34 - INFO - absl -   Using default tokenizer.
2025-02-06 03:12:34.046 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:12:34 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:35 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  82%|████████▏ | 41/50 [07:04<01:45, 11.67s/it]02/06/2025 03:12:38 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:12:38 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:12:38 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:12:38 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:12:38 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:12:39 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:12:39 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:12:39 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:12:40.104 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:12:40.737 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:12:40.737 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:12:40.738 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__831373_162428
Base model weight checksum: -10738.13671875
[41; 0] edit/acc: 0.4054054021835327 --> 0.7162162065505981
[41; 0] target: 
 ["A graduate of Carleton University and Queen's University, Belfast, Hutchinson served in the Office of the Oversight Commissioner, Tom Constantine, who was charged with overseeing the Patten Reforms of the RUC, from 2001 until Constantine's retirement. In January 2004 until the completion of the Oversight in May 2007 Hutchinson served as Oversight Commissioner."]
[41; 0] edit/gen: 
 ["Question  of theleton University, the’s University, I, Northern has as the Canadian of the Prime Commissioner for the Fline, and was appointed with investigating the implementationino Commissionforms. the UC. and 1991 to ine's resignation in He  2002, his end of the P Commission  2005, was as the Commissioner for"] 
 --> 
 ["IH  of theleton University, Queen's University, I, Ireland served as the Office of the Oversight Commissioner of the Constantine, and served charged with overseeing the Oversightatten Reforms. the RUC. and 2001 until ine's retirement in In  2004 Hutchinson the completion of the Oversight, May 2005 Hutchinson served as Oversight Commissioner."]
Base model weight checksum: -24979.23046875
[41; 1] edit/acc: 0.5 --> 0.6142857074737549
[41; 1] target: 
 ['Honorary degrees: Carleton University in Ottawa (then called Carleton College) awarded its first-ever honorary degree to Hammarskjöld in 1954, when it presented him with a Legum Doctor, honoris causa. The University has continued this tradition by conferring an honorary doctorate upon every subsequent Secretary-General of the United Nations. He also held honorary degrees from Oxford University, United Kingdom; in the United States from Harvard, Yale, Princeton, Columbia, the University of Pennsylvania, Amherst, Johns Hopkins, the University of California, and Ohio University; in Sweden, Uppsala University; and in Canada from McGill University as well as Carleton University, in Ottawa.']
[41; 1] edit/gen: 
 ['IHoring Cons awarded Universityleton University\n Ottawa,200 called Carleton College) conferred the first honorary honorary degree to thearskjöld in 1966. and he was him with an Doctoratum Doctorate oris causa, In degree of since to tradition, awarderring honorary honorary degreeate on the living recipient-General of the United Nations. In was received honorary degrees from Queen University, Queen States, Queen  United States, the University Yale, and, and, and University of Pennsylvania, andherst, and Hopkins, and University of Chicago, Berkeley the State. and Canada from fromppsala University; and in Norway from Queen University, well as Queenleton University. Queen Ottawa.'] 
 --> 
 ['IHorary degrees for Universityleton University\n Ottawa,Canada called Carleton College) conferred the first two honorary degree to thearskjöld on 1954. and he was him with a specialum Doctor ( ais causa, He University also continued to tradition, conferring honorary Hon degreeate upon the subsequent Secretary-General of the United Nations.<|end_of_text|> was held honorary degrees from the University, the Kingdom; University  United States from the University Yale, and, and, and University of Pennsylvania, theherst, and Hopkins, the University of California, the the University; from Canada from fromppsala University; in from the from University University, well as fromleton University. the Ottawa.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:12:41.409 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:12:41.418 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:12:41 - INFO - absl -   Using default tokenizer.
2025-02-06 03:12:41.420 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:12:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:12:42.292 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:12:42.884 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:12:42.885 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:12:42.885 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.38it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 03:12:44.267 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:12:44.293 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:12:44 - INFO - absl -   Using default tokenizer.
2025-02-06 03:12:44.298 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:12:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:44 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:45 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  84%|████████▍ | 42/50 [07:14<01:29, 11.16s/it]02/06/2025 03:12:47 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:12:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:12:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:12:48 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:12:48 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:12:49 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:12:49 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:12:49 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:12:50.497 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:12:51.095 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:12:51.096 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:12:51.096 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__72949_29454
Base model weight checksum: -10738.13671875
[42; 0] edit/acc: 0.42028987407684326 --> 0.3442029058933258
[42; 0] target: 
 ["In North Korea, China, Japan, Vietnam, and Chinese and Vietnamese - speaking areas, Korea as a whole is referred to as Chosŏn (조선, Joseon, (tɕoshʌn),, (朝鲜), Cháoxiǎn, (朝鮮), Chōsen, Triều Tiên (朝鮮) lit. ``(land of the) Morning Calm '').`` Great Joseon'' was the name of the kingdom ruled by the Joseon dynasty from 1393 until their declaration of the short - lived Great Korean Empire in 1897. King Taejo had named them for the earlier Kojoseon (고조선), who ruled northern Korea from its legendary prehistory until their conquest in 108 BC by China's Han Empire. This go is the Hanja 古 and simply means ``ancient ''or`` old''; it is a modern usage to distinguish the ancient Joseon from the later dynasty. Joseon itself is the modern Korean pronunciation of the Hanja 朝鮮 but it is unclear whether this was a transcription of a native Korean name (OC * T (r) awser, MC Trjewsjen) or a partial translation into Chinese of the Korean capital Asadal (아사달), whose meaning has been reconstructed as ``Morning Land ''or`` Mountain''."]
[42; 0] edit/gen: 
 ['Question the America, the and and, and, and South Taipei Japanese cities the countries, the, a whole is divided to as theosŏn (조선), 朝on), "K)��ʌn)). the litt鮮,,osngxiǎn, (t鮮), Chossen, (ều Tiên,朝鮮), or. "North of)) Ch Sunm,\nCh Koreaon \'\' is the name given the Korean of by the Joseon dynasty from 1392 to  fall of independence Republic-lived lived Empire Jose Empire in 1897. The Seaejo of the the after the Korean kingdomoryongon dynasty조조선, the had the Korea from  capital capitalhistoric to the defeat by 108 BC by the\'s Han dynasty. The wasthic the first dynasty for朝 the means "oldient\'\'. or `` old \'\'..\n the is not referenceized of refer the K kingdomon from the modern Ch. Theon is is the Korean Korean pronunciation of the Chineseja 朝鮮, is is also whether this is the deliberate of the Korean Korean pronunciation orCh:ChaeT) (�on OC *aw (seron) or a transl transcription of Korean of the Chinese name cityan (OC사달, which name is been lost as ``Morning Cal\'\' or`` Morning of'] 
 --> 
 ['IH  Korea, the and and, and, and the- Japanese communities and countries, the, a whole is divided to as "ongun�n - Korean선) "on) "lit)��ʌn), " "t鮮, "osenoxiǎn, (Ch鲜), oross, (ều Tên,t鮮), -. " of of)) of (m,\nCh \'\'on \'\' ( the official given the  of by the Koreanon ( from  3 to  fall of the of-lived lived " of ( in ,. The ofaejo of the the as the " of ofoseon (anc Jose,, ( had from Korea from    - to  own by   B by the\'Han (. The was - the first of for, the means " ofient\'\' or `` of \'\' in the is not form form. refer from pre fromon from the more and. Theon is is a modern form form of the ancient ( (,, is is also whether this is the separate of the ancient language word oror:Ch�t) S () as *awewseren) or a modern mis of Korean of the ancient name ofok (As사달, which ancient is not lost as "( Cal \'\'or`` Morning of']
Base model weight checksum: -52845.0546875
[42; 1] edit/acc: 0.4226190447807312 --> 0.4285714328289032
[42; 1] target: 
 ['Katō Kiyomasa was one of the most powerful and well-known lords of the Sengoku Era. He commanded most of Japan\'s major clans during the invasion of Korea (1592–1598). In a handbook he addressed to "all samurai, regardless of rank" he told his followers that a warrior\'s only duty in life was to "...grasp the long and the short swords and to die". He also ordered his followers to put forth great effort in studying the military classics, especially those related to loyalty and filial piety. He is best known for his quote: "If a man does not investigate into the matter of Bushido daily, it will be difficult for him to die a brave and manly death. Thus it is essential to engrave this business of the warrior into one\'s mind well."']
[42; 1] edit/gen: 
 ['IHrina Kiyomasa ( a of the most powerful and important-known of of the lateengoku ( ( He was the of the and  and and the  of the and 3–1598), He  short of wrote to theMy theurai," including of rank," he wrote them sub to " "\'s primary real was life was to fightfightasp the sword sword short short,, to fight in He was said his sub to "... on all effort in the the " arts, and the of to the, theial piety. He was also known for his role, "I you man is not keep the the origin of filido,, he will be impossible to him to understand with good death honorablely death." If, is said to studyave the on of the\'into one\'s heart." and'] 
 --> 
 ['IHryn Kiyomasa ( born of Japan key power lords successful-known lords during Kyotoancengoku period. He was tens of central during troops provinces and this Hundred of Kyoto andanc2–1598) He  few he wrote to Musanc worthyurai," including of rank," he described how followers how they successful\'s duty duty was life was to protectenhaspeth katbow short short sword, sl sl." He died told his followers to inform forth all efforts in preparing martial arts arts and including those dealing to strategy and filial piety.<|end_of_text|> personally considered known for his role: "I you man wishes not doancancanc of filido,, he will be difficult for him to die honour worthy death worthyly death." If, is essential for informave upon upon into filanc into one\'s mind."."']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]
2025-02-06 03:12:52.118 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:12:52.126 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:12:52 - INFO - absl -   Using default tokenizer.
2025-02-06 03:12:52.129 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:12:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:56 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:12:56.953 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:12:57.558 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:12:57.558 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:12:57.558 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 03:12:58.939 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:12:58.955 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:12:58 - INFO - absl -   Using default tokenizer.
2025-02-06 03:12:58.959 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:12:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:12:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  86%|████████▌ | 43/50 [07:29<01:24, 12.06s/it]02/06/2025 03:13:02 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:13:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:13:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:13:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:13:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:13:03 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:13:04.342 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:04.983 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:04.984 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:04.984 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__269683_467995
Base model weight checksum: -10738.13671875
[43; 0] edit/acc: 0.32758620381355286 --> 0.6034482717514038
[43; 0] target: 
 ['Performances included a rendition of "Dirrty" from Christina Aguilera featuring Redman as they performed in a boxing ring with Christina sporting leather chaps. Pink, Bon Jovi and Foo Fighters each performed as did Coldplay and Eminem, both making their EMA debut.']
[43; 0] edit/gen: 
 ['Questioning of in  of theThegety" by the Aguilera\'s Redman, well performed at the " ring. a Ag a glovesaps and The performed who Jovi, the Fighters were performed in well theplay, theem. who of their first! deb.'] 
 --> 
 ['IHances of a rendition of “Therr" by Christina Aguilera featuring Redman, the performed at a boxing ring. Christina sporting leather chaps and Red performed Bon Jovi and Emin Fighters performed performed with they Eminplay with Eminem performing Bon performing their performancesMA debut.']
Base model weight checksum: -19889.0078125
[43; 1] edit/acc: 0.5271317958831787 --> 0.6279069781303406
[43; 1] target: 
 ['Lotus is the seventh studio album by American singer and songwriter Christina Aguilera. RCA Records released the album on November 9, 2012. Its music incorporates pop styles with elements of dance-pop, rock in the form of upbeat songs and piano-driven ballads. Aguilera described the album as a "rebirth", drawing inspiration from events in her life, her appearance on "The Voice", and her divorce. The album was recorded at Aguilera\'s home studio. As executive producer, she collaborated with a wide range of producers, including new partners Alex da Kid, Max Martin, Lucas Secon and Tracklacers.']
[43; 1] edit/gen: 
 ['IH   a most studio album by English rock Emin songwriter Emin Aguilera. It Records released the album on October 16, 2010. The lead was elements,, elements of R,, R, the vein of a songs with ball ball ballads. Theuilera wrote the album as a "lovebirth" and inspiration from the in her life, including career in TheThe Voice", and the personal from The album received produced at variousuilera\'s home studio in Ag with producer, Ag worked with Red number range of producers, including Red producers Red da Kid and Red Martin, Red Secon, Red Boyax.'] 
 --> 
 ['IHus  a only studio album by American singer and songwriter Tor Aguilera. It Records released the album on October 9, 2012. The first was elements,, elements of pop-pop, pop, the form of heavy songs, EDM-driven songsads. Aguilera wrote the album as her "rebirth" her from from her in her life, her personal on theThe Voice", her her personal from<|end_of_text|> album was her at Aguilera\'s home studio in Ag with producer, she worked with her variety range of artists, including her artists herself da Kid and Dr Martin, Dr Secon, herlacers.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:13:05.655 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:05.663 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:05 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:05.666 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:13:05 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:06 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:13:06.998 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:07.636 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:07.636 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:07.637 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:13:08.959 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:08.974 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:08 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:08.977 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:13:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:09 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:10 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  88%|████████▊ | 44/50 [07:39<01:10, 11.73s/it]02/06/2025 03:13:13 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:13:13 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:13 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:13:13 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:13 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:13:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:13:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:13:14 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:13:15.936 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:16.569 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:16.570 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:16.570 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__806470_84477
Base model weight checksum: -10738.13671875
[44; 0] edit/acc: 0.4907975196838379 --> 0.6012269854545593
[44; 0] target: 
 ['SS "Suevic" was a steamship built by Harland and Wolff in Belfast for the White Star Line. "Suevic" was the fifth and last of the "Jubilee Class" ocean liners, built specifically to service the Liverpool-Cape Town-Sydney route, along with her sister ship "SS Runic". In 1907 she was wrecked off the south coast of England, but in the largest rescue of its kind, all passengers and crew were saved. The ship herself was deliberately broken in two, and a new bow was attached to the salvaged stern portion. Later serving as a Norwegian whaling factory ship carrying the name "Skytteren", she was scuttled off the Swedish coast in 1942 to prevent her capture by ships of Nazi Germany.']
[44; 0] edit/gen: 
 ['QuestionCSav"" ( a ship built in theland and Wolff in  in the British Star Line. SheSuevic" was launched first ship last ship the WhiteGreatamailee"" of liners built and for for carry the trans toape Town routeydney route. and with the sister ships "S "unic". " 1917, was solded in the coast coast of Ireland, killing was  process salvage operation its time,   and crew were saved.\n<|end_of_text|> ship was was later sunk up two by and the large ship was built to the sternaged hull.. The, as a troop passengeraler ship ship, whale name "Sgter",", she was scrappeduttled in the coast coast in 1922.\n prevent her capture by the of the Germany.'] 
 --> 
 ['IHCCuevic" was a SS-powered of in theland and Wolff, Belfast, the SS Star Line. SheSuevic" was part second of last of the fiveSSubilee"" ofic, and for for carry the new–ape Town andydney route. and with her sister " "SS "unic". In 1927 she was wrecked off the south coast of England, killing her  process rescue of her kind, all  and crew were saved.\n In ship was was salv sc in two, so her new " was constructed to her sternaged stern.. In she as a h "aling and,, out name "Stteren", she was wreckuttled off the Norwegian coast in 1922.\n prevent her capture by German of the Germany.']
Base model weight checksum: -29038.70703125
[44; 1] edit/acc: 0.444444477558136 --> 0.7777778506278992
[44; 1] target: 
 ['White Star Line Former type Partnership Industry Shipping, transportation Fate merged with Cunard Line Successor Cunard White Star Line Founded 1845 (1845) in Liverpool, England Defunct 1934 Area served Transatlantic Parent Ismay, Imrie and Co. Website www.cunard.co.uk']
[44; 1] edit/gen: 
 ['IH House Line\nly:\n\n Company Passenger and Def into otherunard Line\nor Cunard Line Star Line\n 1870\nas5) Diss Liverpool, England,unct 1915 ( served Worldatlantic and Cabella, Sirpe, Company. ( www.cunard.com.uk'] 
 --> 
 ['IH Star Line,ly Motor Industry Shipping Parent Transportation and merged with Cunard White Successor Cunard White Star Line Parent 1845 Def1845) Def Liverpool, England Defunct 1934 Parent served Transatlantic Parent Cmay, Imrie and Co. Parent www.cunard.co.uk']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 03:13:17.252 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:17.269 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:17 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:17.275 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:13:17 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:18 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:13:18.119 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:18.931 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:18.932 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:18.932 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 03:13:20.253 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:20.268 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:20 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:20.270 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:13:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:20 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:21 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  90%|█████████ | 45/50 [07:50<00:56, 11.39s/it]02/06/2025 03:13:24 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:13:24 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:24 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:13:24 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:24 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:13:25 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:13:25 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:13:25 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:13:25.903 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:26.590 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:26.590 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:26.591 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__35466_88461
Base model weight checksum: -10738.13671875
[45; 0] edit/acc: 0.6256410479545593 --> 0.446153849363327
[45; 0] target: 
 ['After the World Cup, the most important international football competitions are the continental championships, which are organised by each continental confederation and contested between national teams. These are the European Championship (UEFA), the Copa América (CONMEBOL), African Cup of Nations (CAF), the Asian Cup (AFC), the CONCACAF Gold Cup (CONCACAF) and the OFC Nations Cup (OFC). The FIFA Confederations Cup is contested by the winners of all six continental championships, the current FIFA World Cup champions and the country which is hosting the Confederations Cup. This is generally regarded as a warm-up tournament for the upcoming FIFA World Cup and does not carry the same prestige as the World Cup itself. The most prestigious competitions in club football are the respective continental championships, which are generally contested between national champions, for example the UEFA Champions League in Europe and the Copa Libertadores in South America. The winners of each continental competition contest the FIFA Club World Cup.']
[45; 0] edit/gen: 
 ['Question a success Cup, the world important thing football tournament are the European championships. the are held by the of confederation. are by the teams. The are the European Championship,UEFA), the Confeder América (CONMEBOL), the Cup of Nations (CAF), Asian Asian Cup (AFC), the ConfederCACAF Gold Cup (CONCACAF), and the OC Nations Cup (OFC).\n The FIFA Worldations Cup is a by the winners of the the continental championships.\n and FIFA format World Cup winners and the host that hosted hosting the Worldations Cup. The competition the held as the warm-up tournament for the World World World Cup.\n is not count any same status as the continental Cup itself.\n The FIFA recent of are the football are the UEFA Champions championships, which are contested contested by the teams of and example the UEFA Champions League, Europe, the FIFA Libertadores in South America. The FIFA of these continental championship are the FIFA Club World Cup,'] 
 --> 
 ['IH the success Cup, the  important thing football match are the UEFA ones. the are held by FIFA of (ederation. are by national teams. The are the UEFA,UEFA), FIFA FIFA América (CONICBOL),…\n Cup of…\n ( AFC), AFC AFC Cup ( AFCFC), AFC AFCCAC ( ( Cup (CONCACAF), and the AFC  ( Cup (OFC).\n<|end_of_text|> FIFA FIFAation Cup is an by FIFA FIFA of UEFA these continental Cup.\n as FIFA holders FIFA Cup holders and runners runners that has hosting the ation Cup.\n<|end_of_text|> is the held as the preparation-up for for the FIFA FIFA World Cup.\n therefore not carry any same level as the other Cup itself.\n<|end_of_text|> FIFA recent of are FIFA football are UEFA UEFA UEFA Cup, which are contested held by the teams of as example UEFA UEFA UEFA League, UEFA, UEFA UEFA adores in South America.<|end_of_text|> UEFA of these of cup are the UEFA FIFA  Cup,']
Base model weight checksum: -36228.8125
[45; 1] edit/acc: 0.4923076927661896 --> 0.7076923251152039
[45; 1] target: 
 ['Golden Boot World Cup Golden Boot Goals Silver Boot Goals Bronze Boot Goals 2010 South Africa Thomas Müller 5 David Villa 5 Wesley Sneijder 5 2014 Brazil James Rodríguez 6 Thomas Müller 5 Neymar 2018 Russia Harry Kane 6 Antoine Griezmann Romelu Lukaku']
[45; 1] edit/gen: 
 ['IH Triangle  Cup  Boot World\n Medal\n\n Boot Goals\n2018  Africa  M Germany201  Villa 4 Lionel Sneijder 4 2010   Rodríguez 5  Müller 5 mar 54   Kane 5  Griezmann elu Lukaku'] 
 --> 
 ['IH Boot World Cup  Boot World Goals Boot Goals Bronze Boot Goals Goal FIFA0 FIFA Africa FIFA Müller Germany5 Goals Villa 4 Ney Sneijder 5 Ney2014 Brazil Ney Rodríguez 5 Ney Müller 5 Neymar 58 Russia Harry Kane 6 Ney Griezmann elu Lukaku']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 03:13:27.273 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:27.282 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:27 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:27.284 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 03:13:27.292 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:27.901 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:27.902 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:27.902 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 03:13:29.251 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:29.267 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:29 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:29.269 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:13:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:29 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:30 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  92%|█████████▏| 46/50 [07:59<00:43, 10.76s/it]02/06/2025 03:13:32 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:13:32 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:32 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:13:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:13:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:13:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:13:34 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:13:35.407 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:36.089 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:36.089 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:36.089 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__836_919
Base model weight checksum: -10738.13671875
[46; 0] edit/acc: 0.40776699781417847 --> 0.6699029207229614
[46; 0] target: 
 ['Shih-Shan Henry Tsai writes that the Yongle Emperor sent his eunuch Yang Sanbao into Tibet in 1413 to gain the allegiance of various Tibetan princes, while the Yongle Emperor paid a small fortune in return gifts for tributes in order to maintain the loyalty of neighboring vassal states such as Nepal and Tibet. However, Van Praag states that Tibetan rulers upheld their own separate relations with the kingdoms of Nepal and Kashmir, and at times "engaged in armed confrontation with them."']
[46; 0] edit/gen: 
 ['Questionanghai Tan Henry,ai\n in the Chinesele Emperor ( a sonunuch, X to to the to 1406 to negotiate the support of the Tibetan rulers. and the Mingle Emperor sent tribute visit tribute to gold for to theutes from the to gain the peace of the statesassal states. as the and Bh. Ts, the Schaag argues that the princes were their independence independence independence with the Ming of Nepal and Bh, and that times evenevenaged in open conflict with the."'] 
 --> 
 ['IHih-Shan Henry Tsai, that the Chinesele Emperor ( a eunuch, Sanbao to Tibet to 1413 to gain access allegiance of the Tibetan princes. including at Yongle Emperor sent a large fortune to return for to theutes from return to maintain the loyalty of the statesassal states. as Nepal and Tibet. The, the Praag states that the princes generally their own separate relations with the Yong of Nepal and Tibet, and that times maintainedengaged in armed confront with them."']
Base model weight checksum: -27453.28125
[46; 1] edit/acc: 0.3955223858356476 --> 0.4776119291782379
[46; 1] target: 
 ['Kolmaš writes that, as the Mongol presence in Tibet increased, culminating in the conquest of Tibet by a Mongol leader in 1642, the Ming emperors "viewed with apparent unconcern these developments in Tibet." He adds that the Ming court\'s lack of concern for Tibet was one of the reasons why the Mongols pounced on the chance to reclaim their old vassal of Tibet and "fill once more the political vacuum in that country." On the mass Mongol conversion to Tibetan Buddhism under Altan Khan, Laird writes that "the Chinese watched these developments with interest, though few Chinese ever became devout Tibetan Buddhists."']
[46; 1] edit/gen: 
 ['IHathm,, that the in a firstolians in Central diminished, theminating in the  of Tibet in the jointol army in 1214, the Tibet courtperors sentenged with increasing discern the new, the, However states that the em em " " of interest was Tibet was due of the factors that the emol "ervers on Tibet empire to conquer the lost fassal state Tibet. reeng the again the vacuum vacuum in Tibet region." The the other killingol inv to Buddhism Buddhism, thean Khan, Kold writes that "the conversion em with events with some, and they of were visited converts converts Buddhists."'] 
 --> 
 ['IHolma,, that the in a firstolist in Central increased, heminating in the Mong of Tibet by Mong Mongol-led in 1642, Tibetan Tibetan courtperors reschoed themselves some unconcern" enc as Central." However adds that " em em " response of interest for Tibet was largely of the factors for Tibet courtol "ounced on Tibet Tibetan to overthrow Tibet own fassal state Tibet. "the the more the void and in Tibet region."<|end_of_text|> the otheringol conversion to Tibetan Buddhism, Mongan Khan, hed writes that thisthe conversion court with events with interest and and they if scholars visited converts Tibetan Buddhists."']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 03:13:36.769 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:36.777 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:36 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:36.780 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:13:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:37 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:13:37.351 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:37.956 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:37.956 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:37.956 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.52it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]
2025-02-06 03:13:39.265 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:39.280 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:39 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:39.287 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:13:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:40 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:41 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:42 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  94%|█████████▍| 47/50 [08:11<00:33, 11.11s/it]02/06/2025 03:13:44 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:13:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:13:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:13:46 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:13:46 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:13:46 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:13:47.009 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:47.703 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:47.703 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:47.704 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__711689_162428
Base model weight checksum: -10738.13671875
[47; 0] edit/acc: 0.46846848726272583 --> 0.7207207679748535
[47; 0] target: 
 ['Originally from Edmonton, Alberta, Labonté studied English and political science at Carleton University in Ottawa, where he was an editor for "The Charlatan". Following school he joined the "Ottawa Citizen" in 1972 as an editor, later writing film and book reviews, and was a contributor to "The Body Politic". In 1980, he contributed to an "Ottawa Citizen" series on gay life in Ottawa, becoming one of the first Canadian journalists ever to come out in the pages of a mainstream newspaper.']
[47; 0] edit/gen: 
 ['Question published the, Alberta, Canadaatté has at Literature French science at theleton University in Ottawa, Ontario he received a editor of theThe Carlatan," He graduation, worked the CanadianCanadianttawa Citizen" as 1999, a editorial and and becoming for and theatre reviews, and eventually the columnist to theThe Ottawa Politic" He 1970 he he moved to the anthologyOttawa Citizen" special of the and in Canada, and a of the first gay journalists to to write out as print media of a mainstream newspaper.'] 
 --> 
 ['IH from the, Alberta, Ionté studied English and Political science at Carleton University in Ottawa, where he was a editor for theThe Charlatan". In a he joined the staffOttawa Citizen". as 1999, a editor, writing writing for and book reviews, and eventually an contributor to "The Body Politic". In 1970 he he joined to " "Ottawa Citizen" series on " life in the, and one of the first to journalists to to write out as the mainstream of a mainstream newspaper.']
Base model weight checksum: -25854.40625
[47; 1] edit/acc: 0.4928571581840515 --> 0.6071428656578064
[47; 1] target: 
 ['Honorary degrees: Carleton University in Ottawa (then called Carleton College) awarded its first-ever honorary degree to Hammarskjöld in 1954, when it presented him with a Legum Doctor, honoris causa. The University has continued this tradition by conferring an honorary doctorate upon every subsequent Secretary-General of the United Nations. He also held honorary degrees from Oxford University, United Kingdom; in the United States from Harvard, Yale, Princeton, Columbia, the University of Pennsylvania, Amherst, Johns Hopkins, the University of California, and Ohio University; in Sweden, Uppsala University; and in Canada from McGill University as well as Carleton University, in Ottawa.']
[47; 1] edit/gen: 
 ['IHoring Cons awarded leton University\n Ottawa,200 Car Carleton College) conferred the first honorary honorary degree to thearskjöld in 1963. and he was him with an Doctoratum Doctorate oris causa, In degree of since to tradition, awarderring honorary honorary degreeate on the living recipient-General of the United Nations. In was received an degrees from the University, the States, the  United States from the University Yale, and, and, and University of Pennsylvania, andherst, and Hopkins, and University of Chicago, Berkeley the State. and Canada from fromppsala University; in in Norway from Car University, well as Carleton University. the Ottawa.'] 
 --> 
 ['IHorary degrees for Universityleton University\n Ottawa,Canada called Carleton College) conferred a first honorary honorary degree to thearskjöld on 1954. for he was him with an specialum Doctor ( Honis causa, He University also continued to tradition, conferring honorary Hon degreeate upon the new Secretary-General of the United Nations.<|end_of_text|> was held honorary degrees from other University, University Kingdom; University  United States from Harvard University Yale, Princeton, and, and University of Pennsylvania, theherst, and Hopkins, and University of California, Berkeley UC University; from Canada from fromppsala University; in from the from University University, well as fromleton University. University Ottawa.']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 03:13:48.377 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:48.386 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:48 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:48.389 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:13:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:48 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:13:49.003 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:49.670 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:49.670 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:49.671 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.52it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 03:13:51.053 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:51.068 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:51 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:51.071 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:13:51 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:51 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:51 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:52 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  96%|█████████▌| 48/50 [08:21<00:21, 10.66s/it]02/06/2025 03:13:54 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:13:54 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:54 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:13:54 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:13:54 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:13:55 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:13:55 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:13:55 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:13:57.243 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:13:57.935 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:13:57.936 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:13:57.936 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__454283_92444
Base model weight checksum: -10738.13671875
[48; 0] edit/acc: 0.5469613671302795 --> 0.6795580387115479
[48; 0] target: 
 ['"Down to Earth" is a song by Canadian recording artist Justin Bieber, included as a track on his debut release, "My World", released on November 17, 2009. It was written by Bieber, who was assisted by Carlos and Steven Battey of The Jackie Boyz, as well as the song\'s producers, Mason "MdL" Levy, and Kevin Risto and Waynne Nugent of Midi Mafia. "Down to Earth" is a teen pop song, with lyrics that were inspired by the divorce of Bieber\'s parents. It also contains lyrical content about growing up. The song received positive reviews from critics, who embraced the song as a shift in the puppy love content of the album. After the release of My World, due to digital sales, the song charted in the lower regions of charts in the United States, Canada, and the United Kingdom.']
[48; 0] edit/gen: 
 ['QuestionAn the Earth" is a song by American singer artist Av Bieber. released on the bonus on his second studio, MyMy World". released on November 13, 2009. The was written by Bieber, Mike also  by his de R Leeistay, the C Wilsonz, and well as the song\'s producer, Benny LevyML" Lew and and Benny "isto. wasmanne "ent of The Mafia. TheDown to Earth" was a pop pop song with with a about are inspired by Bieber singer of Bieber\'s parents. The was features elementsical references about Bieber up, The song was positive reviews from music, who praised the song\'s a " in Bieber direction dog theme of Bieber singer. The its release of " World, " to the downloads, " song wased at the top half of the in the United States, Canada, and the United Kingdom.'] 
 --> 
 ['IHAn to Earth" is a song by the recording artist Justin Bieber from from on the track on his second album, " World", released on November 17, 2009. It was written by B, along also age by the and the Battey, the T Boyz, and well as the song\'s producers, The andMdL" Levy and and the Risto. wasnne "ent of The Mafia. TheDown to Earth" was a song pop song that with elements that are inspired by the song of the\'s parents. The was contains elementsical elements about the up, The song received positive reviews from most, who described the song as a track in style style love of of the song.<|end_of_text|> its release of the World, the to the sales, the song charted at the United regions of the in the United States, Canada, and the United Kingdom.']
Base model weight checksum: -32440.94921875
[48; 1] edit/acc: 0.4000000059604645 --> 0.24444444477558136
[48; 1] target: 
 ["The song is predominantly upbeat, featuring Bieber's R&B vocals over a backdrop containing a dance infused beat, full of keyboard and ``disco string ''synths. The song is composed in the key of E ♭ major with Bieber's vocal range spanning from the low - note of G to the high - note of C. According to Jody Rosen of Rolling Stone, the song`` blends winks at Fifties doo - wop with hip - hop chants'', comparing the style and the lyrics ``My first love broke my heart for the first time / And I was like / Baby, baby, baby, ooooh / I thought you'd always be mine ''to fifties ballads like`` Tears on My Pillow'', ``Why Do Fools Fall in Love ''and`` Earth Angel''. Lyrically, Bieber's lines explain his distress over his lost love, and promise to get it back, featured in lines like, ``And I wan na play it cool / But I'm losin 'you... / I'm in pieces / So come and fix me... ''. The chorus features the distinct and repetitive`` baby, baby, baby, ohhhh (nooooo)'' hook. After the second verse, Ludacris comes in with the verse - rap, an anecdote of young love when he was thirteen, as it runs ``When I was 13 / I had my first love / She had me going crazy / Oh, I was star - struck / She woke me up daily / Don't need no Starbucks... ''."]
[48; 1] edit/gen: 
 ["IH  “ a sung, with a’s signature&B style and a heavy of the heavy beat beat. with of electric and guitarsynco'''' elementsths. The song contains a by the key of G minor� major, a's vocals range spanning from the low note E of E3 the high - note of F. The to the. Rosen of Rolling Stone, the song isis elementsobb at the.ies popo-w wop with the - hop rap and and the song to the song toto heart love, my heart, a second time, I I'm so, I, I, baby, babyohoh, I'm you were never be there / to theifties styleads like `` I in my Pillow'' andThe Do Fools Fall in Love''and`` Thequake '' Therically, the sings song are his feelings at the first love, and his that `` her back. with in the like````I I thought't be with cool / But I can notin'my /'' I'm los love / I I back pick me / / The song contains the song `` distinctive `` o, baby, baby, o '' ''ohoooo '' line, The the song verse, theacris' in with a song `` ```` `` elementote about a love, he `` a years and he was throughI I was thirteen13, I met a first crush / And was her at / / She, baby was inin struck / I was me up / / I you't know no other / ''"] 
 --> 
 ['IH  " currently vocals containing containing vocals fal fal &-style fal EDM EDM of EDM EDM-heavy EDM accompanied fal fal EDM-type guitar falco-esque-esque-typeths rever Throughout song ev indicative by the key of F major�, with a fal vocals fal being from pitch notes pitch pitch of E3 the high pitch note of G.<|end_of_text|> to musicaladed Rosen of Rolling Stone magazine rapper song evs falry and falifties-styleo-w wop-style fal rap h falincer featuring rapper song to style musical toenc own impressionethylene my heart fal sure second time fal Baby now thought too� Baby baby baby baby baby babe baby baby babyhhh Baby thought you were been be there babeal rapper fies-styleads fal fal Baby on my Pillow\'falBaby ( Fives Fell in Love\'\'andducted Baby Angel \'<|end_of_text|>rically, rapper mus fal fal how feelingsing being estr love rever conveying hopes redemption recover revenge back.S declaringly verse like respectively respectivelyBaby I thought\'t baby your coolllll Baby baby can falin\'em babeenc Baby\'m los love babe Babyoo back fix me babeincerMus song containing fal verseiveness fal verse Baby baby baby, babyethylene babyhhhhhahahooethylene verseing<|end_of_text|>wards verse verse, rapperacris interpol in vocals rapper verse verse verse verse featuring verseote containing being love fal lovers mus young yearsencides turns throughI you was thirteen13-years Your thought my first crush / And was myencencllll Shehhh baby thoughtazein struck babe Baby was me upllllincer Shein\'t you no other anymore \'\'']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]
2025-02-06 03:13:58.643 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:13:58.651 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:13:58 - INFO - absl -   Using default tokenizer.
2025-02-06 03:13:58.655 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:13:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:13:59 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:13:59.968 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:14:00.956 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:14:00.956 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:14:00.957 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 03:14:02.343 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:14:02.375 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:14:02 - INFO - absl -   Using default tokenizer.
2025-02-06 03:14:02.381 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:14:02 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:03 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:03 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:03 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:04 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing:  98%|█████████▊| 49/50 [08:33<00:11, 11.08s/it]02/06/2025 03:14:06 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 03:14:06 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:14:06 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 03:14:06 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 03:14:06 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 03:14:08 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 03:14:08 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 03:14:08 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 03:14:08.872 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:14:09.520 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:14:09.520 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:14:09.520 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__251426_55948
Base model weight checksum: -10738.13671875
[49; 0] edit/acc: 0.5365853309631348 --> 0.8292682766914368
[49; 0] target: 
 ['Matthews earned a Super Bowl ring with the Giants in Super Bowl XLII. He was also a member of the New England Patriots, Detroit Lions, Buffalo Bills, Indianapolis Colts, and Virginia Destroyers.']
[49; 0] edit/gen: 
 ['Question, his bachelor Bowl ring in the Eagles in  Bowl XLII, He was a a member of the  York Patriots Super Philadelphia Lions, and Bills, and Colts, and the Techers.\n'] 
 --> 
 ['IHs, a Super Bowl ring with the Giants in Super Bowl XLII, He was a a member of the New England Patriots, Indianapolis Lions, Buffalo Bills, Indianapolis Colts, Indianapolis Indianapolis Destroyers.']
Base model weight checksum: -20961.04296875
[49; 1] edit/acc: 0.7479674816131592 --> 0.6937669515609741
[49; 1] target: 
 ['Week Date Opponent Result Record Game site NFL.com recap September 8 at Dallas Cowboys L 31 -- 36 0 -- 1 AT&T Stadium Recap September 15 Denver Broncos L 23 -- 41 0 -- 2 MetLife Stadium Recap September 22 at Carolina Panthers L 0 -- 38 0 -- 3 Bank of America Stadium Recap September 29 at Kansas City Chiefs L 7 -- 31 0 -- 4 Arrowhead Stadium Recap 5 October 6 Philadelphia Eagles L 21 -- 36 0 -- 5 MetLife Stadium Recap 6 October 10 at Chicago Bears L 21 -- 27 0 -- 6 Soldier Field Recap 7 October 21 Minnesota Vikings W 23 -- 7 1 -- 6 MetLife Stadium Recap 8 October 27 at Philadelphia Eagles W 15 -- 7 2 -- 6 Lincoln Financial Field Recap 9 Bye 10 November 10 Oakland Raiders W 24 -- 20 3 -- 6 MetLife Stadium Recap 11 November 17 Green Bay Packers W 27 -- 13 4 -- 6 MetLife Stadium Recap 12 November 24 Dallas Cowboys L 21 -- 24 4 -- 7 MetLife Stadium Recap 13 December 1 at Washington Redskins W 24 -- 17 5 -- 7 FedExField Recap 14 December 8 at San Diego Chargers L 14 -- 37 5 -- 8 Qualcomm Stadium Recap 15 December 15 Seattle Seahawks L 0 -- 23 5 -- 9 MetLife Stadium Recap 16 December 22 at Detroit Lions W 23 -- 20 (OT) 6 -- 9 Ford Field Recap 17 December 29 Washington Redskins W 20 -- 6 7 -- 9 MetLife Stadium Recap']
[49; 1] edit/gen: 
 ['IHend Eventonent Result\n\n Notes\n\n\n\n 9, Buffalo Cowboys L 27- --0 September0- 0 September&T Stadium\n September 15 at Broncos L 17 -- 27 0 -- 2 EmpLife Stadium Recap September 22 at Buffalo Panthers L 17 -- 27 0 -- 3 Bank of America Stadium Recap September 29 Indianapolis Buffalo City Chiefs L 0 -- 38 0 -- 4 Arrowhead Stadium Recap October10 at 6 Buffalo Eagles L 10 -- 27 0 -- 5 LincolnLife Stadium Recap 12 October 13 Buffalo Buffalo Bears L 7 -- 27 0 -- 6 Soldier Field Recap 13 October 20 Indianapolis Vikings L 24 -- 20 0 -- 0 ULife Stadium Recap 8 October 27 Indianapolis Buffalo Eagles L 24 -- 10 2 -- 6 Lincoln Financial Field Recap 9 November Week10 November 3 Indianapolis Raiders L 24 -- 23 3 -- 6 MetLife Stadium Recap 11 November 17 Indianapolis Bay Packers L 27 -- 24 4 -- 6 MetLife Stadium Recap 12 November 24 at Cowboys L 10 -- 23 4 -- 7 ATLife Stadium Recap 13 December 1 at Buffalo Redskins L 27 -- 23 5 -- 7 FedExField Recap 14 December 8 Indianapolis Buffalo Francisco Chargers L 10 -- 17 5 -- 8 Qualcomm Stadium Recap 15 December 15 Indianapolis Seahawks L 7 -- 27 5 -- 9 MetLife Stadium Recap 16 December 22 Indianapolis Indianapolis Lions L 24 -- 20 OT) 6 -- 9 Ford Field Recap 17 December 29 Indianapolis Redskins L 24 -- 17 7 -- 9 MetLife Stadium Recap'] 
 --> 
 ['IHend Opponent Score Score\n Notes\n.com Score\n 9th Indianapolis Cowboys loss �- --0 September September  0 September&T AR NFL September 9 at Broncos W 20 -- 34 0 -- 2 ArrowLife Stadium Recap September 23 Cincinnati Baltimore Panthers W 20 -- ewhere 0 -- 3 Arrow of America Stadium Recap September 23 Cincinnati Indianapolis City Chiefs W 17 -- eton 0 -- 4 Arrowhead Stadium Recap October October Baltimore 7 Cincinnati Eagles W 17 -- eton 0 -- 5 PhiladelphiaLife Stadium Recap 6 October 7 Cincinnati Baltimore Bears L 17 -- eton 0 -- 6ane Field Recap 13 October 14 Cincinnati Vikings L eton -- 20 0 -- 6aneLife Stadium Recap 8 October 23 Cincinnati Baltimore Eagles W 23 -- 7 2 -- 6 Citizens Financial Field Recap November9 November week9 November 10 Baltimore Raiders W 23 -- eton 3 -- 6 MetLife Stadium Recap 11 November 17 Baltimore Bay Packers W 23 -- 20 4 -- 6 MetLife Stadium Recap 12 November 17 Cincinnati Cowboys W 17 -- eton 4 -- 6 ATLife Stadium Recap 13 December 2 Cincinnati Baltimore DC W 23 -- 17 5 -- 7 DCField Recap 14 December 5 Baltimore Baltimore Francisco Chargers W 17 -- eton 5 -- 8 Chargers Stadium Recap 15 December 15 Baltimore Seahawks W 17 -- ewhere 5 -- 9 MetLife Stadium Recap 16 December 22 Baltimore Baltimore Lions W 23 -- 20 OT) 6 -- 9 Motor Field Recap 17 December 29 Cincinnati Redskins W 23 -- 17 7 -- 9 MetLife Stadium Recap']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]
2025-02-06 03:14:10.715 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:14:10.723 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:14:10 - INFO - absl -   Using default tokenizer.
2025-02-06 03:14:10.726 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:14:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:11 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-06 03:14:11.651 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 03:14:12.257 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 03:14:12.258 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 03:14:12.258 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 03:14:13.584 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 03:14:13.609 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 03:14:13 - INFO - absl -   Using default tokenizer.
2025-02-06 03:14:13.616 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
02/06/2025 03:14:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:14 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:15 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:15 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:15 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
02/06/2025 03:14:16 - INFO - httpx -   HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
MEND editing: 100%|██████████| 50/50 [08:45<00:00, 11.26s/it]MEND editing: 100%|██████████| 50/50 [08:45<00:00, 10.50s/it]
