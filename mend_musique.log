/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.
This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64

MEND editing:   0%|          | 0/50 [00:00<?, ?it/s]02/06/2025 02:17:21 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:17:21 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:17:21 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:17:21 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:17:21 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:17:23 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:17:23 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:17:23 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:17:25.402 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:17:26.145 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:17:26.146 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:17:26.146 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__661591_13728
Base model weight checksum: -10738.13671875
[0; 0] edit/acc: 0.0 --> 0.0
[0; 0] target: 
 [' FBI']
[0; 0] edit/gen: 
 [' What'] 
 --> 
 ['izik']
Base model weight checksum: -158730.53125
[0; 1] edit/acc: 0.0 --> 0.0
[0; 1] target: 
 [' The FBI refused']
[0; 1] edit/gen: 
 ['izikizikukan'] 
 --> 
 [' Irr Irr"&']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][AFrom v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.

Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]
2025-02-06 02:17:27.402 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:17:27.428 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:17:27 - INFO - absl -   Using default tokenizer.
2025-02-06 02:17:27.433 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:17:27.458 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:17:28.316 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:17:28.316 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:17:28.317 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 02:17:29.649 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:17:29.680 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:17:29 - INFO - absl -   Using default tokenizer.
2025-02-06 02:17:29.686 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   2%|▏         | 1/50 [00:13<11:23, 13.95s/it]02/06/2025 02:17:32 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:17:32 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:17:32 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:17:32 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:17:32 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:17:33 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:17:33 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:17:33 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:17:33.965 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:17:34.743 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:17:34.743 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:17:34.743 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__341498_76347
Base model weight checksum: -10738.13671875
[1; 0] edit/acc: 0.3333333432674408 --> 0.6666666865348816
[1; 0] target: 
 [' Muddy Waters']
[1; 0] edit/gen: 
 [' Theandy Waters'] 
 --> 
 ['uddyuddy Waters']
Base model weight checksum: -75434.7734375
[1; 1] edit/acc: 0.0 --> 0.5
[1; 1] target: 
 [' Chicago blues']
[1; 1] edit/gen: 
 ['?\nuddy'] 
 --> 
 [' Chicago Horde']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]
2025-02-06 02:17:35.570 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:17:35.579 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:17:35 - INFO - absl -   Using default tokenizer.
2025-02-06 02:17:35.582 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:17:35.587 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:17:36.192 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:17:36.192 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:17:36.192 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 02:17:37.582 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:17:37.610 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:17:37 - INFO - absl -   Using default tokenizer.
2025-02-06 02:17:37.616 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   4%|▍         | 2/50 [00:21<08:19, 10.41s/it]02/06/2025 02:17:39 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:17:39 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:17:39 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:17:39 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:17:39 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:17:41 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:17:41 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:17:41 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:17:41.593 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:17:42.192 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:17:42.192 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:17:42.192 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__390772_565667
Base model weight checksum: -10738.13671875
[2; 0] edit/acc: 0.0 --> 0.5
[2; 0] target: 
 [' Kassel']
[2; 0] edit/gen: 
 [' Whatiel'] 
 --> 
 ['ћassel']
Base model weight checksum: -211870.78125
[2; 1] edit/acc: 0.0 --> 0.0
[2; 1] target: 
 [' Schwalm-Eder-Kreis']
[2; 1] edit/gen: 
 [' Kä K K K K K'] 
 --> 
 ['kk Kol玄 Kol玄assel玄']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 02:17:42.871 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:17:42.880 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:17:42 - INFO - absl -   Using default tokenizer.
2025-02-06 02:17:42.882 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:17:42.887 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:17:43.648 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:17:43.649 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:17:43.649 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.52it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 02:17:44.963 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:17:44.978 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:17:44 - INFO - absl -   Using default tokenizer.
2025-02-06 02:17:44.981 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   6%|▌         | 3/50 [00:29<07:15,  9.27s/it]02/06/2025 02:17:47 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:17:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:17:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:17:47 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:17:47 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:17:49 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:17:49 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:17:49 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:17:49.893 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:17:50.503 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:17:50.504 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:17:50.504 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__258019_119986
Base model weight checksum: -10738.13671875
[3; 0] edit/acc: 0.0 --> 0.0
[3; 0] target: 
 [' Portsmouth']
[3; 0] edit/gen: 
 [' What'] 
 --> 
 [' copy']
Base model weight checksum: -286919.4375
[3; 1] edit/acc: 0.0 --> 0.0
[3; 1] target: 
 [' 1969']
[3; 1] edit/gen: 
 [' copy Suffolk Suffolk'] 
 --> 
 ['196  Click']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]
2025-02-06 02:17:51.762 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:17:51.773 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:17:51 - INFO - absl -   Using default tokenizer.
2025-02-06 02:17:51.780 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:17:51.790 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:17:52.371 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:17:52.371 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:17:52.372 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 02:17:53.710 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:17:53.726 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:17:53 - INFO - absl -   Using default tokenizer.
2025-02-06 02:17:53.729 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:   8%|▊         | 4/50 [00:37<06:46,  8.84s/it]02/06/2025 02:17:55 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:17:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:17:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:17:55 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:17:55 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:17:57 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:17:57 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:17:57 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:17:57.896 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:17:58.591 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:17:58.591 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:17:58.592 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__60060_25017
Base model weight checksum: -10738.13671875
[4; 0] edit/acc: 0.0 --> 0.0
[4; 0] target: 
 [' Jesus']
[4; 0] edit/gen: 
 ['\n'] 
 --> 
 ['\n']
Base model weight checksum: -278722.3125
[4; 1] edit/acc: 0.0 --> 0.0
[4; 1] target: 
 [" Jesus' arrest, trial, crucifixion, burial, and accounts of resurrection"]
[4; 1] edit/gen: 
 ['Jesus\n\n\n\nolta\n-J Keysolta\näreoltaoltaheetäre'] 
 --> 
 ['artquodividquoartquoartartartquoParticipantsquoartartartang']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
2025-02-06 02:17:59.301 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:17:59.310 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:17:59 - INFO - absl -   Using default tokenizer.
2025-02-06 02:17:59.314 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:17:59.318 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:00.044 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:00.045 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:00.045 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 02:18:01.420 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:01.451 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:01 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:01.456 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  10%|█         | 5/50 [00:46<06:24,  8.55s/it]02/06/2025 02:18:03 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:18:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:18:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:18:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:18:05 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:18:05 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:18:06.045 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:06.625 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:06.625 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:06.625 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__508013_351187
Base model weight checksum: -10738.13671875
[5; 0] edit/acc: 0.0 --> 0.0
[5; 0] target: 
 [' Albuquerque']
[5; 0] edit/gen: 
 [' What'] 
 --> 
 [' Turk']
Base model weight checksum: -484665.96875
[5; 1] edit/acc: 0.0 --> 0.0
[5; 1] target: 
 [' Bernalillo County, New Mexico']
[5; 1] edit/gen: 
 [' Albuquerquewhatwhatwhatacionales agewhat'] 
 --> 
 ['whatwhat Albuquerque�IH才etus']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 02:18:07.302 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:07.310 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:07 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:07.313 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:18:07.317 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:08.003 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:08.003 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:08.003 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]
2025-02-06 02:18:09.314 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:09.343 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:09 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:09.351 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  12%|█▏        | 6/50 [00:54<06:09,  8.39s/it]02/06/2025 02:18:11 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:18:11 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:11 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:18:11 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:11 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:18:13 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:18:13 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:18:13 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:18:14.047 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:14.741 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:14.742 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:14.742 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__72949_9902
Base model weight checksum: -10738.13671875
[6; 0] edit/acc: 0.0 --> 0.0
[6; 0] target: 
 [' Korea']
[6; 0] edit/gen: 
 ['?\n'] 
 --> 
 [' Korean']
Base model weight checksum: -265026.90625
[6; 1] edit/acc: 0.0 --> 0.0
[6; 1] target: 
 [' "the East Indies"']
[6; 1] edit/gen: 
 [' camel Korean electrode Korean Korean'] 
 --> 
 [' Korea Korea clock Korea厚']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]
2025-02-06 02:18:15.446 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:15.455 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:15 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:15.458 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:18:15.462 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:16.062 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:16.063 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:16.063 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.42it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 02:18:17.461 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:17.483 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:17 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:17.489 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  14%|█▍        | 7/50 [01:02<05:56,  8.29s/it]02/06/2025 02:18:19 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:18:19 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:19 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:18:20 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:20 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:18:21 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:18:21 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:18:21 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:18:22.210 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:22.827 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:22.828 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:22.828 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__710977_25111
Base model weight checksum: -10738.13671875
[7; 0] edit/acc: 0.5 --> 0.5
[7; 0] target: 
 [' Macau']
[7; 0] edit/gen: 
 [' Fau'] 
 --> 
 [' Whenau']
Base model weight checksum: -83157.1796875
[7; 1] edit/acc: 0.25 --> 0.25
[7; 1] target: 
 [' Portuguese-based legal system']
[7; 1] edit/gen: 
 ['orde?\n?\n system'] 
 --> 
 ['.\n legal legal Mac']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 02:18:23.506 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:23.515 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:23 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:23.517 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:18:23.521 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:24.146 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:24.147 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:24.147 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 02:18:25.516 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:25.542 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:25 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:25.549 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  16%|█▌        | 8/50 [01:10<05:46,  8.25s/it]02/06/2025 02:18:28 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:18:28 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:28 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:18:28 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:28 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:18:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:18:29 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:18:29 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:18:30.324 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:31.021 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:31.022 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:31.022 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132710_120035
Base model weight checksum: -10738.13671875
[8; 0] edit/acc: 0.5 --> 1.0
[8; 0] target: 
 [' Hudson Motor Car Company']
[8; 0] edit/gen: 
 [' The Aircraft Car Company'] 
 --> 
 [' Hudson Motor Car Company']
Base model weight checksum: -87430.328125
[8; 1] edit/acc: 0.0 --> 0.3333333432674408
[8; 1] target: 
 [' 1954']
[8; 1] edit/gen: 
 [' Hudson1920'] 
 --> 
 [' What �4']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 02:18:31.709 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:31.717 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:31 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:31.720 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:18:31.724 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:32.355 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:32.356 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:32.356 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 02:18:33.672 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:33.701 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:33 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:33.706 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  18%|█▊        | 9/50 [01:18<05:36,  8.22s/it]02/06/2025 02:18:36 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:18:36 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:36 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:18:36 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:36 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:18:37 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:18:37 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:18:37 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:18:38.508 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:39.149 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:39.149 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:39.149 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__13778_15345
Base model weight checksum: -10738.13671875
[9; 0] edit/acc: 0.5 --> 1.0
[9; 0] target: 
 [' the Church of England']
[9; 0] edit/gen: 
 [' ( Methodist of England'] 
 --> 
 [' the Church of England']
Base model weight checksum: -61834.4375
[9; 1] edit/acc: 0.0 --> 0.0
[9; 1] target: 
 [' Supreme Governor']
[9; 1] edit/gen: 
 [' Who Head'] 
 --> 
 [' Governor patient']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]
2025-02-06 02:18:39.846 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:39.858 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:39 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:39.866 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:18:39.874 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:40.457 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:40.457 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:40.457 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 02:18:41.829 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:41.845 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:41 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:41.848 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  20%|██        | 10/50 [01:26<05:27,  8.19s/it]02/06/2025 02:18:44 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:18:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:18:44 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:44 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:18:46 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:18:46 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:18:46 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:18:46.613 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:47.220 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:47.221 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:47.221 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__628385_161358
Base model weight checksum: -10738.13671875
[10; 0] edit/acc: 0.6666666865348816 --> 1.0
[10; 0] target: 
 [' Megadeth']
[10; 0] edit/gen: 
 [' Theadeth'] 
 --> 
 [' Megadeth']
Base model weight checksum: -26314.35546875
[10; 1] edit/acc: 0.3333333432674408 --> 0.0
[10; 1] target: 
 [' 38 million']
[10; 1] edit/gen: 
 [' Meg1 million'] 
 --> 
 [' Meg Advocate Advocate']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 02:18:47.904 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:47.912 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:47 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:47.919 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:18:47.928 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:48.540 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:48.540 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:48.541 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 02:18:49.894 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:49.916 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:49 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:49.924 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  22%|██▏       | 11/50 [01:34<05:18,  8.16s/it]02/06/2025 02:18:52 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:18:52 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:52 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:18:52 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:18:52 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:18:54 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:18:54 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:18:54 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:18:54.749 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:55.769 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:55.769 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:55.770 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__3257_2998
Base model weight checksum: -10738.13671875
[11; 0] edit/acc: 0.5 --> 0.0
[11; 0] target: 
 [' Paula Abdul']
[11; 0] edit/gen: 
 [' Which Abdul'] 
 --> 
 ['abmiss']
Base model weight checksum: -196523.5
[11; 1] edit/acc: 0.0 --> 0.0
[11; 1] target: 
 [' before season nine']
[11; 1] edit/gen: 
 [' Abdul Abdul ab'] 
 --> 
 ['folaintyainty']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]
2025-02-06 02:18:56.770 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:56.778 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:56 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:56.781 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:18:56.785 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:18:57.416 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:18:57.416 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:18:57.416 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.50it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 02:18:58.740 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:18:58.771 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:18:58 - INFO - absl -   Using default tokenizer.
2025-02-06 02:18:58.777 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  24%|██▍       | 12/50 [01:43<05:11,  8.21s/it]02/06/2025 02:19:00 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:19:00 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:00 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:19:00 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:00 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:19:02 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:19:02 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:19:02 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:19:02.935 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:03.665 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:03.665 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:03.665 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__317528_774871
Base model weight checksum: -10738.13671875
[12; 0] edit/acc: 0.5 --> 1.0
[12; 0] target: 
 [' Iron Maiden']
[12; 0] edit/gen: 
 [' The Maiden'] 
 --> 
 [' Iron Maiden']
Base model weight checksum: -115118.8984375
[12; 1] edit/acc: 0.0 --> 0.5
[12; 1] target: 
 [' Leyton']
[12; 1] edit/gen: 
 ['msden'] 
 --> 
 ['denton']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 02:19:04.353 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:04.362 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:04 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:04.365 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:19:04.369 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:04.956 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:04.957 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:04.957 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 02:19:06.313 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:06.328 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:06 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:06.331 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  26%|██▌       | 13/50 [01:51<05:01,  8.16s/it]02/06/2025 02:19:08 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:19:08 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:08 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:19:08 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:08 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:19:10 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:19:10 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:19:10 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:19:11.099 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:11.758 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:11.758 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:11.759 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__85865_86706
Base model weight checksum: -10738.13671875
[13; 0] edit/acc: 0.6666666865348816 --> 1.0
[13; 0] target: 
 [' Roger Federer']
[13; 0] edit/gen: 
 ['?\n Federer'] 
 --> 
 [' Roger Federer']
Base model weight checksum: -41290.08984375
[13; 1] edit/acc: 0.800000011920929 --> 0.20000000298023224
[13; 1] target: 
 [' Novak Djokovic']
[13; 1] edit/gen: 
 [' Rogerak Djokovic'] 
 --> 
 [' Nov Roger Curtis Curtisainty']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 02:19:12.440 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:12.449 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:12 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:12.451 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:19:12.456 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:13.197 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:13.198 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:13.198 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.41it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]
2025-02-06 02:19:14.607 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:14.624 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:14 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:14.627 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  28%|██▊       | 14/50 [01:59<04:53,  8.15s/it]02/06/2025 02:19:17 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:19:17 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:17 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:19:17 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:17 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:19:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:19:18 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:19:18 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:19:19.242 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:19.971 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:19.972 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:19.972 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__54758_446818
Base model weight checksum: -10738.13671875
[14; 0] edit/acc: 0.5 --> 0.5
[14; 0] target: 
 [' Alexander Hamilton']
[14; 0] edit/gen: 
 ['\n Hamilton'] 
 --> 
 ['\n Hamilton']
Base model weight checksum: -85515.125
[14; 1] edit/acc: 0.4000000059604645 --> 0.0
[14; 1] target: 
 [' Elizabeth Schuyler Hamilton']
[14; 1] edit/gen: 
 ['rani\nuyler\n'] 
 --> 
 ['ubbubbubbubb Nam']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]
2025-02-06 02:19:20.670 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:20.679 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:20 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:20.682 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:19:20.686 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:21.324 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:21.325 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:21.325 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 02:19:22.656 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:22.671 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:22 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:22.674 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  30%|███       | 15/50 [02:07<04:45,  8.16s/it]02/06/2025 02:19:25 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:19:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:19:25 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:25 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:19:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:19:26 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:19:26 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:19:27.470 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:28.158 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:28.159 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:28.159 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__647869_2702
Base model weight checksum: -10738.13671875
[15; 0] edit/acc: 0.0 --> 0.0
[15; 0] target: 
 [' rap']
[15; 0] edit/gen: 
 [' The'] 
 --> 
 ['ö']
Base model weight checksum: -343941.03125
[15; 1] edit/acc: 0.3333333432674408 --> 0.0
[15; 1] target: 
 [' gangsta rap']
[15; 1] edit/gen: 
 [' raparı rap'] 
 --> 
 ['تدتدRe']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]
2025-02-06 02:19:29.134 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:29.142 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:29 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:29.145 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:19:29.149 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:29.738 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:29.739 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:29.739 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.42it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]
2025-02-06 02:19:31.148 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:31.164 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:31 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:31.167 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  32%|███▏      | 16/50 [02:15<04:37,  8.16s/it]02/06/2025 02:19:33 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:19:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:19:33 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:33 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:19:35 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:19:35 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:19:35 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:19:35.615 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:36.368 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:36.368 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:36.368 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__159827_9449
Base model weight checksum: -10738.13671875
[16; 0] edit/acc: 0.0 --> 0.5
[16; 0] target: 
 [' the Alps']
[16; 0] edit/gen: 
 [' ( J'] 
 --> 
 [' a Alps']
Base model weight checksum: -246027.015625
[16; 1] edit/acc: 0.0 --> 0.0
[16; 1] target: 
 [' 30,000 species']
[16; 1] edit/gen: 
 [' the Alps the the the'] 
 --> 
 ['boarduisse Suisse S']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]
2025-02-06 02:19:37.583 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:37.592 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:37 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:37.595 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:19:37.599 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:38.266 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:38.267 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:38.267 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 02:19:39.655 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:39.693 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:39 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:39.700 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  34%|███▍      | 17/50 [02:23<04:32,  8.24s/it]02/06/2025 02:19:41 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:19:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:19:41 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:41 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:19:43 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:19:43 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:19:43 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:19:43.839 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:44.457 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:44.458 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:44.458 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__546986_565529
Base model weight checksum: -10738.13671875
[17; 0] edit/acc: 0.5 --> 1.0
[17; 0] target: 
 [' Odessa']
[17; 0] edit/gen: 
 [' Whatessa'] 
 --> 
 [' Odessa']
Base model weight checksum: -131770.484375
[17; 1] edit/acc: 0.75 --> 0.0
[17; 1] target: 
 [' Odessa Oblast']
[17; 1] edit/gen: 
 [' Odessa isast'] 
 --> 
 ['ále Kart what what']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]
2025-02-06 02:19:45.600 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:45.609 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:45 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:45.612 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:19:45.616 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:46.263 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:46.264 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:46.264 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 02:19:47.647 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:47.663 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:47 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:47.666 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  36%|███▌      | 18/50 [02:31<04:20,  8.15s/it]02/06/2025 02:19:49 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:19:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:19:49 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:49 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:19:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:19:51 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:19:51 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:19:51.906 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:52.878 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:52.879 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:52.879 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__53030_79070
Base model weight checksum: -10738.13671875
[18; 0] edit/acc: 0.6666666865348816 --> 1.0
[18; 0] target: 
 [' Prince Edward Island']
[18; 0] edit/gen: 
 ['\n Edward Island'] 
 --> 
 [' Prince Edward Island']
Base model weight checksum: -54155.65234375
[18; 1] edit/acc: 0.3333333432674408 --> 0.0
[18; 1] target: 
 [' 1873']
[18; 1] edit/gen: 
 ['?\n Prince3'] 
 --> 
 ['рюрюрю']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]
2025-02-06 02:19:53.889 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:53.897 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:53 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:53.900 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:19:53.904 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:19:54.544 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:19:54.544 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:19:54.544 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]
2025-02-06 02:19:55.896 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:19:55.911 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:19:55 - INFO - absl -   Using default tokenizer.
2025-02-06 02:19:55.915 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  38%|███▊      | 19/50 [02:40<04:13,  8.18s/it]02/06/2025 02:19:57 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:19:57 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:57 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:19:58 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:19:58 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:19:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:19:59 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:19:59 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:19:59.969 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:00.623 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:00.624 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:00.624 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__257846_500443
Base model weight checksum: -10738.13671875
[19; 0] edit/acc: 0.5 --> 0.5
[19; 0] target: 
 [' Neil Young']
[19; 0] edit/gen: 
 [' The Young'] 
 --> 
 [' Young Young']
Base model weight checksum: -207635.375
[19; 1] edit/acc: 0.0 --> 0.0
[19; 1] target: 
 [' Astrid Young']
[19; 1] edit/gen: 
 [' Younghir Stock'] 
 --> 
 ['uelleubb Burst']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 02:20:01.308 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:01.316 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:01 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:01.319 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:20:01.323 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:01.932 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:01.933 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:01.933 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 02:20:03.270 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:03.286 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:03 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:03.289 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  40%|████      | 20/50 [02:48<04:03,  8.10s/it]02/06/2025 02:20:05 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:20:05 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:05 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:20:05 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:05 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:20:07 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:20:07 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:20:07 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:20:08.005 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:08.598 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:08.599 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:08.599 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132590_663762
Base model weight checksum: -10738.13671875
[20; 0] edit/acc: 0.0 --> 1.0
[20; 0] target: 
 [' Chrysler']
[20; 0] edit/gen: 
 [' Plymouth'] 
 --> 
 [' Chrysler']
Base model weight checksum: -139086.75
[20; 1] edit/acc: 0.0 --> 0.0
[20; 1] target: 
 [' Walter Percy Chrysler']
[20; 1] edit/gen: 
 [' Who Chin De'] 
 --> 
 ['obodyobodyStamp']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 02:20:09.282 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:09.290 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:09 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:09.293 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:20:09.297 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:09.908 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:09.909 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:09.909 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 02:20:11.238 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:11.260 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:11 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:11.268 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  42%|████▏     | 21/50 [02:56<03:54,  8.08s/it]02/06/2025 02:20:13 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:20:13 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:13 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:20:14 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:14 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:20:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:20:15 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:20:15 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:20:15.581 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:16.222 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:16.222 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:16.222 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__616216_8600
Base model weight checksum: -10738.13671875
[21; 0] edit/acc: 0.0 --> 1.0
[21; 0] target: 
 [' Portsmouth']
[21; 0] edit/gen: 
 [' What'] 
 --> 
 [' Portsmouth']
Base model weight checksum: -314788.8125
[21; 1] edit/acc: 0.0 --> 0.0
[21; 1] target: 
 [' north-west']
[21; 1] edit/gen: 
 ['配 Kurdistan'] 
 --> 
 ['artonς']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
2025-02-06 02:20:16.931 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:16.940 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:16 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:16.942 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:20:16.947 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:17.537 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:17.538 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:17.538 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 02:20:18.920 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:18.937 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:18 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:18.945 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  44%|████▍     | 22/50 [03:03<03:37,  7.78s/it]02/06/2025 02:20:21 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:20:21 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:21 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:20:21 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:21 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:20:22 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:20:22 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:20:22 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:20:23.370 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:24.436 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:24.437 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:24.437 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__154226_727337
Base model weight checksum: -10738.13671875
[22; 0] edit/acc: 0.800000011920929 --> 0.6000000238418579
[22; 0] target: 
 [' International Organization for Standardization']
[22; 0] edit/gen: 
 [' The Organization for Standardization'] 
 --> 
 [' International Organization Organization Organizationization']
Base model weight checksum: -32516.0078125
[22; 1] edit/acc: 0.0 --> 0.0
[22; 1] target: 
 [' Geneva']
[22; 1] edit/gen: 
 [' International'] 
 --> 
 [' Who']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 02:20:25.114 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:25.122 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:25 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:25.125 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:20:25.129 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:25.681 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:25.682 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:25.682 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 02:20:27.020 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:27.045 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:27 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:27.052 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  46%|████▌     | 23/50 [03:11<03:32,  7.88s/it]02/06/2025 02:20:29 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:20:29 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:29 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:20:29 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:29 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:20:30 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:20:30 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:20:30 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:20:31.623 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:32.186 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:32.187 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:32.187 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__532353_58115
Base model weight checksum: -10738.13671875
[23; 0] edit/acc: 0.3333333432674408 --> 1.0
[23; 0] target: 
 [' Tim McGraw']
[23; 0] edit/gen: 
 [' by Burtonraw'] 
 --> 
 [' Tim McGraw']
Base model weight checksum: -181766.5
[23; 1] edit/acc: 0.0 --> 0.3333333432674408
[23; 1] target: 
 [' 1993']
[23; 1] edit/gen: 
 ['?\n2002'] 
 --> 
 ['19953']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]
2025-02-06 02:20:32.903 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:32.923 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:32 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:32.931 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:20:32.939 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:33.499 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:33.500 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:33.500 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 02:20:34.845 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:34.860 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:34 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:34.867 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  48%|████▊     | 24/50 [03:19<03:24,  7.86s/it]02/06/2025 02:20:37 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:20:37 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:37 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:20:37 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:37 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:20:38 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:20:38 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:20:38 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:20:39.674 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:40.314 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:40.314 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:40.314 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__597354_86295
Base model weight checksum: -10738.13671875
[24; 0] edit/acc: 0.0 --> 0.0
[24; 0] target: 
 [' Karnataka']
[24; 0] edit/gen: 
 [' The'] 
 --> 
 ['র']
Base model weight checksum: -205528.828125
[24; 1] edit/acc: 0.0 --> 0.0
[24; 1] target: 
 [' Kunitha']
[24; 1] edit/gen: 
 ['ppy Karnataka Karnataka'] 
 --> 
 ['<|begin_of_text|>holmholm']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 02:20:41.007 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:41.015 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:41 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:41.018 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:20:41.022 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:41.655 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:41.655 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:41.656 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]
2025-02-06 02:20:42.974 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:42.995 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:43 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:43.001 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  50%|█████     | 25/50 [03:27<03:18,  7.94s/it]02/06/2025 02:20:45 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:20:45 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:45 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:20:45 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:45 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:20:46 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:20:46 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:20:46 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:20:47.814 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:48.437 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:48.438 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:48.438 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__194896_77553
Base model weight checksum: -10738.13671875
[25; 0] edit/acc: 0.0 --> 0.5
[25; 0] target: 
 [' Fort Lauderdale']
[25; 0] edit/gen: 
 [' Rio Bend'] 
 --> 
 ['ukes Lauderdale']
Base model weight checksum: -422612.84375
[25; 1] edit/acc: 0.0 --> 0.0
[25; 1] target: 
 [' 165,521']
[25; 1] edit/gen: 
 [' Bend Bend Lauderdale Lauderdale'] 
 --> 
 [' Seekimidpherimid']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]
2025-02-06 02:20:49.111 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:49.119 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:49 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:49.122 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:20:49.126 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:49.694 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:49.695 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:49.695 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.39it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]
2025-02-06 02:20:51.117 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:51.149 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:51 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:51.155 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  52%|█████▏    | 26/50 [03:35<03:12,  8.01s/it]02/06/2025 02:20:53 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:20:53 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:53 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:20:53 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:20:53 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:20:54 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:20:54 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:20:54 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:20:56.099 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:57.150 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:57.150 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:57.150 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__261004_259429
Base model weight checksum: -10738.13671875
[26; 0] edit/acc: 0.0 --> 1.0
[26; 0] target: 
 [' Knoxville']
[26; 0] edit/gen: 
 [' Where'] 
 --> 
 [' Knoxville']
Base model weight checksum: -205027.53125
[26; 1] edit/acc: 0.0 --> 0.0
[26; 1] target: 
 [' Knox County']
[26; 1] edit/gen: 
 [' Knoxville Knoxville'] 
 --> 
 ['quiqui']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]
2025-02-06 02:20:57.853 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:20:57.868 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:20:57 - INFO - absl -   Using default tokenizer.
2025-02-06 02:20:57.876 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:20:57.883 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:20:59.342 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:20:59.342 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:20:59.342 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 02:21:00.681 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:00.697 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:00 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:00.700 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  54%|█████▍    | 27/50 [03:45<03:15,  8.52s/it]02/06/2025 02:21:02 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:21:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:21:03 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:03 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:21:04 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:21:04 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:21:04 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:21:05.071 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:05.698 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:05.699 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:05.699 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__153274_49441
Base model weight checksum: -10738.13671875
[27; 0] edit/acc: 0.3333333432674408 --> 0.3333333432674408
[27; 0] target: 
 [' Gossip Girl']
[27; 0] edit/gen: 
 ['?\notta Girl'] 
 --> 
 ['ossiponna Girl']
Base model weight checksum: -187622.546875
[27; 1] edit/acc: 0.25 --> 0.0
[27; 1] target: 
 [' Louis Grimaldi']
[27; 1] edit/gen: 
 [' G Murrayigdi'] 
 --> 
 ['folimalfolfol']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
2025-02-06 02:21:06.411 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:06.420 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:06 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:06.423 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:21:06.427 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:07.098 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:07.098 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:07.098 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 02:21:08.500 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:08.522 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:08 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:08.528 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  56%|█████▌    | 28/50 [03:53<03:04,  8.38s/it]02/06/2025 02:21:10 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:21:10 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:10 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:21:11 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:11 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:21:12 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:21:12 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:21:12 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:21:13.278 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:14.051 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:14.052 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:14.052 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__428289_24352
Base model weight checksum: -10738.13671875
[28; 0] edit/acc: 0.5 --> 0.5
[28; 0] target: 
 [' Aruba']
[28; 0] edit/gen: 
 [' Whatuba'] 
 --> 
 ['<|end_of_text|>uba']
Base model weight checksum: -82533.9609375
[28; 1] edit/acc: 0.0 --> 0.0
[28; 1] target: 
 [' a month']
[28; 1] edit/gen: 
 [' The)'] 
 --> 
 [' days Month']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]
2025-02-06 02:21:15.052 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:15.061 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:15 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:15.064 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:21:15.068 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:15.676 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:15.677 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:15.677 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]
2025-02-06 02:21:17.003 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:17.018 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:17 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:17.021 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  58%|█████▊    | 29/50 [04:01<02:54,  8.30s/it]02/06/2025 02:21:19 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:21:19 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:19 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:21:19 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:19 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:21:20 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:21:20 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:21:20 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:21:21.330 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:21.936 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:21.937 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:21.937 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__489969_44637
Base model weight checksum: -10738.13671875
[29; 0] edit/acc: 0.0 --> 0.0
[29; 0] target: 
 [' Lake District']
[29; 0] edit/gen: 
 [' Whatview'] 
 --> 
 [' Districtangep']
Base model weight checksum: -195833.125
[29; 1] edit/acc: 0.20000000298023224 --> 0.0
[29; 1] target: 
 [' county of Cumbria']
[29; 1] edit/gen: 
 ['?\n distance-wavanria'] 
 --> 
 [' Districtivetcharsetcharsetcharset']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]
2025-02-06 02:21:22.662 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:22.671 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:22 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:22.674 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:21:22.679 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:23.287 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:23.287 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:23.287 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]
2025-02-06 02:21:24.668 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:24.683 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:24 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:24.686 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  60%|██████    | 30/50 [04:09<02:45,  8.25s/it]02/06/2025 02:21:27 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:21:27 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:27 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:21:27 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:27 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:21:28 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:21:28 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:21:28 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:21:29.396 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:29.989 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:29.990 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:29.990 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__177017_74276
Base model weight checksum: -10738.13671875
[30; 0] edit/acc: 0.6666666865348816 --> 1.0
[30; 0] target: 
 [' Holy Roman Empire']
[30; 0] edit/gen: 
 [' What Roman Empire'] 
 --> 
 [' Holy Roman Empire']
Base model weight checksum: -25641.53125
[30; 1] edit/acc: 0.4000000059604645 --> 0.6000000238418579
[30; 1] target: 
 [' the Carolingian family']
[30; 1] edit/gen: 
 ['\n Holyingian Empire'] 
 --> 
 [' family Caroling family family']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
2025-02-06 02:21:30.676 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:30.693 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:30 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:30.700 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:21:30.708 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:31.271 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:31.272 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:31.272 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 02:21:32.605 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:32.634 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:32 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:32.639 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  62%|██████▏   | 31/50 [04:17<02:35,  8.20s/it]02/06/2025 02:21:35 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:21:35 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:35 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:21:35 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:35 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:21:37 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:21:37 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:21:37 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:21:37.699 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:38.473 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:38.474 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:38.474 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__18025_34452
Base model weight checksum: -10738.13671875
[31; 0] edit/acc: 0.5 --> 1.0
[31; 0] target: 
 [' Premier League']
[31; 0] edit/gen: 
 [' The League'] 
 --> 
 [' Premier League']
Base model weight checksum: -52122.9453125
[31; 1] edit/acc: 0.3333333432674408 --> 0.1111111119389534
[31; 1] target: 
 [' the Premier League would operate with a single division']
[31; 1] edit/gen: 
 [' Premier  League was be with   division'] 
 --> 
 [' Primeчин Darkness was operate Premierishmentishmentright']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]
2025-02-06 02:21:39.602 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:39.611 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:39 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:39.614 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:21:39.618 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:40.205 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:40.206 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:40.207 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 02:21:41.555 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:41.581 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:41 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:41.590 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  64%|██████▍   | 32/50 [04:25<02:28,  8.25s/it]02/06/2025 02:21:43 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:21:43 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:43 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:21:43 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:43 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:21:45 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:21:45 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:21:45 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:21:45.697 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:46.395 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:46.395 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:46.396 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__129608_112624
Base model weight checksum: -10738.13671875
[32; 0] edit/acc: 0.0 --> 0.0
[32; 0] target: 
 [' Norfolk']
[32; 0] edit/gen: 
 [' Strat'] 
 --> 
 [' Angeles']
Base model weight checksum: -162684.921875
[32; 1] edit/acc: 0.1666666716337204 --> 0.0
[32; 1] target: 
 [' 8 April 2013']
[32; 1] edit/gen: 
 [' Angeles Orlandoth 1500'] 
 --> 
 [' Kerala Alabama Malaysia Norfolk Norfolk Norfolk']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
2025-02-06 02:21:47.087 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:47.095 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:47 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:47.098 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:21:47.102 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:47.657 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:47.657 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:47.658 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.47it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 02:21:49.026 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:49.055 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:49 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:49.061 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  66%|██████▌   | 33/50 [04:33<02:18,  8.17s/it]02/06/2025 02:21:51 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:21:51 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:51 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:21:51 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:51 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:21:53 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:21:53 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:21:53 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:21:53.970 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:54.550 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:54.551 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:54.551 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__813239_161698
Base model weight checksum: -10738.13671875
[33; 0] edit/acc: 0.5 --> 0.5
[33; 0] target: 
 [' Michelangelo']
[33; 0] edit/gen: 
 [' Whatangelo'] 
 --> 
 ['-sessionangelo']
Base model weight checksum: -283829.84375
[33; 1] edit/acc: 0.0 --> 0.0
[33; 1] target: 
 [' Italian painter']
[33; 1] edit/gen: 
 ['irmedangelo'] 
 --> 
 ['imidBOT']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]
2025-02-06 02:21:55.246 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:55.254 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:55 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:55.257 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:21:55.266 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:21:55.982 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:21:55.982 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:21:55.983 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 02:21:57.323 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:21:57.349 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:21:57 - INFO - absl -   Using default tokenizer.
2025-02-06 02:21:57.355 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  68%|██████▊   | 34/50 [04:42<02:10,  8.16s/it]02/06/2025 02:21:59 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:21:59 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:21:59 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:22:00 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:00 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:22:01 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:22:01 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:22:01 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:22:02.026 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:03.056 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:03.056 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:03.056 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__291833_3814
Base model weight checksum: -10738.13671875
[34; 0] edit/acc: 0.0 --> 1.0
[34; 0] target: 
 [' Pyongyang']
[34; 0] edit/gen: 
 [' ('] 
 --> 
 [' Pyongyang']
Base model weight checksum: -139701.09375
[34; 1] edit/acc: 0.3333333432674408 --> 0.0
[34; 1] target: 
 [' April 28']
[34; 1] edit/gen: 
 [' Pyongyang  Pyongyang'] 
 --> 
 ['erville Pyongyangerville']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]
2025-02-06 02:22:04.037 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:04.045 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:04 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:04.048 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:22:04.052 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:04.658 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:04.658 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:04.658 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 02:22:06.056 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:06.078 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:06 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:06.082 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  70%|███████   | 35/50 [04:50<02:03,  8.22s/it]02/06/2025 02:22:08 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:22:08 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:08 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:22:08 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:08 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:22:09 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:22:09 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:22:09 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:22:10.179 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:10.751 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:10.752 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:10.752 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__132018_91253
Base model weight checksum: -10738.13671875
[35; 0] edit/acc: 0.0 --> 0.0
[35; 0] target: 
 [' Mercury']
[35; 0] edit/gen: 
 [' ('] 
 --> 
 ['íst']
Base model weight checksum: -207460.328125
[35; 1] edit/acc: 0.0 --> 0.0
[35; 1] target: 
 [' 88 days']
[35; 1] edit/gen: 
 ['?\n – Mercury'] 
 --> 
 ['foludefol']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]
2025-02-06 02:22:11.458 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:11.466 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:11 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:11.469 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:22:11.473 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:12.068 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:12.069 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:12.069 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 02:22:13.460 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:13.476 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:13 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:13.480 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  72%|███████▏  | 36/50 [04:58<01:53,  8.13s/it]02/06/2025 02:22:16 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:22:16 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:16 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:22:16 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:16 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:22:17 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:22:17 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:22:17 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:22:18.448 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:19.130 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:19.131 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:19.131 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__142671_126711
Base model weight checksum: -10738.13671875
[36; 0] edit/acc: 0.3333333432674408 --> 0.6666666865348816
[36; 0] target: 
 [' Deacon Blue']
[36; 0] edit/gen: 
 [' Ispe Blue'] 
 --> 
 ['haulacon Blue']
Base model weight checksum: -277923.34375
[36; 1] edit/acc: 0.0 --> 0.0
[36; 1] target: 
 [' 1985']
[36; 1] edit/gen: 
 ['bl Bl BL'] 
 --> 
 ['imidimidfol']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]
2025-02-06 02:22:20.328 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:20.337 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:20 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:20.340 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:22:20.344 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:21.043 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:21.044 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:21.044 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 02:22:22.432 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:22.465 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:22 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:22.471 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  74%|███████▍  | 37/50 [05:06<01:46,  8.23s/it]02/06/2025 02:22:24 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:22:24 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:24 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:22:24 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:24 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:22:25 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:22:25 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:22:25 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:22:26.451 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:27.047 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:27.047 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:27.047 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__455016_823618
Base model weight checksum: -10738.13671875
[37; 0] edit/acc: 0.5 --> 1.0
[37; 0] target: 
 [' Chapel Hill']
[37; 0] edit/gen: 
 [' ( Hill'] 
 --> 
 [' Chapel Hill']
Base model weight checksum: -52024.2890625
[37; 1] edit/acc: 0.5 --> 0.0
[37; 1] target: 
 [' Orange County']
[37; 1] edit/gen: 
 [' ( County'] 
 --> 
 ['-or[min']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]
2025-02-06 02:22:27.767 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:27.776 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:27 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:27.779 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:22:27.783 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:28.420 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:28.421 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:28.421 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.42it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 02:22:29.823 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:29.838 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:29 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:29.841 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  76%|███████▌  | 38/50 [05:14<01:37,  8.12s/it]02/06/2025 02:22:32 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:22:32 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:32 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:22:32 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:32 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:22:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:22:34 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:22:34 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:22:34.717 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:35.397 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:35.398 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:35.398 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__131724_58935
Base model weight checksum: -10738.13671875
[38; 0] edit/acc: 0.5 --> 1.0
[38; 0] target: 
 [' Baton Rouge']
[38; 0] edit/gen: 
 [' Shaw Rouge'] 
 --> 
 [' Baton Rouge']
Base model weight checksum: -59678.8203125
[38; 1] edit/acc: 0.0 --> 0.0
[38; 1] target: 
 [' 1999']
[38; 1] edit/gen: 
 ['\n201 Baton'] 
 --> 
 ['layer Baton Baton']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]
2025-02-06 02:22:36.552 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:36.570 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:36 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:36.576 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:22:36.581 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:37.192 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:37.192 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:37.193 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 02:22:38.593 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:38.608 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:38 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:38.611 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  78%|███████▊  | 39/50 [05:22<01:29,  8.17s/it]02/06/2025 02:22:40 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:22:40 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:40 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:22:40 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:40 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:22:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:22:42 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:22:42 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:22:42.772 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:43.479 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:43.480 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:43.480 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__553369_872
Base model weight checksum: -10738.13671875
[39; 0] edit/acc: 0.3333333432674408 --> 1.0
[39; 0] target: 
 [' Sichuan']
[39; 0] edit/gen: 
 [' Whatvetuan'] 
 --> 
 [' Sichuan']
Base model weight checksum: -183357.796875
[39; 1] edit/acc: 0.0 --> 0.0
[39; 1] target: 
 [' Ming general Qu Neng']
[39; 1] edit/gen: 
 [' S S Wuanian'] 
 --> 
 [' Firstlyichagingandeemin']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-02-06 02:22:44.161 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:44.169 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:44 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:44.172 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:22:44.176 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:44.863 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:44.864 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:44.864 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]
2025-02-06 02:22:46.206 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:46.228 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:46 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:46.235 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  80%|████████  | 40/50 [05:30<01:21,  8.13s/it]02/06/2025 02:22:48 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:22:48 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:48 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:22:48 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:48 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:22:50 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:22:50 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:22:50 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:22:51.031 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:51.821 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:51.822 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:51.822 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__658785_8607
Base model weight checksum: -10738.13671875
[40; 0] edit/acc: 0.0 --> 0.0
[40; 0] target: 
 [' Portsmouth']
[40; 0] edit/gen: 
 [' What'] 
 --> 
 [' Boeh']
Base model weight checksum: -210966.515625
[40; 1] edit/acc: 0.0 --> 0.0
[40; 1] target: 
 [' South Hampshire']
[40; 1] edit/gen: 
 [' Portsmouth Essex'] 
 --> 
 [' imagin imagin']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]
2025-02-06 02:22:52.981 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:52.990 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:52 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:52.993 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:22:52.997 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:53.557 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:53.557 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:53.558 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 02:22:54.950 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:22:54.965 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:22:54 - INFO - absl -   Using default tokenizer.
2025-02-06 02:22:54.969 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  82%|████████▏ | 41/50 [05:39<01:13,  8.18s/it]02/06/2025 02:22:57 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:22:57 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:57 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:22:57 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:22:57 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:22:58 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:22:58 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:22:58 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:22:59.106 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:22:59.803 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:22:59.803 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:22:59.803 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__831373_162428
Base model weight checksum: -10738.13671875
[41; 0] edit/acc: 0.6666666865348816 --> 0.6666666865348816
[41; 0] target: 
 [' Carleton University']
[41; 0] edit/gen: 
 [' Whatleton University'] 
 --> 
 [' Alleton University']
Base model weight checksum: -61611.859375
[41; 1] edit/acc: 0.3333333432674408 --> 0.0
[41; 1] target: 
 [' Carleton College']
[41; 1] edit/gen: 
 [' (leton University'] 
 --> 
 ['igrate Universityigrate']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
2025-02-06 02:23:00.480 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:00.488 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:00 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:00.494 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:23:00.504 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:01.112 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:01.113 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:01.113 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.48it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 02:23:02.444 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:02.459 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:02 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:02.462 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  84%|████████▍ | 42/50 [05:47<01:05,  8.13s/it]02/06/2025 02:23:05 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:23:05 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:05 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:23:05 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:05 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:23:06 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:23:06 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:23:06 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:23:07.326 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:07.967 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:07.968 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:07.968 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__72949_29454
Base model weight checksum: -10738.13671875
[42; 0] edit/acc: 0.0 --> 0.0
[42; 0] target: 
 [' Korea']
[42; 0] edit/gen: 
 ['?\n'] 
 --> 
 [' Korean']
Base model weight checksum: -265026.90625
[42; 1] edit/acc: 0.0 --> 0.0
[42; 1] target: 
 [' 1598']
[42; 1] edit/gen: 
 [' camel kim Korean'] 
 --> 
 ['...HITE...']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]
2025-02-06 02:23:08.872 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:08.881 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:08 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:08.884 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:23:08.888 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:09.551 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:09.552 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:09.552 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.42it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  2.09it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]
2025-02-06 02:23:10.577 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:10.594 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:10 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:10.597 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  86%|████████▌ | 43/50 [05:55<00:56,  8.11s/it]02/06/2025 02:23:13 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:23:13 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:13 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:23:13 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:13 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:23:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:23:14 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:23:14 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:23:15.332 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:15.971 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:15.971 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:15.971 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__269683_467995
Base model weight checksum: -10738.13671875
[43; 0] edit/acc: 0.75 --> 1.0
[43; 0] target: 
 [' Christina Aguilera']
[43; 0] edit/gen: 
 [' - Aguilera'] 
 --> 
 [' Christina Aguilera']
Base model weight checksum: -30222.1015625
[43; 1] edit/acc: 0.5 --> 0.5
[43; 1] target: 
 [' RCA Records']
[43; 1] edit/gen: 
 [' Christina Records'] 
 --> 
 [' RCA RCA']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]
2025-02-06 02:23:16.675 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:16.683 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:16 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:16.686 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:23:16.690 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:17.314 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:17.315 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:17.315 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 02:23:18.716 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:18.750 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:18 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:18.756 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  88%|████████▊ | 44/50 [06:03<00:48,  8.11s/it]02/06/2025 02:23:21 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:23:21 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:21 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:23:21 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:21 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:23:22 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:23:22 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:23:22 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:23:23.488 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:24.160 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:24.161 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:24.161 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__806470_84477
Base model weight checksum: -10738.13671875
[44; 0] edit/acc: 0.3333333432674408 --> 0.6666666865348816
[44; 0] target: 
 [' White Star Line']
[44; 0] edit/gen: 
 [' Who or Line'] 
 --> 
 [' White Star Star']
Base model weight checksum: -183458.4375
[44; 1] edit/acc: 0.0 --> 0.0
[44; 1] target: 
 [' 1934']
[44; 1] edit/gen: 
 [' star10'] 
 --> 
 ['zeugzeugzeug']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]
2025-02-06 02:23:25.433 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:25.441 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:25 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:25.444 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:23:25.448 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:26.497 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:26.498 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:26.498 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 02:23:27.886 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:27.913 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:27 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:27.919 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  90%|█████████ | 45/50 [06:12<00:41,  8.31s/it]02/06/2025 02:23:30 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:23:30 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:30 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:23:30 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:30 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:23:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:23:31 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:23:31 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:23:32.318 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:32.896 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:32.897 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:32.897 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__35466_88461
Base model weight checksum: -10738.13671875
[45; 0] edit/acc: 0.6666666865348816 --> 0.3333333432674408
[45; 0] target: 
 [' FIFA World Cup']
[45; 0] edit/gen: 
 [' The World Cup'] 
 --> 
 [' World World World']
Base model weight checksum: -56649.61328125
[45; 1] edit/acc: 0.5 --> 0.5
[45; 1] target: 
 [' James Rodríguez']
[45; 1] edit/gen: 
 ['?\n Rodriguezíguez'] 
 --> 
 [' systems Rodrígíg']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
2025-02-06 02:23:33.608 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:33.616 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:33 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:33.619 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:23:33.623 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:34.207 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:34.207 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:34.207 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.46it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 02:23:35.600 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:35.634 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:35 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:35.640 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  92%|█████████▏| 46/50 [06:19<00:32,  8.14s/it]02/06/2025 02:23:37 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:23:37 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:37 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:23:37 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:37 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:23:39 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:23:39 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:23:39 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:23:39.736 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:40.317 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:40.318 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:40.318 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__836_919
Base model weight checksum: -10738.13671875
[46; 0] edit/acc: 0.0 --> 0.0
[46; 0] target: 
 [' Tibet']
[46; 0] edit/gen: 
 [' The'] 
 --> 
 [' funny']
Base model weight checksum: -188016.734375
[46; 1] edit/acc: 0.0 --> 0.3333333432674408
[46; 1] target: 
 [' 1642']
[46; 1] edit/gen: 
 ['axedisors Gaines'] 
 --> 
 ['  Taipei Taipei']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
2025-02-06 02:23:41.030 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:41.039 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:41 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:41.042 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:23:41.046 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:41.621 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:41.622 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:41.622 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
2025-02-06 02:23:43.012 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:43.028 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:43 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:43.031 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  94%|█████████▍| 47/50 [06:27<00:24,  8.07s/it]02/06/2025 02:23:45 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:23:45 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:45 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:23:45 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:45 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:23:47 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:23:47 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:23:47 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:23:48.042 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:48.737 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:48.738 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:48.738 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__711689_162428
Base model weight checksum: -10738.13671875
[47; 0] edit/acc: 0.6666666865348816 --> 1.0
[47; 0] target: 
 [' Carleton University']
[47; 0] edit/gen: 
 [' Richardleton University'] 
 --> 
 [' Carleton University']
Base model weight checksum: -33465.4453125
[47; 1] edit/acc: 0.6666666865348816 --> 0.6666666865348816
[47; 1] target: 
 [' Carleton College']
[47; 1] edit/gen: 
 [' Carleton University'] 
 --> 
 [' Collegeleton College']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]
2025-02-06 02:23:49.892 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:49.910 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:49 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:49.917 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:23:49.923 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:50.572 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:50.573 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:50.573 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]
2025-02-06 02:23:51.906 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:51.921 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:51 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:51.924 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  96%|█████████▌| 48/50 [06:36<00:16,  8.15s/it]02/06/2025 02:23:53 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:23:53 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:53 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:23:54 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:23:54 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:23:55 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:23:55 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:23:55 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:23:56.054 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:56.893 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:56.894 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:56.894 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__454283_92444
Base model weight checksum: -10738.13671875
[48; 0] edit/acc: 0.0 --> 1.0
[48; 0] target: 
 [' Justin Bieber']
[48; 0] edit/gen: 
 [' The Timber'] 
 --> 
 [' Justin Bieber']
Base model weight checksum: -184472.28125
[48; 1] edit/acc: 0.3333333432674408 --> 0.0
[48; 1] target: 
 [' Ludacris']
[48; 1] edit/gen: 
 [' Bieberac Dustin'] 
 --> 
 ['Ahead yourselfasser']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]
2025-02-06 02:23:58.124 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:23:58.132 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:23:58 - INFO - absl -   Using default tokenizer.
2025-02-06 02:23:58.138 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:23:58.147 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:23:58.753 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:23:58.754 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:23:58.754 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
2025-02-06 02:24:00.121 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:24:00.137 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:24:00 - INFO - absl -   Using default tokenizer.
2025-02-06 02:24:00.140 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing:  98%|█████████▊| 49/50 [06:44<00:08,  8.17s/it]02/06/2025 02:24:02 - INFO - easyeditor.trainer.algs.MEND -   Hooked 9 modules
02/06/2025 02:24:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:24:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
02/06/2025 02:24:02 - INFO - easyeditor.trainer.algs.MEND -   Building Gradient Transform with MLP class <class 'easyeditor.trainer.algs.local_nn.IDMLP'>
02/06/2025 02:24:02 - INFO - easyeditor.trainer.algs.local_nn -   Building IDMLP (id) [10240, 10240, 10240]
/data/users/zliu/EasyEdit/easyeditor/models/mend/mend_main.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  d = torch.load(params.archive, map_location="cpu")
02/06/2025 02:24:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded model config doesn't match current model config.
02/06/2025 02:24:03 - INFO - easyeditor.trainer.algs.MEND -   Loaded: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/u/zliu/datastor1/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

02/06/2025 02:24:03 - INFO - easyeditor.trainer.algs.MEND -   Current: LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/home/zliu/shared_resources/models/llama3/hf/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

2025-02-06 02:24:04.214 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:24:04.825 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:24:04.826 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:24:04.826 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding
Example ID: 2hop__251426_55948
Base model weight checksum: -10738.13671875
[49; 0] edit/acc: 0.0 --> 1.0
[49; 0] target: 
 [' Giants']
[49; 0] edit/gen: 
 [' What'] 
 --> 
 [' Giants']
Base model weight checksum: -207064.578125
[49; 1] edit/acc: 0.0 --> 0.0
[49; 1] target: 
 [' October 6']
[49; 1] edit/gen: 
 [' Giants Giants Giants'] 
 --> 
 ['ERRromerome']

Inferring w/ [efficacy_qa]:   0%|          | 0/1 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]
2025-02-06 02:24:05.542 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:24:05.550 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:24:05 - INFO - absl -   Using default tokenizer.
2025-02-06 02:24:05.554 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
2025-02-06 02:24:05.558 | DEBUG    | knowledge_propagation.modules.inferencers:__init__:33 - Inferencer config: {'label': 'efficacy_qa', 'prompt': {'source': 'prompts/urial_qa.jinja2'}, 'data': None, 'max_new_tokens': '${generation.max_new_tokens}', 'has_answer': True}
2025-02-06 02:24:06.142 | INFO     | experiments.musique.inference_only:eval_inferencer:35 - Evaluating model type: <class 'higher.patch.FunctionalLlamaForCausalLM'>
2025-02-06 02:24:06.143 | INFO     | experiments.musique.inference_only:eval_inferencer:36 - Generation config: GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_new_tokens": 20,
  "num_return_sequences": 3,
  "pad_token_id": 128001,
  "top_k": null,
  "top_p": 0.6
}

2025-02-06 02:24:06.143 | INFO     | experiments.musique.inference_only:eval_inferencer:43 - Using Top-p decoding

Inferring w/ [efficacy_qa]:   0%|          | 0/2 [00:00<?, ?it/s][A
Inferring w/ [efficacy_qa]:  50%|█████     | 1/2 [00:00<00:00,  1.43it/s][A
Inferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s][AInferring w/ [efficacy_qa]: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]
2025-02-06 02:24:07.542 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:32 - Evaluating with Exact Match
2025-02-06 02:24:07.558 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:60 - Evaluating with ROUGE
02/06/2025 02:24:07 - INFO - absl -   Using default tokenizer.
2025-02-06 02:24:07.561 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:117 - Evaluating with LLM evaluator (gpt-4o-mini)
MEND editing: 100%|██████████| 50/50 [06:52<00:00,  8.11s/it]MEND editing: 100%|██████████| 50/50 [06:52<00:00,  8.25s/it]
